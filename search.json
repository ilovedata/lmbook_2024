[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "통계적 예측모형",
    "section": "",
    "text": "Preface\n이 책은 통계적 예측모형에 대한 교재이며 일반 선형모형을 포함하여 예측에 사용되는 기본적인 통계 모형에 대한 이론을 최대 가능도 추정법의 관점에서 설명합니다. 또한 실제 예제를 통한 실습, 모형을 적합하는 계산방법과 연관된 행렬이론에 대하여 다루고자 합니다.\n\n\n\n\n\n\n노트\n\n\n\n이 책에서 사용된 기호, 표기법, 프로그램의 규칙과 쓰임은 다음과 같습니다.\n\n스칼라(scalar)와 일변량 확률변수는 일반적으로 보통 글씨체의 소문자로 표기한다. 특별한 이유가 있는 경우 대문자로 표시할 것이다.\n벡터, 행렬, 다변량 확률벡터는 굵은 글씨체로 표기한다.\n통계 프로그램은 R을 이용하였다. 각 예제에 사용된 R 프로그램은 코드 상자를 열면 나타난다.\n\n\n\n강의의 부교재는 강근석 와/과 유형조 (2016) 을 사용한다.\n이 교과서에서 이용하는 R 패키지는 다음과 같다.\n\nlibrary(here)           # file pathways\nlibrary(tidyverse)      # data management, summary, and visualization\nlibrary(MASS)\nlibrary(knitr)\nlibrary(kableExtra)\n\nlibrary(agricolae)\nlibrary(emmeans)\nlibrary(car)\n\nlibrary(olsrr)\nlibrary(leaps)\n\nlibrary(plotly)\nlibrary(plot3D)\n\n# 아래 3 문장은 한글을 포함한 ggplot 그림이 포함된 HTML, PDF로 만드는 경우 사용\nlibrary(showtext)\nfont_add_google(\"Nanum Pen Script\", \"gl\")\nshowtext_auto()\n\n#강의 부교재 자료를 포함한 패키지 설치\ninstall.packages(\"remotes\")\nremotes::install_github(\"regbook/regbook\")\nlibrary(regbook)\n\n\n\n\n강근석, 와/과 유형조. 2016. R을 활용한 선형회귀분석. 1st ed. 교우사. https://github.com/regbook/regbook.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "qmd/lse.html",
    "href": "qmd/lse.html",
    "title": "1  선형 회귀모형의 소개",
    "section": "",
    "text": "1.1 예제-단순 회귀모형",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#예제-단순-회귀모형",
    "href": "qmd/lse.html#예제-단순-회귀모형",
    "title": "1  선형 회귀모형의 소개",
    "section": "",
    "text": "보기 1.1 (자동차의 제동거리) 자동차가 달리는 속도(speed,단위는 mph; mile per hour)와 제동거리(dist, 단위는 ft;feet)의 관계를 알아보기 위하여 50대의 자동차로 실험한 결과의 자료 cars 는 다음과 같다(처음 10개의 자료만 보여준다). 자료는 R 의 data.frame 형식으로 저장되어 있다.\n아래 자료를 보면 실험에서 2대의 자동차는 7 mph 로 달리다가 브레이크를 밟고 정지하는 경우 각각 4, 22 feet 의 제동거리가 필요한 것으로 나타났다. 또한 3대의 는 10 mph 로 달리다가 각각 18, 26, 34 feet 의 제동거리가 필요한 것으로 나타났다.\n\ncars %&gt;% head(n=10) \n\n   speed dist\n1      4    2\n2      4   10\n3      7    4\n4      7   22\n5      8   16\n6      9   10\n7     10   18\n8     10   26\n9     10   34\n10    11   17\n\n\n자동차의 속도와 제동거리에 대한 산포도는 아래와 같다.\n\nggplot(cars, aes(x=speed, y=dist)) + geom_point() + labs(x = \"속도\", y = \"거리\") +\n  labs(title=\"자동차의 속도와 제동거리의 관계\")\n\n\n\n\n\n\n\n\n위와 같은 자료를 이용하여 자동차의 속도가 주어졌을 경우 제동거리를 예측하려고 한다면 어떤 방법을 사용해야 할까?\n\n\n보기 1.2 (아파트 판매가격) 다음 살펴볼 자료는 2019년 거래된 서울 아파트의 실거래 데이터 중 4개의 구(동대문구, 서초구, 관악구, 노원구)에서 거래된 아파트 중 1000개의 아파트를 임의로 추출한 자료이다.\n\napart_2019 &lt;- read.csv(here(\"data\", \"seoul_apartment_2019_sample.csv\"), header = T)\nhead(apart_2019,10)\n\n       gu year  area price\n1  관악구 1974 65.09   450\n2  관악구 1978 56.86   276\n3  관악구 1982 91.24   599\n4  관악구 1982 91.24   560\n5  관악구 1984 60.27   425\n6  관악구 1984 60.27   420\n7  관악구 1985 38.92   217\n8  관악구 1988 61.74   485\n9  관악구 1991 71.90   328\n10 관악구 1991 84.44   438\n\n\n아파트의 면적(area;제곱미터)에 따른 거래가격(price;백만원)의 변화는 다음 그림과 같다.\n\nggplot(apart_2019, aes(x=area, y=price)) + geom_point() + labs(x = \"면적(제곱미터)\", y = \"거래가격(백만원)\") +\n   labs(title = \"아파트의 면적과 거래가격의 관계\")\n\n\n\n\n\n\n\n\n만약 아파트의 면적(x)과 거래가격(y) 대신 각각의 로그값(log(x), `log(y))을 시용하면 다음과 같은 산포도가 나타난다.\n\n# log scale for y\nggplot(apart_2019, aes(x=area, y=price)) + geom_point() + labs(x = \"면적(제곱미터)\", y = \"거래가격(백만원)\") +\n  scale_y_log10() + \n  scale_x_log10() +\n  labs(title = \"아파트의 면적과 거래가격의 관계 (로그스케일)\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#선형-회귀모형",
    "href": "qmd/lse.html#선형-회귀모형",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.2 선형 회귀모형",
    "text": "1.2 선형 회귀모형\n회귀모형(regression model)는 변수들의 함수적 관계를 분석하는 통계적 방법이다. 일반적으로 한 개 또는 여러 개의 설명변수들(explanatory variables, x)이 관심있는 반응변수(response variable, y)에 어떤 형태로 영향을 미치는지에 파악하고 설명변수와 반응변수의 함수 관계를 통계적으로 추론하는 것이 회귀분석의 목적이다.\n위에서 살펴본 두 예제에서 자동차의 속도(x)가 증가하면 제동거리(y)가 증가하는 경향이 있다는 것을 알 수 있으며, 아파트의 면적(x)과 거래가격(y)도 유사한 관계임을 알 수 있다.\n이러한 두 변수의 관계를 다음과 같은 반응변수 \\(y\\) 와 설명변수의 선형 에측식(linear predictor)으로 나타내어 보자. 이러한 관계는 반응변수의 값의 변화를 근사적으로 설명변수의 선형식으로 예측할 수 있다는 의미이다.\n\\[ y \\approx \\beta_0 + \\beta_1 x \\]\n위와 같은 근사적인 관계를 더 구체화하여 다음과 같이 반응변수의 평균값이 설명변수의 선형식으로 나타나는 것을 가정할 수 있으며 이를 선형 회귀모형(linear regression model)이라고 한다.이\n\\[\nE(y|x) = \\beta_0 + \\beta_1 x\n\\tag{1.1}\\]\n식 1.1 은 반응변수 \\(y\\)의 평균이 설명변수 \\(x\\) 의 선형 예측식으로 나타나는 관계를 가정한 것이며 절편 \\(\\beta_0\\) 와 기울기 \\(\\beta_1\\) 는 회귀계수(regression coefficient)라고 부르는 모수(parameter)로서 추정해야 한다.\n특별히 하나의 설명변수를 사용하는 회귀 모형을 단순선형 회귀모형(simple linear regression model)이라고 한다.\n위에서 본 두 예제와 같이 \\(n\\) 개의 자료 \\((x_1,y_1),(x_2,y_2),..,(x_n, y_n)\\)을 독립적으로 추출하였다면 자료의 생성 과정을 다음과 같은 단순선형 회귀모형으로 나타낼 수 있다. 반응변수 \\(y_i\\)는 설명변수 \\(x_i\\)의 선형함수로 표현된 선형 예측식 식 1.1 과 임의의 오차항 (random error) \\(e_i\\) 의 합으로 나타내어진다고 가정하자.\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + e_i, \\quad i=1,2,\\dots,n\n\\tag{1.2}\\]\n여기서 오차항 \\(e_i\\)는 평균이 \\(0\\)이고 분산이 \\(\\sigma^2\\) 인 임의의 확률분포를 따르며 서로 독립이라고 가정한다.\n\\[ E(e_i)=0, \\quad V(e_i) = \\sigma^2 \\quad i=1,2,\\dots,n \\]\n오차항의 분산 \\(\\sigma^2\\)도 추정해야할 모수(parameter)이다.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#최소제곱법",
    "href": "qmd/lse.html#최소제곱법",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.3 최소제곱법",
    "text": "1.3 최소제곱법\n앞에서 언급한 것과 같이 선형회귀모형 식 1.2 에서 모수 \\(\\beta_0\\)와 \\(\\beta_1\\)를 회귀계수라고 하며 자료를 이용하여 추정해야 한다. \\(n\\)개의 자료를 이용하여 회귀계수 \\(\\beta_0\\)와 \\(\\beta_1\\)를 추정하려고 할 때 사용할 수 있는 방법들 중에서 가장 쉽고 유용한 방법은 최소제곱법(least square method)이다.\n회귀모형 식 1.2 에서 \\(\\beta_0\\)와 \\(\\beta_1\\)의 값이 주어졌다면 설명변수 \\(x_i\\) 에서 반응변수의 관측값 \\(y_i\\)에 가장 합리적인 예측값은 무었일까? 가장 합리적인 예측값은 주어진 \\(x_i\\)에서 반응변수의 평균값인 \\(E(y_i | x_i)=\\beta_0 + \\beta_1 x_i\\)이다. 여기서 실제 관측하여 얻어진 값 \\(y_i\\)와 예측값 \\(\\beta_0 + \\beta_1 x_i\\) 사이에는 오차에 의해서 차이가 발생할 수 있다. 그 차이를 잔차(residual)라고 하며 \\(r_i\\) 라고 표기한다.\n\\[  r_i = y_i - E(y_i|x_i) = y_i - (  \\beta_0 +  \\beta_1 x_i) \\]\n잔차는 위에 식에서 알 수 있듯이 관측값과 회귀식을 통한 예측값의 차이를 나타낸 것이다. 그러면 자료를 가장 잘 설명할 수 있는 회귀직선을 얻기 위해서는 잔차 \\(r_i\\)를 가장 작게하는 회귀모형을 세워야 한다. 잔차들을 최소로 하는 방법들 중 하나인 최소제곱법은 잔차들의 제곱합을 최소로 하는 회귀계수 \\(\\beta_0\\)와 \\(\\beta_1\\)를 추정하는 방법이다. 잔차들의 제곱합은 다음과 같이 표현된다.\n\\[\nS(\\beta_0 , \\beta_1) = \\sum^n_{i=1}r^2_i = \\sum^n_{i=1}[y_i-(\\beta_0 + \\beta_1 x_i)]^2\n\\tag{1.3}\\]\n\n\n\n\n\n\n노트\n\n\n\n식 1.3 를 잔차제곱합(residusl sum of square)이라고 부른다. 일반적으로 회귀계수의 값이 특정지어져서 실제로 잔차를 계산할 수 있는 경우 잔차제곱합이라고 부른다. 뒤에 분산분석에서는 잔차제곱합을 SSE(sum of square error)라고 부른다.\n잔차제곱합을 최소로 하는 회귀계수의 값을 찾는 최적화의 목표로 잔차제곱합이 제시될 때 이를 오차제곱합(error sum of square)이라고 부른다.\n\n\n위의 오차제곱합 \\(S(\\beta_0 , \\beta_1)\\) 을 최소화하는 \\(\\beta_0\\)와 \\(\\beta_1\\)의 값을 구하는 방법은 오차제곱합이 \\(\\beta_0\\)와 \\(\\beta_1\\)의 미분 가능한 2차 함수이고 아래로 볼록한 함수(convex function)임을 이용한다.\n\ngridnum &lt;- 60\nsizing &lt;- 5\nextrascale &lt;- 10\nextrascale2 &lt;- 0.7\nb0 &lt;- seq(-17.6-sizing*extrascale,  -17.6+sizing*extrascale, length=gridnum )\nb1 &lt;- seq(4-sizing*extrascale2, 4+sizing*extrascale2, length=gridnum )\n\nSSE &lt;- matrix(0, gridnum, gridnum )\nfor (i in 1:gridnum ) {\n  for (j in 1:gridnum ){\n    r &lt;- cars$dist- b0[i] -b1[j]*cars$speed\n    SSE[i,j]  &lt;- (sum(r^2))/1000\n  }\n}\n\npersp3D(b0, b1, SSE, theta =10, phi = 20, expand = 1)\n\n## Interactive 3d graph\n#fig &lt;- plot_ly(z = ~SSE)\n#fig &lt;- fig %&gt;% add_surface()\n#fig\n\n\n\n\n\n\n오차제곱합의 함수 형태\n\n\n\n\n위의 그림을 보면 볼록한 모양이 너무 평평하여 오차제곱합이 최소가 되는 \\(\\beta_0\\)와 \\(\\beta_1\\)의 위치가 명확하지 않다.\n이제 모든 변수들을 표준화하고 표준화된 변수들에 단순회귀모형에 대한 오차제곱합을 \\(\\beta_0\\)와 \\(\\beta_1\\)의 함수로서 그림을 그리면 아래와 같다.\n\\[\nv_i = \\beta_0 + \\beta_1 w_i + e_i, \\quad i=1,2,\\dots,n\n\\tag{1.4}\\]\n여기서\n\\[ v_i  = \\frac{y_i -\\bar y}{s_y}, \\quad w_i = \\frac{x_i -\\bar x}{s_x} \\]\n\n# 변수들을 표준화!\nstd_cars &lt;- as.data.frame(scale(cars))\ngridnum &lt;- 60\nsizing &lt;- 1\nb0 &lt;- seq(0-sizing,  0+sizing, length=gridnum )\nb1 &lt;- seq(1-sizing, 1+sizing, length=gridnum )\n\nSSE &lt;- matrix(0, gridnum, gridnum )\nfor (i in 1:gridnum ) {\n  for (j in 1:gridnum ){\n    r &lt;- std_cars$dist- b0[i] -b1[j]*std_cars$speed\n    SSE[i,j]  &lt;- sum(r^2)\n  }\n}\n\npersp3D(b0, b1, SSE, theta =40, phi = 15, expand = 1)\n\n## Interactive 3d graph\n#fig &lt;- plot_ly(z = ~SSE)\n#fig &lt;- fig %&gt;% add_surface()\n#fig\n\n\n\n\n\n\n표준화 시 오차제곱합의 함수 형태\n\n\n\n\n위위 같이 변수들을 표준화하면 오차제곱합 함수의 볼록한 정도가 덜 평평하게 변하여 최적값을 더 확실하게 보인다. 기계학습이나 인공지능 모형에서 적합하기 전에 모든 변수를 표준화하는 이유가 위의 그림에서 나타난다.\n식 1.3 의 오차제곱합을 각 회귀계수에 대해서 편미분을 하고 0으로 놓으면 아래와 같이 두 방정식이 얻어진다.\n\\[\n\\begin{aligned}\n\\pardiff{ S(\\beta_0 , \\beta_1)}{\\beta_0} & = \\sum^n_{i=1}(-2)[y_i-(\\beta_0+\\beta_1 x_i)]=0    \\\\ \\notag\n\\pardiff{ S(\\beta_0 , \\beta_1)}{\\beta_1}  & = \\sum^n_{i=1}(-2 x_i)[y_i-(\\beta_0+\\beta_1 x_i)]=0\n\\end{aligned}\n\\tag{1.5}\\]\n위의 연립방정식을 행렬식으로 표시하면 다음과 같이 나타낼 수 있다.\n\\[\n\\begin{bmatrix}\nn & \\sum_i x_i \\\\\n\\sum_i x_i & \\sum_i x^2_i\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sum_i y_i \\\\\n\\sum_i x_i y_i\n\\end{bmatrix}\n\\]\n위의 방정식을 풀어서 구한 회귀계수의 추정치를 \\(\\hat \\beta_0\\), \\(\\hat \\beta_1\\) 이라고 하면 다음과 같이 주어진다.\n\\[\n\\begin{aligned}\n\\hat \\beta_0 &= \\bar y - \\hat \\beta_1 \\bar x  \\\\\n  \\hat \\beta_1 &=  \\frac{ \\sum_i (x_i - \\bar x)(y_i - \\bar y)}{\\sum_i (x_i - \\bar x)^2}\n\\end{aligned}\n\\]\n최소제곱법에서 얻어진 회귀계수의 추정량 \\(\\hat \\beta_0\\)과 \\(\\hat \\beta_1\\) 을 이용한 반응변수 \\(y_i\\) 에 대한 예측값 \\(\\hat y_i\\)는 다음과 같이 정의되고\n\\[ \\hat y_i = \\hat E(y_i|x_i) = \\hat \\beta_0 + \\hat \\beta_1 x_i \\]\n\n\n\n\n\n\n표준화 전과 후\n\n\n\n두 개의 회귀방정식 식 1.2 과 식 1.4 에서 각각 최소제곱법으로 구한 기울기의 추정치 \\(\\hat \\beta_1\\) 이 동일하게 나타나는 경우는 어떤 경우일까 생각해보자.\n\n\n잔차 \\(r_i\\)는 다음과 같이 계산한다.\n\\[\nr_i = y_i - (\\hat \\beta_0 + \\hat \\beta_1 x_i) = y_i -\\hat y_i  \n\\tag{1.6}\\]\n잔차 \\(r_i\\)는 다음과 같은 성질을 가진다.\n\\[ \\sum_{i=1}^n r_i = 0 \\]\n\\[ \\sum_{i=1}^n x_i r_i = 0 \\]\n이제 위에서 본 cars 자료를 가지고 선형회귀모형 식 1.2 에 나타난 회귀계수를 추정해보자. 아래는 R 프로그램에서 함수 lm을 이용한 추정결과이다.\n\nlm_car &lt;- lm(dist~speed, data=cars)\nsummary(lm_car)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n위에서 주어진 선형회귀모형 식 1.2 에 대한 추정 결과를 이용하면 자동차의 속도(\\(x\\) = speed)와 제동거리(\\(y\\) = dist)의 관계는 다음과 같은 회귀식으로 나타낼 수 있다.\n\\[ \\hat E(y | x) = −17.58 + 3.93 x \\]\n\nggplot(cars, aes(x=speed, y=dist)) + geom_point() + labs(x = \"속도\", y = \"거리\") +\n  labs(title=\"자동차의 속도와 제동거리의 관계\") + \n  geom_abline(intercept = -17.58, slope = 3.93, color = \"red\")\n\n\n\n\n\n\n\n\n위의 추정식을 이용하면 주어진 자동차의 속도에서 제동거리를 예측할 수 있다. 예를 들어 자동차의 속도가 25 mph인 경우에는 제동거리의 평균이 80.73 mph 임을 알 수 있다.\n\\[ E(y|x=25) = −17.58 + 3.93 (25) = 80.73 \\]\n\nnewcars &lt;- data.frame(speed = c(25))\npredict(lm_car, newdata=newcars)\n\n       1 \n80.73112 \n\n\n기울기의 추정값 \\(\\hat \\beta_1 = 3.93\\) 은 자동차의 속도 (\\(x\\))가 1 mph 증가할 때 평균 제동거리 (\\(E(y|x)\\))가 3.93 ft 증가한다는 의미이다.\n이제 아파트 거애 가격에 대한 단순선형회귀모형을 적합해보자. 이 경우 면적과 가격대신 각각의 로그값을 사용하여 회귀모형을 적합해 보자. 아래는 아파트의 면적과 거래가격에 대한 단순선형 회귀모형을 적합한 결과이다.\n\napart_2019_log &lt;- apart_2019 %&gt;% \n  mutate(log_area = log10(area), log_price = log10(price)) %&gt;%\n  dplyr::select(log_area, log_price)\n\nhead(apart_2019_log,10)\n\n   log_area log_price\n1  1.813514  2.653213\n2  1.754807  2.440909\n3  1.960185  2.777427\n4  1.960185  2.748188\n5  1.780101  2.628389\n6  1.780101  2.623249\n7  1.590173  2.336460\n8  1.790567  2.685742\n9  1.856729  2.515874\n10 1.926548  2.641474\n\n\n\nlm_apart &lt;- lm( log_price~ log_area, data=apart_2019_log)\nsummary(lm_apart)\n\n\nCall:\nlm(formula = log_price ~ log_area, data = apart_2019_log)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45348 -0.12132 -0.04075  0.07531  0.62358 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.76902    0.05217   14.74   &lt;2e-16 ***\nlog_area     1.08797    0.02843   38.27   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1894 on 998 degrees of freedom\nMultiple R-squared:  0.5948,    Adjusted R-squared:  0.5944 \nF-statistic:  1465 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n위의 결과는 다음과 같이 나타낼 수 있다.\n\\[ \\hat E( \\log10 y | x) = 0.769 + 1.0797 \\log10 (x) \\]\n\nggplot(apart_2019_log, aes(x=log_area, y=log_price)) + geom_point() + labs(x = \"로그 면적(제곱미터)\", y = \"로그 거래가격(백만원)\") +\n  labs(title = \"아파트의 면적과 거래가격의 관계 (로그스케일)\") + \n  geom_abline(intercept = 0.769, slope = 1.0797, color = \"red\")\n\n\n\n\n\n\n\n\n이제 위의 결과를 응용하면 아파트의 면적이 100 제곱미터인 경우의 아파트의 평균 거래가격을 880(백만원)으로 예측할 수 있다.\n\nnewapart &lt;- data.frame(log_area = c(log10(100)))\npred_y &lt;- 10^predict(lm_apart, newdata=newapart)\npred_y\n\n       1 \n880.9605",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#결정계수",
    "href": "qmd/lse.html#결정계수",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.4 결정계수",
    "text": "1.4 결정계수\n고려한 설명변수와 반응변수에 대하여 제시된 회귀식을 적합한 후 회귀모형이 두 변수의 관계를 얼마나 잘 설명하는지에 대한 기준이 필요하다. 회귀식의 적합에 대한 기준으로서 결정계수(coefficient of determination; \\(R^2\\))가 있다. 결정계수는 적합의 정도(degree of fitting)를 측정한다. 즉 “설명변수는 반응변수를 얼마나 잘 예측하느냐”에 대한 정도를 수치로 표현한 것이다.\n회귀분석에서 설명변수와 반응변수 간에 전혀 관계가 없다면 당연히 반응변수의 값은 설명변수 값의 변동 여하에 전혀 영향을 받지 않아야 한다. 단순회귀모형에서 설명변수 \\(x\\)의 값의 변화를 반응변수 \\(y\\)로 값으로 표현하는것이 바로 기울기 \\(\\beta_1\\)이다. 이렇게 고려한 설명변수 \\(x\\)가 반응변수 \\(y\\)를 예측하는데 전혀 소용이 없다면 이는 기울기에 대한 회귀계수가 0 \\(\\beta_1=0\\) 이라는 것을 의미이다. 이러한 경우에 대하여 다음과 같은 모형을 생각할 수 있다.\n\\[\ny_i = \\beta_0 +e_i, \\quad e_i \\sim (0,\\sigma^2)\n\\tag{1.7}\\]\n기울기에 대한 회귀계수가 0인 경우에 대한 모형을 식 1.7 과 같이 표현할 수 있으며 평균 모형(mean model)이라고 부른다. 평균 모형은 우리가 생각할 수 있는 모형 중에서 가장 간단한, 하지만 별로 쓸모없는 모형이라고 할 수 있다.\n이러한 평균 모형에 대한 최소제곱법을 적용하여 \\(\\beta_0\\)의 추정량을 구하면 추정량 \\(\\hat \\beta_0\\)는 \\(\\bar y\\)가 된다. 그 이유는 위의 모형에 오차제곱합을 구해보면 다음과 같은 형식이 된다\n\\[ S(\\beta_0)=  \\sum^n_{i=1}[y_i-\\beta_0]^2 \\]\n여기서 \\(\\beta_0\\)에 대하여 최고로 하는 지점을 찾아보면 다음과 같은 방정식을 얻을 수 있다.\n\\[\n\\frac{\\partial S(\\beta_0)}{\\partial \\beta_0}  = 0  \\Rightarrow  \\sum^n_{i=1}[y_i - \\beta_0] = 0\n\\]\n이 방정식을 풀면 \\(\\hat \\beta_0 = \\bar y\\)가 됨을 알 수 있다. 결국 설명변수가 반응변수에 아무른 영향을 주지 못하게 되면 \\(y\\)의 예측값은 평균 \\(\\bar y\\) 임을 알 수 있다. 참고로 평균모형 식 1.7 경우 \\(\\bar y\\)는 \\(\\beta_0\\) 의 최소제곱추정량이다.\n여기서 주목해야할 점은 평균 모형 식 1.7 에서의 잔차 \\(r_{0i}\\)는 다음과 같이 정의된다.\n\\[ r_{0i} = y_i -\\hat \\beta_0 = y_i - \\bar y \\]\n주어진 회귀식이 유의한 경우, 즉 회귀식의 기울기가 0이 아닌 경우 (\\(\\beta_1 \\ne 0\\)) 적합된 회귀식에 대한 잔차는 식 1.6 과 같이 나타난다. 만약 회귀식이 유의하다면 식 1.6 으로 구해진 잔차 \\(r_i = y_i -\\hat \\beta_0 - \\hat \\beta_1 x_i\\) 와 평균 모형에서 구해지는 잔차 \\(r_{0i}  = y_i - \\bar y\\) 간의 어떤 차이가 있을까?\n아래의 그림은 앞의 예제 cars 자료에 대하여 설명변수가 없는 평균 모형(파란 선)과 설명변수가 있는 회귀모형(빨간 선)을 나타낸 그림이다. 잔차는 적합된 직선과 반응 변수 간의 차이를 의미하며 차이의 절대값이 작을 수록 좋은 모형이다.\n\nggplot(cars, aes(x=speed, y=dist)) +\n     geom_point() +\n     labs(x = \"속도\", y = \"거리\") +\n     geom_smooth(method = lm, color='red', se = FALSE) +\n     geom_hline(yintercept = mean(cars$dist), color='blue')\n\n\n\n\n\n\n\n\n잔차의 절대값보다 제곱한 양이 다루기가 쉬우므로(why?) 평균 모형과 회귀 모형의 적합도를 비교하는 양으로서 다음과 같은 각각의 모형에서 나온 두개의 잔차제곱합을 생각할 수 있다.\n먼저 평균 모형은 예측에 사용할 변수가 없는 경우로서 이때의 잔차는 각 관측값에 대한 예측값이 관측값의 평균이다 . 이러한 경우 잔차는 관측값 자체가 가지고 있는 변동으로 생각할 수 있다. 이러한 평균모형에서의 잔차 또는 관측값이 가지고 있는 변동을 총제곱합(Total Sum of Squares ; \\(SST\\))이다\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n r^2_{0i} & = \\sum_{i=1}^n (y_i -\\bar y)^2 \\\\\n  & =  \\text{ Residual Sum of Squares from mean model }  \\\\\n   & = \\text{ Variation of response variables} \\\\\n   & = \\text{ Total Sum of Squares } \\\\\n   & = SST\n\\end{aligned}\n\\]\n이제 설명변수가 있는 회귀모형에서 예측치 \\(\\hat y_i=\\hat \\beta_0 + \\hat \\beta_1 x_i\\)를 고려하면 이 경우의 잔차들의 제곱합은 회귀식의 잔차제곱합(Residual Sum of Squares; \\(SSE\\))이라고 부르며 아래와 같이 정의한다.\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n r_i^2 & = \\sum_{i=1}^n (y_i -\\hat \\beta_0 - \\hat \\beta_1 x_i) \\\\\n  & = \\text{Residual Sum of Squares from linear regression model } \\\\\n  & = \\text{ Residual Sum of Squares } \\\\\n  & = SSE\n\\end{aligned}\n\\]\n만약 회귀식에서 고려한 설명변수가 반응변수를 예측하는데 매우 적합하다면 회귀모형에서 구한 잔차들의 제곱합이 평균모형에서 구한 잔차들의 제곱합보다 작을 것이다. 이러한 차이를 비교하려면 두 제곱합 \\(SST\\) 와 \\(SSE\\)의 관계를 이해하는 것이 중요하다.\n두 제곱합 \\(SST\\) 와 \\(SSE\\)의 관계를 보기 위하여 먼저 두 잔차 \\(r^0_i\\) 와 \\(r_i\\)의 차이를 비교해 보자\n\\[ r^0_i - r_i = (y_i - \\bar y) - (y_i - \\hat y_i) = \\hat y_i - \\bar y \\]\n위의 식에서 두 잔차의 차이 \\(\\hat y_i - \\bar y\\)는 예측값과 평균 간 차이로서 그 절댸값이 크면 회귀직선이 반응변수를 설명할 수 있는 능력이 크다는 것을 의미한다.\n위의 식을 다시 쓰면 다음과 같다.\n\\[  (y_i - \\bar y) = (y_i - \\hat y_i) + (\\hat y_i - \\bar y) \\]\n즉, (평균모형의 잔차)=(회귀모형의 잔차))+(회귀모형의 설명부분)으로 분해되는 것으로 이해할 수 있다. 이 분해에서 회귀모형의 잔차가 작을수록 회귀 모형의 예측 능력, 즉 적합도가 커지는 것을 알 수 있다.\n이제 총제곱합은 다음과 같이 분해할 수 있다.\n\\[\n\\begin{aligned}\n\\sum^n_{i=1}(y_i - \\bar y)^2 &=  \\sum^n_{i=1}[(y_i-\\hat y_i)+(\\hat y_i - \\bar y)]^2 \\\\\n&= \\sum^n_{i=1}(y_i-\\hat y_i)^2+\\sum^n_{i=1}(\\hat y_i - \\bar y)^2\n        + 2\\sum^n_{i=1}(y_i-\\hat y_i)(\\hat y_i-\\bar y)  \\\\\n&= \\sum^n_{i=1}(y_i-\\hat y_i)^2+\\sum^n_{i=1}(\\hat y_i - \\bar y)^2 + 0 \\quad \\text{(why?)}\n\\end{aligned}\n\\]\n따라서 다음과 같은 제곱합의 분해를 얻게 된다.\n\\[\n\\sum^n_{i=1}(y_i - \\bar y)^2 = \\sum^n_{i=1}(y_i-\\hat y_i)^2+\\sum^n_{i=1}(\\hat y_i - \\bar y)^2\n\\] 여기서 모형제곱합(regression sum of square; SSR)를 다음과 같이 정의하면\n\\[  SSR = \\sum^n_{i=1}(\\hat y_i-\\bar y_i)^2 \\]\n총제곱합은 잔차제곱합 과 모형제곱합으로 분해된다.\n\\[\nSST = SSE + SSR\n\\tag{1.8}\\]\n관측값들이 보여주는 총 변동인 총제곱합(SST)에서 회귀모형으로 설명할 수 있는 변동, 즉 모형제곱합(SSR)이 차지하는 비율을 결정계수(coefficient of determination)라 하며 \\(R^2\\)으로 표현한다.\n\\[\nR^2 = \\frac{SSR}{SST} =  1 -\\frac{SSE}{SST}  =1- \\frac{\\sum^n_{i=1}(y_i-\\hat y_i)^2}{ \\sum^n_{i=1}(y_i - \\bar y)^2}\n\\]\n위에서 정의된 \\(R^2\\) 는 평균 모형의 잔차제곱합 \\(SST\\)과 회귀모형의 잔차제곱합 \\(SSE\\)의 비율로 정의되는 것으로 해석할 수 있다. 즉,\n\\[  \nR^2 = 1 -\\frac{SSE}{SST} =1 -\\frac{\\text{Residual SS from regression model}}{\\text{Residual SS from mean model}}\n\\]\n결정계수의 정의를 보면 회귀모형의 잔차제곱합(\\(SSE\\))가 평균 모형의 잔차제곱합(\\(SST\\))에 대하여 상대적으로 작아질수록 결정계수가 커진다. 결정계수 \\(R^2\\)는 언제나 0 이상 1 이하의 값을 갖는다. 회귀모형이 데이터에 아주 잘 적합되면 결정계수의 값은 1 에 가깝게 된다.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#중회귀-모형",
    "href": "qmd/lse.html#중회귀-모형",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.5 중회귀 모형",
    "text": "1.5 중회귀 모형\n일반적으로 회귀모형에서 반응변수의 수는 하나인 경우가 많지만 설명변수의 수는 여러 개인 경우가 많다. 이런 경우 중회귀 모형(multiple linear regression)은 다음과 같이 표현할 수 있고, \\(p-\\)개의 설명변수가 있다고 가정하고 \\((x_1, x_2, \\cdots, x_{p-})\\) 표본의 크기 \\(n\\)인 자료가 얻어지면 선형회귀식을 행렬로 다음과 같이 표현할 수 있다.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{p} x_{i,p} +  e_i = \\pmb x^t_i \\pmb \\beta +  e_i\n\\tag{1.9}\\]\n위의 식을 다시 표현하면 다음과 같이 쓸 수 있다.\n\\[\ny_i  = \\pmb x^t_i \\pmb \\beta  + e_i  =\n\\begin{bmatrix}\n1 & x_{i1} & x_{i2} & \\cdots & x_{i,p}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_{0} \\\\\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\vdots \\\\\n\\beta_{p}\n\\end{bmatrix}\n+ e_i\n\\]\n이제 \\(n\\)개의 관측치 \\(y_1,y_2, \\dots, y_n\\) 으로 이루어진 관측값 벡터 \\(\\pmb y\\)를 고려하면 n개의 관측치에 대한 회귀식을 행렬식으로 다음과 같이 표현할 수 있다.\n\\[\n\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_{11} & \\cdots & x_{1,p} \\\\\n1 & x_{21} & \\cdots & x_{2,p} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & x_{n1} & \\cdots & x_{n,p}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_{0} \\\\\n\\beta_{1} \\\\\n\\vdots \\\\\n\\beta_{p}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\ne_{1} \\\\\ne_{2} \\\\\n\\vdots \\\\\ne_{n}\n\\end{bmatrix}\n\\]\n위의 식을 간단히 행렬식으로 표시하면 다음과 같다.\n\\[\n\\pmb y = \\pmb X \\pmb \\beta + \\pmb e\n\\tag{1.10}\\]\n위의 행렬식에서 각 벡터와 행렬의 차원은 다음과 같다.\n\n\\(\\pmb y\\): \\(n \\times 1\\)\n\\(\\pmb X\\): \\(n \\times (p+1)\\)\n\\(\\pmb \\beta\\): \\((p+1) \\times 1\\)\n\\(\\pmb e\\): \\(n \\times 1\\)\n\n여기서 회귀분석의 오차항 \\(e_i\\)은 서로 설명이고 동일한 분산을 갖는다. 즉, 오차항은 다음의 분포를 따른다.\n\\[ \\pmb e  \\sim (\\pmb 0,\\sigma^2 \\pmb I_n) \\]\n따라서 관측값 벡터 \\(\\pmb y\\)의 평균은 다음과 같고\n\\[\nE(\\pmb y|\\pmb X) = E(\\pmb X \\pmb \\beta+\\pmb e)= \\pmb X \\pmb \\beta + E(\\pmb e) = \\pmb X \\pmb \\beta\n\\tag{1.11}\\]\n\\(\\pmb y\\)의 분산은 아래와 같이 주어진다.\n\\[\nV( \\pmb y| \\pmb X) = E[( \\pmb y -  \\pmb X \\pmb \\beta)( \\pmb y -  \\pmb X \\pmb \\beta)^t] = E( \\pmb  e   \\pmb  e^t) = \\sigma^2 \\pmb I_n\n\\tag{1.12}\\]\n여기서 오차항이 정규분포를 따른다면\n\\[ \\pmb  e   \\sim N(\\pmb 0,\\sigma^2 \\pmb I_n) \\]\n관측값 벡터 \\(\\pmb y\\) 또한 정규분포를 따른다\n\\[  \\pmb y \\sim N( \\pmb X \\pmb \\beta, \\sigma^2 \\pmb I_n) \\]\n\n1.5.1 최소제곱추정\n이제 중회귀모형 식 1.10 에서 회귀계수벡터 \\(\\pmb \\beta\\)의 추정량을 구하기 위하여 최소제곱법을 적용해보자.\n\\[\n\\min_{\\pmb \\beta} \\sum_{i=1}^n (y_i -  \\pmb x_i^t \\pmb \\beta )^2 = \\min_{\\pmb \\beta } ( \\pmb y -  \\pmb X \\pmb \\beta )^t( \\pmb y -  \\pmb X \\pmb \\beta )\n\\tag{1.13}\\]\n\n1.5.1.1 방법 1\n\\(\\hat {\\pmb \\beta}\\)는 잔차의 제곱합 식 1.13 을 최소로 하는 최소제곱 추정량이다. 잔차의 제곱합을 \\(S( \\pmb \\beta)\\)이라고 하면\n\\[\n\\begin{aligned}\nS( \\pmb \\beta ) & =  ( \\pmb y -  \\pmb X \\pmb \\beta)^t( \\pmb y -  \\pmb X \\pmb \\beta ) \\notag \\\\\n  & = \\pmb y^t \\pmb y - \\pmb y^t \\pmb X \\pmb \\beta - \\pmb \\beta^t \\pmb X^t \\pmb y\n    + \\pmb \\beta^t \\pmb X^t \\pmb X \\pmb \\beta \\notag \\\\\n  & = \\pmb y^t \\pmb y -2  \\pmb \\beta^t \\pmb X^t \\pmb y\n    + \\pmb \\beta^t \\pmb X^t \\pmb X \\pmb \\beta\n\\end{aligned}\n\\tag{1.14}\\]\n여기서 \\(S( \\pmb \\beta)\\)를 최소로 하는 회귀계수벡터의 값을 구하기 위하여 \\(S( \\pmb \\beta)\\)를 회귀계수벡터 \\(\\pmb \\beta\\)로 미분한후 \\(\\pmb 0\\) 으로 놓고 선형 방정식을 풀어야 한다.\n벡터미분을 이용하면\n\\[\n\\begin{aligned}\n\\pardifftwo{ S( {\\pmb \\beta})}{\\pmb \\beta} & =\n\\pardifftwo{}{\\pmb \\beta} (\\pmb y^t \\pmb y -2  \\pmb \\beta^t \\pmb X^t \\pmb y\n    + \\pmb \\beta^t \\pmb X^t \\pmb X \\pmb \\beta) \\\\\n& = \\pmb 0 -2 \\pmb X^t \\pmb y + 2 \\pmb X^t \\pmb X \\pmb \\beta \\\\\n& =\\pmb 0\n\\end{aligned}\n\\]\n최소제곱 추정량을 구하기 위한 정규방정식은 다음과 같이 쓸 수 있다.\n\\[\n\\pmb X^t \\pmb X \\pmb \\beta =  \\pmb X^t \\pmb y\n\\tag{1.15}\\]\n방정식 식 1.15 를 정규방정식(normal equation)이라고 한다. 만약 \\(\\pmb X^t \\pmb X\\)가 정칙행렬일 경우 최소제곱법에 의한 회귀계수 추정량 \\(\\hat {\\pmb \\beta}\\) 다음과 같다.\n\\[\n\\hat {\\pmb \\beta} = ( \\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y\n\\tag{1.16}\\]\n예측값 벡터 \\(\\hat {\\pmb y}\\) 는 \\(E(\\pmb y | \\pmb X)\\)의 추정치로서 다음과 같다.\n\\[ \\hat E(\\pmb y | \\pmb X)= \\hat {\\pmb y} = \\pmb X \\hat {\\pmb \\beta} = \\pmb  X(\\pmb X^t \\pmb X)^{-1} \\pmb X^t y \\]\n만약 \\(\\pmb X^t \\pmb X\\)가 정칙행렬이 아닐 경우 최소제곱법에 의한 회귀계수 추정량 \\(\\hat {\\pmb \\beta}\\)은 \\(\\pmb X^t \\pmb X\\)의 일반화 역행렬 \\((\\pmb X^t \\pmb X)^-\\)를 이용하여 다음과 같이 구한다. 이 경우 일반화 역행렬이 유일하지 않기 때문에 회귀계수 추정량도 유일하지 않다.\n\\[\n  \\hat {\\pmb \\beta} = ( \\pmb X^t \\pmb X)^{-} \\pmb X^t \\pmb y\n\\]\n\n\n1.5.1.2 방법 2\n식 1.13 에서 나오는 오차벡터를 정의하고 \\(\\pmb e = (\\pmb y - \\pmb X \\pmb \\beta)\\) 오차벡터를 모수벡터 \\(\\pmb \\beta\\)로 미분하면 다음과 같은 결과를 얻는다.\n\\[\n\\pardifftwo{\\pmb e}{\\pmb \\beta} = \\pardifftwo{ (\\pmb y - \\pmb X \\pmb \\beta)}{ \\pmb \\beta} =\n- \\pardifftwo{ \\pmb X \\pmb \\beta}{ \\pmb \\beta} \\equiv\n- \\pardifftwo{\\pmb \\beta^t \\pmb X^t }{\\pmb \\beta} = -\\pmb X^t\n\\]\n이제 오차제곱합 \\(S( {\\pmb \\beta})=\\pmb e^t \\pmb e\\) 를 모수벡터로 미분하면 이차형식의 미분공식과 합성함수 미분공식을 차례로 적용하면 된다.\n\\[  \n\\pardifftwo{ S( {\\pmb \\beta})}{\\pmb \\beta}=\\pardifftwo{\\pmb e^t \\pmb e}{\\pmb \\beta} =  \\pardifftwo{\\pmb e }{\\pmb \\beta} \\pardifftwo{\\pmb e^t  \\pmb e}{\\pmb e} = -\\pmb X^t \\left( 2 \\pmb e \\right ) = -2 \\pmb X^t (\\pmb y - \\pmb X \\pmb \\beta)  \n\\]\n위의 방정식을 \\(\\pmb 0\\)으로 놓으면 최소제곱 추정량 (열)벡터를 구한다.\n\\[\n\\pmb X^t \\pmb y - \\pmb X^t  \\pmb X \\pmb \\beta = \\pmb 0 \\quad \\rightarrow \\quad\n\\hat{\\pmb \\beta}  = (\\pmb X^t  \\pmb X)^{-1} \\pmb X^t  \\pmb y\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#예제-중회귀모형",
    "href": "qmd/lse.html#예제-중회귀모형",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.6 예제-중회귀모형",
    "text": "1.6 예제-중회귀모형\n\n보기 1.3 (중고차 가격자료) 강의 부교재의 usedcars 자료를 이용하여 중회귀모형을 적합해보자. 자료를 구성하는 변수는 다음과 같다.\n\nprice : 자동차 가격\nyesr :연식\nmileage : 주행거리\ncc : 엔진 크기\nautomatic : 자동 변속기 여부\n\n\nusedcars %&gt;% head(n=10) \n\n   price year mileage   cc automatic\n1    790   78  133462 1998         1\n2   1380   39   33000 2000         1\n3    270  109  120000 1800         0\n4   1190   20   69727 1999         1\n5    590   70  112000 2000         0\n6   1120   58   39106 1998         1\n7    815   53   95935 1800         1\n8    450   68  120000 1800         0\n9   1290   15   20215 1798         1\n10   420   96  140000 1800         0\n\n\n자동차의 가격을 반응변수로 한고 나머지 변수를 설명변수로 설정한 중회귀 모형에 대한 모수의 추정 결과는 다음과 같다.\n\nusedcars_lm &lt;- lm(price ~ year + mileage + cc + automatic, data=usedcars)\nsummary(usedcars_lm)\n\n\nCall:\nlm(formula = price ~ year + mileage + cc + automatic, data = usedcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-177.35  -63.91   -0.99   70.34  212.69 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.253e+02  3.998e+02   1.314 0.200823    \nyear        -5.800e+00  9.283e-01  -6.247 1.55e-06 ***\nmileage     -2.263e-03  7.211e-04  -3.138 0.004324 ** \ncc           3.888e-01  2.022e-01   1.923 0.065958 .  \nautomatic    1.653e+02  3.986e+01   4.147 0.000339 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 101.1 on 25 degrees of freedom\nMultiple R-squared:  0.9045,    Adjusted R-squared:  0.8892 \nF-statistic: 59.21 on 4 and 25 DF,  p-value: 2.184e-12",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#최소제곱-추정량의-분포",
    "href": "qmd/lse.html#최소제곱-추정량의-분포",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.7 최소제곱 추정량의 분포",
    "text": "1.7 최소제곱 추정량의 분포\n회귀식을 추정하기 위한 회귀계수 추정값인 \\(\\hat {\\pmb \\beta}\\)의 분포를 알아보기 위해서 우선 선형추정량을 보면 다음과 같다.\n\\[ \\hat {\\pmb \\beta}  = (\\pmb X^t\\pmb X)^{-1}\\pmb X' \\pmb y \\equiv  \\pmb M \\pmb y \\] 따라서 최소제곱 추정량은 관측값들의 선형 변환이다. 회귀계수 추정값 \\(\\hat {\\pmb \\beta}\\)의 기대값은\n\\[\n\\begin{aligned}\nE( \\hat {\\pmb \\beta}  ) &= E( \\pmb M \\pmb y) = E(( \\pmb X^t \\pmb X)^{-1} \\pmb X' \\pmb y) \\\\\n&= ( \\pmb X^t \\pmb X)^{-1} \\pmb X'E( \\pmb y) \\\\\n&= ( \\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb X \\pmb \\beta \\\\\n  &= \\pmb \\beta\n\\end{aligned}\n\\]\n따라서 최소제곱 추정량 \\(\\hat {\\pmb \\beta}\\)는 \\(\\pmb \\beta\\)의 불편추정량이다. 최소제곱 추정량 \\(\\hat {\\pmb \\beta}\\)의 공분산 행렬을 전개해보면\n\\[\n\\begin{aligned}\nVar( \\hat {\\pmb \\beta} ) &= Var(( \\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y) \\\\\n&= ( \\pmb X^t \\pmb X)^{-1} \\pmb X^t ~ Var( \\pmb y) ~ \\pmb X ( \\pmb X^t \\pmb X)^{-1} \\\\\n&=( \\pmb X^t \\pmb X)^{-1} \\pmb X^t[\\sigma^2  \\pmb I_n] \\pmb X( \\pmb X^t \\pmb X)^{-1} \\\\\n&= \\sigma^2( \\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb X( \\pmb X^t \\pmb X)^{-1} \\\\\n&= \\sigma^2( \\pmb X^t \\pmb X)^{-1} \\\\\n\\end{aligned}\n\\]\n위에서 최소제곱 추정량의 평균과 공분산을 구할 때에는 정규성 가정이 필요하지않다. 만일 \\(\\pmb y\\)가 정규분포를 따른다면 \\(\\pmb y\\)의 선형변환으로부터 얻어진 \\(\\hat {\\pmb \\beta}\\)의 분포는 정규분포이며 다음과 같다.\n\\[  \\hat {\\pmb \\beta}  \\sim N \\left (\\pmb \\beta, \\sigma^2( \\pmb X^t \\pmb X)^{-1} \\right ) \\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#가우스-마코브-정리",
    "href": "qmd/lse.html#가우스-마코브-정리",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.8 가우스-마코브 정리",
    "text": "1.8 가우스-마코브 정리\n\n정리 1.1 (가우스-마코브 정리) 선형회귀모형 \\(\\pmb y =  \\pmb X  \\pmb \\beta +  \\pmb e\\)에서 \\(E( \\pmb e)=0, Var( \\pmb e)=\\sigma^2 \\pmb I\\)이 성립하면 최소제곱 추정량\n\\[ \\hat{\\pmb \\beta}=(\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y\\]\n는 \\(\\pmb \\beta\\)의 최소분산 선형 불편추정량이다.\n\n위의 정리를 가우스-마코브 정리 (Gauss-Markov Theorem)라고 하며 이는 회귀계수 \\(\\pmb \\beta\\)의 모든 선형 불편 추정량들 중에 최소제곱 추정량 \\(\\hat {\\pmb \\beta}=(\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y\\)이 가장 작은 분산을 가짐을 뜻한다 (Best Linear Unbiased Estimator; BLUE).\n가우스-마코브 정리를 정확하게 표현하면 \\(E(\\pmb L \\pmb y) = \\pmb \\beta\\)를 만족하는 모든 \\(n \\times n\\) 차원의 행렬 \\(\\pmb L\\)과 임의의 벡터 \\(\\pmb c\\)에 대하여 다음이 성립한다.\n\\[ V(\\pmb c^t \\hat {\\pmb \\beta}) \\le V(\\pmb c^t \\pmb L \\pmb y)  \\]\n이제 가우스-마코브 정리를 증명해보자. 관측벡터 \\(\\pmb y\\)에 대한 임의의 선형 추정량 \\(\\pmb \\beta^* = \\pmb L \\pmb y\\)를 생각해보면 다시 다음의 형태로 표시할 수 있다.\n\\[  \n\\pmb \\beta^* =  \\pmb L  \\pmb y = (\\pmb M + \\pmb L -\\pmb M ) \\pmb y = ( \\pmb M +  \\pmb A)  \\pmb y\n\\]\n여기서 \\(\\pmb M = ( \\pmb X^t \\pmb X)^{-1} \\pmb X^t\\) 이고 \\(\\pmb A= \\pmb L- \\pmb M\\) 이다. 임의의 선형 추정량 \\(\\pmb \\beta^*\\)가 불편 추정량일 조건을 구해보자\n\\[\n\\begin{aligned}\nE( \\pmb \\beta^*) & = E[( \\pmb M+ \\pmb A) \\pmb y] \\\\\n            & = ( \\pmb M+ \\pmb A)E( \\pmb y) \\\\\n            & = ( \\pmb M+ \\pmb A)X \\pmb \\beta \\\\\n            & = ( \\pmb X^t  \\pmb X)^{-1} \\pmb X^t  \\pmb X  \\pmb \\beta +  \\pmb A  \\pmb X  \\pmb \\beta \\\\\n            & =  \\pmb \\beta+AX \\pmb \\beta\\\\\n\\end{aligned}\n\\]\n여기서 불편추정량이 되기 위해서는 \\(E( \\pmb \\beta^*)= \\pmb \\beta\\) 조건을 만족 해야되며 따라서 \\(\\pmb A \\pmb X=0\\)이되어야한다 (이 조건은 \\(\\pmb A=0\\)를 의미하는 것은 아니다).\n이제 최소분산을 가지기 위해서 \\(\\pmb A \\pmb X=0\\)을 만족하는 행렬 \\(\\pmb A\\)중에서 \\(Var( \\pmb \\beta^*)\\)을 최소로하는 행렬 \\(\\pmb A\\)를 구해야 한다. \\(\\pmb \\beta^*\\)의 공분산 행렬은 \\(AX=0\\)이므로\n\\[\n\\begin{aligned}\nV( \\pmb \\beta^*) & = ( \\pmb M+ \\pmb A)V( \\pmb y)( \\pmb M+ \\pmb A)^t\\\\\n            & = ( \\pmb M+ \\pmb A)\\sigma^2 I_n( \\pmb M+ \\pmb A)^t\\\\\n            & = \\sigma^2 ( \\pmb M \\pmb M^t+ \\pmb A \\pmb M^t+ \\pmb M  \\pmb A^t+ \\pmb A  \\pmb A^t)\\\\\n            & = \\sigma^2 [ ( \\pmb X^t  \\pmb X)^{-1}  \\pmb X^t  \\pmb X( \\pmb X^t \\pmb X)^{-1} + \\pmb A  \\pmb X ( \\pmb X^t  \\pmb X)^{-1}+( \\pmb X^t  \\pmb X)^{-1} \\pmb X^t  \\pmb A^t+  \\pmb A  \\pmb A^t ]\\\\\n            & = \\sigma^2[( \\pmb X^t  \\pmb X)^{-1} +  \\pmb A  \\pmb A^t ]  \\\\\n            & = V( \\hat {\\pmb \\beta} ) + \\sigma^2 \\pmb A \\pmb A^t\\\\\n\\end{aligned}\n\\]\n이제 임의의 벡터 \\(\\pmb c\\)에 대하여\n\\[\n\\begin{aligned}\nV( \\pmb c^t \\pmb \\beta^*)  & = \\pmb c^t V (\\pmb \\beta^*) \\pmb c \\\\\n   & =  \\pmb c^t V( \\hat {\\pmb \\beta} ) \\pmb c + \\sigma^2 \\pmb c^t \\pmb A \\pmb A^t \\pmb c \\\\\n    & =  V( \\pmb c^t  \\hat {\\pmb \\beta} ) + \\sigma^2 \\pmb c^t \\pmb A \\pmb A^t \\pmb c\n\\end{aligned}\n\\]\n다음이 성립하므로\n\\[ \\pmb c^t \\pmb A \\pmb A^t \\pmb c = \\pmb u^t \\pmb u = \\sum_{i=1}^n u_i^2 \\ge 0 \\]\n임의의 벡터 \\(\\pmb c\\)에 대하여\n\\[ V( \\pmb c^t \\pmb \\beta^*) \\ge  V( \\pmb c^t  \\hat {\\pmb \\beta} ) \\]\n이제 \\(V( \\pmb c^t \\pmb \\beta^*)\\) 이 \\(V( \\pmb c^t \\hat {\\pmb \\beta} )\\)과 같으려면 다음 조건이 성립해야 하며\n\\[ \\pmb u = \\pmb c^t \\pmb A = \\pmb 0 \\]\n임의의 모든 벡터 \\(\\pmb c\\)에 대해서 위의 조건 성립해야 하므로 이는 \\(\\pmb A = \\pmb 0\\) 이 성립해야 한다. 또한 이조건은 \\(\\pmb A \\pmb X=0\\)도 만족 시켜준다. 따라서 \\(\\pmb \\beta\\)의 최소분산 선형 불편추정량은 최소제곱법으로 구한 추정량이다.\n여기서 주의할 점은 가우스-마코브 정리에서 관측값 \\(\\pmb y\\)에 대한 가정은 평균과 공분산의 가정만 주어졌으며 \\(\\pmb y\\)의 분포에 대한 가정이 없다. 참고로 만약에 \\(\\pmb y\\)가 정규분포를 따른다면 최소제곱 추정량은 최소분산 불편추정량이다.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#최대가능도-추정",
    "href": "qmd/lse.html#최대가능도-추정",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.9 최대가능도 추정",
    "text": "1.9 최대가능도 추정\n관측값 벡터 \\(\\pmb y\\) 가 다음과 같이 선형모형이며 정규분포를 따른다고 가정하자.\n\\[\n\\pmb y \\sim N( \\pmb X \\pmb \\beta, \\sigma^2 \\pmb I_n)\n\\tag{1.17}\\]\n선형모형 식 1.17 에 대한 가능도 함수는 다음과 같이 주어진다.\n\\[\n\\begin{aligned}\nL_n( \\pmb \\theta ;  \\pmb y) & = L( \\pmb \\beta,\\sigma^2|  \\pmb y) \\\\\n   & = \\prod^n_{i=1} f(y_i)\\\\\n   & = \\prod^n_{i=1}(2 \\pi \\sigma^2)^{-\\frac{1}{2}} \\exp \\left [-\\frac{1}{2\\sigma^2} (y_i- {\\pmb x}_i^t \\pmb \\beta)^2 \\right ] \\\\\n   & = (2\\pi\\sigma^2)^{-\\frac{n}{2}} \\exp \\left [ -\\frac{1}{2\\sigma^2}( \\pmb y- \\pmb  X  \\pmb \\beta)^t( \\pmb y- \\pmb X  \\pmb \\beta) \\right ]\n\\end{aligned}\n\\]\n또한 분산에 대한 모수를 \\(\\tau=\\sigma^2\\) 과 같이 쓰면 로그 가능도함수는 다음과 같다.\n\\[\n\\begin{aligned}\n\\ell_n( \\pmb \\theta; \\pmb y) & = -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\sigma^2 -\\frac { ( \\pmb y- \\pmb X  \\pmb \\beta)^t (\\pmb  y- \\pmb X  \\pmb \\beta) }{2\\sigma^2} \\\\\n   &= -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac { ( \\pmb y- \\pmb X  \\pmb \\beta)^t ( \\pmb y- \\pmb X  \\pmb \\beta) }{2\\tau}  \n\\end{aligned}\n\\]\n이제 로그가능도함수로부터 구할 수 있는 스코어함수 \\(s( \\pmb \\theta;\\pmb y)\\) 와 그에 대한 관측 피셔정보 \\(J_n( \\pmb \\theta; \\pmb y)\\) 은 다음과 같이 주어진다.\n\\[\n\\begin{aligned}\ns( \\pmb \\theta;  \\pmb y) & =  \\pardifftwo{}{ \\pmb \\theta}\\ell_n( \\pmb \\theta;  \\pmb y ) \\\\\n  & =  \\begin{bmatrix}\n    \\pardifftwo{}{ \\pmb  \\beta}\\ell_n( \\pmb \\theta;  \\pmb y ) \\\\\n    \\pardifftwo{}{\\tau}\\ell_n( \\pmb \\theta;  \\pmb y )\n  \\end{bmatrix} \\\\\n  & =\n  \\begin{bmatrix}\n     \\pmb X^t ( \\pmb y- \\pmb X  \\pmb \\beta)/\\tau \\\\\n    -\\frac{n}{2\\tau} +\\frac { ( \\pmb y- \\pmb X  \\pmb \\beta)^t ( \\pmb y- \\pmb X  \\pmb \\beta) }{2\\tau^2}\n  \\end{bmatrix}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nJ_n( \\pmb \\theta;  \\pmb y) & =  -\\pardiffdd{}{ \\pmb \\theta}{ {\\pmb \\theta}} \\ell_n( \\pmb \\theta;\\pmb y ) \\\\\n   & =\n  - \\begin{bmatrix}\n    \\pardiffdd{}{\\pmb \\beta}{ {\\pmb \\beta}}\\ell_n( \\pmb \\theta;\\pmb y ) & \\pardiffdd{}{ \\pmb \\beta}{\\tau} \\ell_n( \\pmb \\theta;\\pmb y )  \\\\\n    \\pardiffdd{}{\\tau}{ {\\pmb \\beta}} \\ell_n( \\pmb \\theta;\\pmb y )  & \\pardiffdd{}{\\tau}{\\tau}\\ell_n( \\pmb \\theta; \\pmb y )\n  \\end{bmatrix} \\\\\n   & =\n  \\begin{bmatrix}\n      {\\pmb X}^t  \\pmb X /\\tau &  - {\\pmb X}^t ( \\pmb y- \\pmb X  \\pmb \\beta)/\\tau^2   \\\\\n    - ( \\pmb y- \\pmb X  \\pmb \\beta)^t {\\pmb X} /\\tau^2  &  - \\frac{n}{2\\tau^2} +\\frac { ( \\pmb y- \\pmb X \\pmb  \\beta)^t ( \\pmb y- \\pmb X  \\pmb \\beta) }{\\tau^3}\n  \\end{bmatrix}\n\\end{aligned}\n\\]\n이제 중회귀모형에서 회귀계수 \\(\\pmb \\beta\\)에 대한 최대가능도 추정량은 스코어함수로 부터 얻어진 방정식 \\(s( \\pmb \\theta; y)= 0\\) 으로부터 얻어지며 다음과 같은 형태를 가진다.\n\\[ \\hat { \\beta} = ( {\\pmb X}^t  \\pmb X)^{-1} {\\pmb  X}^t  \\pmb y \\]\n\\[   \\hat \\sigma^2 = \\hat \\tau = ( \\pmb y-\\pmb  X  \\hat {\\pmb \\beta})^t ( \\pmb y- \\pmb X  \\hat {\\pmb \\beta})/n = \\frac{SSE(\\hat{\\pmb  \\beta})}{n} \\]\n여기서 유의할 점은 회귀계수 \\(\\pmb \\beta\\) 의 최대가능도 추정량은 최소제곱법으로 구한 추정량과 동일하다. 따라서 \\(\\hat {\\pmb  \\beta}\\)은 최소분산 불편 추정량이다. 하지만 오차항의 분산 \\(\\sigma^2\\) 에 대한 최대가능도 추정량은 불편추정량이 아니다.\n\\[ E(\\hat \\sigma^2)  = E \\left [ ( \\pmb y- \\pmb X  {\\hat {\\pmb \\beta}})^t ( \\pmb y-\\pmb  X  {\\hat {\\pmb \\beta}})/n \\right ] = E \\left [ \\frac{SSE}{n} \\right ]  \\ne \\sigma^2 \\]\n참고로 오차항의 분산 \\(\\sigma^2\\)에 대한 불편추정량은 \\(SSE/(n-p)\\)이다. 오차항의 분산에 대한 불편추정량은 다음 장에서 논의할 것이다.\n최대가능도 추정량의 점근적 분포를 이용하면 다음과 같이 말할 수 있다. 오차항이 정규분포인 선형모형인 경우 아래의 분포는 점근분포가 아닌 정확한 분포이다.\n\\[ \\hat { \\pmb \\theta}  \\sim  N( \\pmb \\theta ,  I_n^{-1}( \\pmb \\theta)) \\]\n여기서\n\\[  I_n( \\pmb \\theta) = E[ J( \\pmb \\theta;  y)] =  \n\\begin{bmatrix}\n      {\\pmb X}^t \\pmb X /\\tau & \\pmb 0   \\\\\n    \\pmb 0^t  &   \\frac{n}{2\\tau^2}\n  \\end{bmatrix}\n\\] 그리고 \\[  I_n^{-1}( \\pmb \\theta) =\n\\begin{bmatrix}\n     \\tau( {\\pmb X}^t  \\pmb X)^{-1}  & \\pmb 0   \\\\\n    \\pmb 0^t  &   \\frac{2\\tau^2}{n}\n  \\end{bmatrix}\n  =\n\\begin{bmatrix}\n     \\sigma^2( {\\pmb X}^t  \\pmb X)^{-1}  & \\pmb 0   \\\\\n    \\pmb 0^t  &   \\frac{2\\sigma^4}{n}\n  \\end{bmatrix}\n\\]\n따라서 회귀계수 추정량 \\(\\hat { \\beta}\\)의 분포는 평균이 \\(\\pmb \\beta\\) 이고 공분산이 \\(\\sigma^2( {\\pmb X}^t - \\pmb X)^{-1}\\) 인 정규분포를 따른다.\n여기거 주목할 점은 가능도함수에 최대가능도추정량을 대입하면 그 값이 \\(SSE(\\hat { \\beta})\\)의 함수로 나타난다.\n\\[\n\\begin{aligned}\nL_n(\\hat { \\pmb \\theta} ) & = L_n(\\hat { \\pmb \\beta} ,\\hat \\sigma^2 ) \\notag \\\\\n&=  (2\\pi\\hat \\sigma^2)^{-\\frac{n}{2}} \\exp \\left [-\\frac{1}{2 \\hat \\sigma^2}( \\pmb y- \\pmb X \\hat { \\pmb \\beta})^t( \\pmb y- \\pmb X \\hat { \\pmb \\beta} ) \\right ] \\notag  \\\\\n& = (2\\pi\\hat \\sigma^2)^{-\\frac{n}{2}} \\exp \\left [-\\frac{n}{2} \\right ]  \\notag  \\\\\n& = \\left (2\\pi \\frac{SSE(\\hat { \\pmb \\beta})}{n} \\right )^{-\\frac{n}{2}} \\exp \\left [-\\frac{n}{2} \\right ]\n\\end{aligned}\n\\tag{1.18}\\] 또한 가능도함수의 값은 다음과 같다.\n\\[\nl_n(\\hat { \\pmb \\theta} ) = l_n(\\hat { \\pmb \\beta} ,\\hat \\sigma^2 )\n= \\text{constant}  - \\frac{n}{2} \\log \\frac{SSE(\\hat { \\pmb \\beta})}{n}\n\\tag{1.19}\\]\n따라서 잔차제곱함 \\(SSE(\\hat { \\pmb \\beta})\\) 작아지면 가능도함수는 커진다.\n앞 절에서 언급한 평균 모형 식 1.7 에서 최대가능도 추정을 알아보자. 관측값 벡터는 다음과 같은 분포를 따른다.\n\\[\n\\pmb y \\sim N( \\beta_0 \\pmb 1 , \\sigma^2 \\pmb I_n)\n\\tag{1.20}\\]\n선형모형 식 1.17 에 대한 로그 가능도 함수는 다음과 같이 주어진다. 분산에 대한 모수를 \\(\\tau=\\sigma^2\\) 로 바꾸어 사용하면 모수 벡터는 \\(\\pmb \\theta = (\\beta_0, \\tau)^t\\)이다.\n\\[\n\\ell_n( \\pmb \\theta; \\pmb y) = -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac { ( \\pmb y- \\beta_0 \\pmb 1  )^t ( \\pmb y-  \\beta_0 \\pmb 1) }{2\\tau}  \n\\]\n두 개의 모수 \\(\\beta_0\\)와 \\(\\tau\\)에 대하여 미분하여 가능도 방정식을 구하면 다음과 같다.\n\\[\n\\begin{aligned}\ns( \\pmb \\theta;  \\pmb y) & =  \\pardifftwo{}{ \\pmb \\theta}\\ell_n( \\pmb \\theta;  \\pmb y ) \\\\\n  & =  \\begin{bmatrix}\n    \\pardifftwo{}{ \\beta_0}\\ell_n( \\pmb \\theta;  \\pmb y ) \\\\\n    \\pardifftwo{}{\\tau}\\ell_n( \\pmb \\theta;  \\pmb y )\n  \\end{bmatrix} \\\\\n  & =\n  \\begin{bmatrix}\n     \\pmb 1^t ( \\pmb y-  \\beta_0 \\pmb 1)/\\tau \\\\\n    -\\frac{n}{2\\tau} +\\frac { ( \\pmb y- \\beta_0 \\pmb 1))^t ( \\pmb y- \\beta_0 \\pmb 1) }{2\\tau^2}\n  \\end{bmatrix} \\\\\n  & = \\pmb 0\n\\end{aligned}\n\\]\n위의 방정식을 풀면 다음과 같은 최대가능도 추정량을 구할 수 있다.\n\\[\n\\hat \\beta_0 = \\bar y, \\quad {\\hat \\sigma}^2 = \\frac{\\sum_{i=1}^n (y_i - \\bar y)^2}{n} = \\frac{SST}{n}\n\\]\n그리고 가능도함수에 최대가능도추정량을 대입하면 그 값이 다음과 같다.\n\\[\nL_n(\\hat { \\pmb \\theta} )  = L_n(\\hat { \\beta_0} ,\\hat \\sigma^2 )\n= \\left (2\\pi \\frac{SST}{n} \\right )^{-\\frac{n}{2}} \\exp \\left [-\\frac{n}{2} \\right ]\n\\tag{1.21}\\]\n두 개의 모형, 즉 선형회귀모형 식 1.17 과 평균모형 식 1.7 의 가능도 함수의 비, 즉 식 1.18 과 식 1.21 의 비율을 구해보면 결정계수 \\(R^2\\)외의 관계를 볼 수 있다.\n\\[  \n\\frac{ L_n(\\hat { \\beta_0} ,\\hat \\sigma^2 )  }{L_n(\\hat { \\pmb \\beta} ,\\hat \\sigma^2 ) }\n= \\left (2\\pi \\frac{SST}{n} \\right )^{-\\frac{n}{2}} / \\left (2\\pi \\frac{SSE}{n} \\right )^{-\\frac{n}{2}}\n\\propto  \\left [ \\frac{SSE}{SST} \\right ]^{\\frac{n}{2}} = \\left [ 1-R^2 \\right ]^{\\frac{n}{2}}  \n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/inference.html",
    "href": "qmd/inference.html",
    "title": "2  선형회귀에서의 추론",
    "section": "",
    "text": "2.1 제곱합의 분포\n앞 장의 중회귀 모형 식 1.10 에서 관측값 벡터 \\(\\pmb y\\)가 다변량 정규분포 \\(N(\\pmb X \\pmb \\beta, \\sigma^2 \\pmb I)\\)를 따를 때 회귀계수의 추정량 \\(\\hat {\\pmb \\beta}=(\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y\\) 은 다음과 같은 분포를 따르는 것을 보였다.\n\\[ \\hat {\\pmb \\beta} \\sim N(\\pmb \\beta, \\sigma^2 (\\pmb X^t  \\pmb X)^{-1}) \\]\n반응변수의 추정값을 구하는 식에서 다음과 같은 모자행렬(hat matrix) \\(\\pmb H = \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t\\) 을 정의하자. 여기서 중요한 점은 모자행렬은 대칭인 멱등행렬 (\\(\\pmb H \\pmb H =\\pmb H\\))이며 이는 모자행렬이 사영행렬임을 의미한다.\n\\[\n\\hat {\\pmb y} = \\pmb X \\hat {\\pmb \\beta} = \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y = \\pmb H \\pmb y\n\\tag{2.1}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>선형회귀에서의 추론</span>"
    ]
  },
  {
    "objectID": "qmd/inference.html#제곱합의-분포",
    "href": "qmd/inference.html#제곱합의-분포",
    "title": "2  선형회귀에서의 추론",
    "section": "",
    "text": "2.1.1 잔차제곱합의 분포\n이제 제곱합들의 분포를 알아보기로 하자. 먼저 잔차제곱합 \\(SSE\\)를 이차 형식으로 표시해보자.\n\\[\n\\begin{aligned}\nSSE & = \\sum_{i=1}^n (y_i - \\hat y_i) \\\\\n   & = (\\pmb y - \\pmb X \\hat {\\pmb \\beta})^t (\\pmb y - \\pmb X \\hat {\\pmb \\beta}) \\\\\n   & = (\\pmb y - \\pmb H \\pmb y)^t (\\pmb y - \\pmb H \\pmb y) \\\\\n   & = \\pmb y^t (\\pmb I - \\pmb H)^t (\\pmb I - \\pmb H) \\pmb y \\\\\n   & = \\pmb y^t (\\pmb I - \\pmb H) (\\pmb I - \\pmb H) \\pmb y \\\\\n   & = \\pmb y^t (\\pmb I - \\pmb H) \\pmb y \\\\\n\\end{aligned}\n\\]\n위의 식에서 \\(\\pmb I - \\pmb H\\)는 멱등행렬이고 다음이 성립한다.\n\\[\n(\\pmb I - \\pmb H) \\pmb X = \\pmb X - \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb X = \\pmb 0\n\\]\n따라서\n\\[\n\\pmb \\mu^t (\\pmb I - \\pmb H)  \\pmb \\mu = \\pmb \\beta^t \\pmb X^t (\\pmb I - \\pmb H) \\pmb X \\beta =0\n\\]\n이므로 비중심 모수는 0이다.\n또한\n\\[\n\\begin{aligned}\nr(\\pmb I - \\pmb H) & = tr(\\pmb I - \\pmb H) \\\\\n& = tr(\\pmb I_n) - tr \\left [ \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\right ] \\\\\n& = n-tr \\left [ (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb X \\right ]\n\\\\\n&= n-tr (\\pmb I_p ) \\\\\n& = n-p\n\\end{aligned}\n\\]\n이므로 부록의 정리에 의하여 \\(SSE\\)는 다음과 같이 중심 카이제곱 분포를 따른다.\n\\[\n\\frac{SSE}{\\sigma^2} \\sim \\chi^2(n-p)\n\\tag{2.2}\\]\n\n\n2.1.2 회귀제곱합의 분포\n다음으로 회귀제곱합 \\(SSR\\)의 분포를 유도해보자.\n\\[\n\\begin{aligned}\nSSR & = \\sum_{i=1}^n (\\hat y_i - \\bar y) \\\\\n   & = (\\pmb X \\hat {\\pmb \\beta} - \\bar y \\pmb 1 )^t (\\pmb X \\hat {\\pmb \\beta} - \\bar y \\pmb 1 ) \\\\\n   & = \\left ( \\pmb X \\hat {\\pmb \\beta} -  \\pmb 1 (\\pmb 1^t \\pmb y)/n \\right )^t   \\left ( \\pmb X \\hat {\\pmb \\beta} -  \\pmb 1 (\\pmb 1^t \\pmb y)/n \\right )\\\\\n   & = \\left ( \\pmb H \\pmb y-  \\tfrac{1}{n} \\pmb 1 \\pmb 1^t \\pmb y \\right )^t   \\left ( \\pmb H \\pmb y-  \\tfrac{1}{n} \\pmb 1 \\pmb 1^t \\pmb y \\right ) \\\\\n    & =  \\pmb y^t \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right )^t   \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right ) \\pmb y \\\\\n    & = \\pmb y^t \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right )   \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right ) \\pmb y \\\\\n     & = \\pmb y^t  \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right ) \\pmb y \\\\\n\\end{aligned}\n\\]\n위의 유도식에서 다음 두 가지 성질을 이용하였다. 첫 번째 성질은 모자행렬이 사영행렬이며 모자행렬이 투영하는 공간은 일벡터 \\(\\pmb 1\\)을 포함한 공간이다. 이는 계획 행렬 \\(\\pmb X\\)의 첫 번째 열이 절편에 대한 값으로 모두 1인 것 때문이다. 따라서\n\\[\n\\begin{aligned}\n\\pmb H \\pmb J & =  \\pmb H  \\pmb 1 \\pmb 1^t \\\\\n   & =  [ \\pmb H  \\pmb 1 ]  \\pmb 1^t \\\\\n  & = \\left [  \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t  \\pmb 1 \\right ] \\pmb 1^t \\\\\n  & =  \\pmb 1 \\pmb 1^t \\\\\n  & = \\pmb J  \n\\end{aligned}\n\\]\n두 번째로 다음과 같이 \\(\\pmb J \\pmb J = n \\pmb J\\)이므로 \\(\\tfrac{1}{n} \\pmb J\\)는 멱등행렬이다.\n\\[\n\\begin{aligned}\n   \\pmb J \\pmb J & =   \\pmb 1 \\pmb 1^t   \\pmb 1 \\pmb 1^t \\\\\n     & =  \\pmb 1   [ \\pmb 1^t  \\pmb 1 ]  \\pmb 1^t \\\\\n    & = \\pmb 1   [ n ]  \\pmb 1^t \\\\\n    & =  n \\pmb 1 \\pmb 1^t \\\\\n    & =  n \\pmb J  \n\\end{aligned}\n\\]\n\n\n\n\n\n\n노트\n\n\n\n참고로 평균모형 식 1.7 에서 \\(\\pmb X = \\pmb 1\\)으므로 이 경우 모자행렬이 다음과 같다.\n\\[ H_0 = \\pmb 1 ({\\pmb 1}^t \\pmb 1)^{-1} {\\pmb 1}^t = \\tfrac{1}{n} \\pmb J \\]\n\n\n다음으로 비중심 모수를 유도하자.\n\\[\n\\begin{aligned}\n\\pmb \\mu^t \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right )  \\pmb \\mu\n  & =  \\pmb \\beta^t  \\pmb X^t \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right ) \\pmb X \\pmb \\beta  \\\\\n  & =  \\pmb \\beta^t  \\left ( \\pmb X^t \\pmb H \\pmb X -  \\tfrac{1}{n} \\pmb X^t  \\pmb J \\pmb X \\right )  \\pmb \\beta  \\\\\n& =  \\pmb \\beta^t  \\left ( \\pmb X^t \\pmb X -  \\tfrac{1}{n} \\pmb X^t  \\pmb J \\pmb X \\right )  \\pmb \\beta  \\\\\n& =  \\pmb \\beta^t \\pmb X^t  \\left ( \\pmb I -  \\tfrac{1}{n}  \\pmb J \\right )  \\pmb X \\pmb \\beta  \\\\\n& \\equiv \\delta(\\pmb \\beta)\n\\end{aligned}\n\\]\n또한\n\\[\n\\begin{aligned}\nr\\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right )  \n      & = tr(\\pmb H) - tr \\left [ \\tfrac{1}{n} \\pmb J \\right ]  \\\\\n      & = p -\\tfrac{1}{n} tr (\\pmb 1 \\pmb 1^t) \\\\\n      & = p -\\tfrac{1}{n} tr ( \\pmb 1^t \\pmb 1) \\\\\n      &=  p -\\tfrac{1}{n} n \\\\\n      & = p-1 \\\\\n      & = p-1\n\\end{aligned}\n\\]\n위의 결과를 종합하면 회귀제곱합 \\(SSR\\)은 다음과 같은 분포를 따른다.\n\\[\n\\frac{SSR}{\\sigma^2} \\sim \\chi^2(p-1, \\lambda^2),\n\\tag{2.3}\\]\n위에서 비중심 모수는 다음과 같다.\n\\[\n  \\lambda^2 = \\tfrac{1}{\\sigma^2} \\delta(\\pmb \\beta) =\n\\tfrac{1}{\\sigma^2} \\pmb \\beta^t \\pmb X^t  \\left ( \\pmb I -  \\tfrac{1}{n}  \\pmb J \\right )  \\pmb X \\pmb \\beta\n\\tag{2.4}\\]\n\n\n2.1.3 잔차제곱합과 회귀제곱합의 독립\n잔차제곱합과 회귀제곱합에서 나타난 이차형식의 두 멱등행렬의 곱은 \\(\\pmb 0\\)이다.\n\\[\n\\begin{aligned}\n(\\pmb I - \\pmb H) \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right )  \n& = \\pmb H -  \\tfrac{1}{n} \\pmb J  - \\pmb H \\pmb H  + \\tfrac{1}{n} \\pmb H \\pmb J \\\\\n  & = \\pmb H -  \\tfrac{1}{n} \\pmb J  -  \\pmb H  + \\tfrac{1}{n}  \\pmb J \\\\\n  & = \\pmb 0\n\\end{aligned}\n\\]\n따라서 부록의 정리에 의하여 잔차제곱합(\\(SSE\\))과 회귀제곱합(\\(SSR\\))은 서로 독립이다.\n\n\n2.1.4 총제곱합의 분포\n총제곱합 \\(SST\\)의 분포는 위의 결과들을 이용하면 쉽게 구할 수 있다.\n\\[\nSST = \\sum_{i=1}^n (y_i - \\bar y)^2 = \\pmb y^t  \\left ( \\pmb I -  \\tfrac{1}{n}  \\pmb J \\right ) \\pmb y\n\\]\n위의 결과를 종합하면 회귀제곱합 \\(SST\\)은 다음과 같은 분포를 따른다.\n\\[\n\\frac{SST}{\\sigma^2} \\sim \\chi^2(n-1, \\lambda^2),\n\\tag{2.5}\\]\n위에서 비중심 모수 \\(\\lambda^2\\)은 식 식 2.4 과 같다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>선형회귀에서의 추론</span>"
    ]
  },
  {
    "objectID": "qmd/inference.html#모분산의-추정",
    "href": "qmd/inference.html#모분산의-추정",
    "title": "2  선형회귀에서의 추론",
    "section": "2.2 모분산의 추정",
    "text": "2.2 모분산의 추정\n최소제곱법을 통해서 회귀분석을 실시하였을때 우리는 적합된 회귀선이 얼마나 실제 관측값들을 잘 설명하고 있는지를 파악하는 것이 모형의 유용성을 판단하는데 중요한 작업이다. 즉, 적합된 회귀선이 관측값을 예측할 때의 변동성을 측정하는 것이 중요하다. 그 변동의 정도를 나타내는 것이 모분산 \\(\\sigma^2\\)의 추정이다.\n식 식 2.2 에 나타난 잔차제곱합의 분포를 이용하면 다음과 같은 결과를 얻는다.\n\\[ E \\left [ \\frac{SSE}{\\sigma^2} \\right ] = n-p \\]\n위의 방정식에 적률법(Method of Moments)를 적용하면 모분산 \\(\\sigma^2\\)에 대한 불편추정량을 얻을 수 있다. 평균 잔차 제곱합(mean residual sum of square; \\(S^2\\) 또는 MSE)를 다음과 같이 정의하자.\n\\[\nMSE = \\frac{SSE}{n-p} = \\frac{\\sum r^2_i}{n-p}  = \\frac{\\sum(y_i-\\hat y_i)^2}{n-p} \\equiv s^2\n\\tag{2.6}\\]\n\\(S^2=MSE\\)은 모분산의 불편 추정량이다.\n\\[ E(s^2) = E(MSE) =\\sigma^2 \\]\n모분산의 추정량이 작을수록 관측값 \\(y\\)의 변동 중 회귀식이 설명할 수 변동이 크다는 것을 나타낸다. 관측값들이 회귀식으로부터 멀리 떨어져 있으면 \\(MSE\\) 는 커진다.\n회귀계수들의 공분산을 추정하는 경우에도 \\(s^2\\)이 사용된다.\n\\[ \\hat V ( \\hat {\\pmb \\beta }) = \\hat \\sigma^2 (\\pmb X^t \\pmb X)^{-1} = s^2(\\pmb X^t \\pmb X)^{-1} \\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>선형회귀에서의 추론</span>"
    ]
  },
  {
    "objectID": "qmd/inference.html#최소제곱-추정량의-성질",
    "href": "qmd/inference.html#최소제곱-추정량의-성질",
    "title": "2  선형회귀에서의 추론",
    "section": "2.3 최소제곱 추정량의 성질",
    "text": "2.3 최소제곱 추정량의 성질\n최소제곱 추정량의 분포에 대한 성질은 다음과 같다.\n\n\\(\\hat {\\pmb \\beta} \\sim N(\\pmb \\beta, \\sigma^2 (\\pmb X^t \\pmb X)^{-1} )\\)\n\\(\\hat {\\pmb \\beta}\\)와 \\(SSE\\)는 독립이다.\n잔차제곱합(\\(SSE\\))과 회귀제곱합(\\(SSR\\))은 서로 독립이다.\n\\(SSE/\\sigma^2\\)는 자유도가 \\(n-p\\)인 카이제곱 분포를 따른다.\n\\((\\hat {\\pmb \\beta} -\\pmb \\beta)^t (\\pmb X^t \\pmb X) (\\hat {\\pmb \\beta} -\\pmb \\beta) /\\sigma^2\\) 는 자유도가 \\(p\\)인 카이제곱분포를 따른다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>선형회귀에서의 추론</span>"
    ]
  },
  {
    "objectID": "qmd/inference.html#모형의-적합도-검정과-분산분석",
    "href": "qmd/inference.html#모형의-적합도-검정과-분산분석",
    "title": "2  선형회귀에서의 추론",
    "section": "2.4 모형의 적합도 검정과 분산분석",
    "text": "2.4 모형의 적합도 검정과 분산분석\n회귀식을 적합하고 가장 먼저 고려해야할 사항은 적합된 회귀식이 유의한 의미를 가지는지 알아보는 것이다. 회귀식이 가지고 있는 의미는 설명변수의 변화에 따라서 반응변수가 변한다는 것이다. 따라서 회귀 모형이 유의하다는 것은 최소한 하나 이상의 설명변수가 반응변수의 변화를 예측하는데 의미가 있다는 것을 뜻한다. 모든 회귀계수의 값이 0이면 반응변수를 예측하는데 모든 설명변수가 필요가 없다는 것을 의미한다. 이러한 무의미한 모형은 앞장에서 나온 평균모형 식 1.7 이다.\n이제 제시된 회귀식이 유의한 지에 대한 검정은 다음과 같은 두 가설 중 하나를 선택하는 것이다.\n\\[\nH_0: \\text{mean model} \\quad vs. \\quad H_1: \\text{ not } H_0\n\\]\n위의 가설을 바꾸어 쓰면 선형 회귀모형의 유의성 또는 적합도을 검정하는 가설이 된다.\n\\[\nH_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_{p-1} =0 \\quad vs. \\quad H_1: \\text{ At least one of } \\beta_i \\text{ is not equal to } 0\n\\tag{2.7}\\]\n위의 가설 식 2.7 를 검정하는 방법이 분산분석표를 이용한 F-검정이다.\n가설 식 2.7 에서 귀무가설 \\(H_0\\)가 참인 경우는\n\\[\n\\pmb X \\pmb \\beta = [ \\pmb 1 ~ \\pmb x_1 ~ \\dots ~ \\pmb x_{p-1} ]\n\\begin{bmatrix}\n\\beta_0 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix}\n=\\beta_0 \\pmb 1\n\\]\n이 성립하여 식 식 2.4 에 나타난 비중심 모수가 0이 된다.\n\\[  \\lambda^2 = \\tfrac{1}{\\sigma^2} \\pmb \\beta^t \\pmb X^t  \\left ( \\pmb I -  \\tfrac{1}{n}  \\pmb J \\right )  \\pmb X \\pmb \\beta = \\tfrac{\\beta_0^2}{\\sigma^2}  \\pmb 1^t \\left ( \\pmb I -  \\tfrac{1}{n}  \\pmb J \\right )  \\pmb 1 =  \\pmb 0\n\\]\n따라서 귀무가설에서는 회귀제곱합이 자유도가 \\(p-1\\)인 중신ㅁ 카이제곱 분포를 따르게 되고 잔차제곱합과 독립이므로 다음의 통계량 \\(F_0\\)가 자유도가 \\(p-1\\)가ㅗ \\(n-p\\)를 가지는 F-분포를 따른다.\n\\[\nF_0 = \\frac{ SSR/(p-1)}{SSE/(n-p)} = \\frac{MSR}{MSE} \\sim F(p-1, n-p) \\quad \\text{ under } H_0\n\\tag{2.8}\\]\n따라서 위의 검정 통계량의 p-값이 유의수준보다 크면 적합성 검정에 대한 가설 식 2.7 의 귀무가설을 기각한다. 귀무가설의 기각은 회귀모형의 계수 중 적어도 하나는 0이 아니므로 회귀 모형이 유의하다는 의미이다.\n위에서 안급한 F-검정을 위한 통계량들은 다음과 같은 분산분석(Analysis of Variance; ANOVA) 표를 사용하면 쉽게 계산할 수 있다.\n\n적합도 검정을 위한 분산분석표\n\n\n\n\n\n\n\n\n\n\n요인\n제곱합\n자유도\n평균제곱합\nF-통계량\np-값\n\n\n\n\n회귀\n\\(SSR\\)\n\\(p-1\\)\n\\(MSR\\)\n\\(F_0 =\\frac{MSR}{MSE}\\)\n\\(P(F&gt;F_0)\\)\n\n\n오차\n\\(SSE\\)\n\\(n-p\\)\n\\(MSE\\)\n\n\n\n\n전체\n\\(SST\\)\n\\(n-1\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>선형회귀에서의 추론</span>"
    ]
  },
  {
    "objectID": "qmd/inference2.html",
    "href": "qmd/inference2.html",
    "title": "3  모형의 비교",
    "section": "",
    "text": "3.1 직교하는 설명 변수\n다음과 같은 2개의 설명변수(\\(x_1\\), \\(x_2\\)) 와 반응변수 \\(y\\) 를 가진 자료(데이터프레임)이 있다고 하자.\nx1 &lt;- c(1,  1, 1,  1, -1, -1, -1, -1)\nx2 &lt;- c(1, -1, 1, -1, 1, -1, 1, -1)\ny &lt;- c( 2, 5, 3, 4, 6, 9, 5, 10)\ndf &lt;- data.frame(x1, x2, y)\ndf\n\n  x1 x2  y\n1  1  1  2\n2  1 -1  5\n3  1  1  3\n4  1 -1  4\n5 -1  1  6\n6 -1 -1  9\n7 -1  1  5\n8 -1 -1 10\n이제 위의 자료로 선형회귀모형을 적합해 보자.\n\\[\ny_i= \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} +e_i\n\\tag{3.1}\\]\nfm1 &lt;- lm(y ~ x1 + x2, data=df)\nsummary(fm1)$coefficients\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)      5.5  0.3162278 17.392527 0.0000115141\nx1              -2.0  0.3162278 -6.324555 0.0014565818\nx2              -1.5  0.3162278 -4.743416 0.0051344617\n이제 모형 식 3.1 에서 각각 \\(x_1\\)과 \\(x_2\\)를 제거한 축소된 모형을 적합해보자\n\\[\ny_i= \\beta_0 + \\beta_1 x_{i1} +e_i  , \\quad \\quad y_i= \\beta_0 +  \\beta_2 x_{i2} +e_i\n\\tag{3.2}\\]\nfm21 &lt;- lm(y ~ x1 , data=df)\nsummary(fm21)$coefficients\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)      5.5  0.6770032  8.124038 0.0001867963\nx1              -2.0  0.6770032 -2.954196 0.0254739283\nfm22 &lt;- lm(y ~ x2 , data=df)\nsummary(fm22)$coefficients\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)      5.5  0.8660254  6.350853 0.0007143845\nx2              -1.5  0.8660254 -1.732051 0.1339745962\n두 개의 독립변수가 있는 모형 식 3.1 에서 하나의 독립변수를 제거해도 남아 있는 독립 변수의 회귀계수 추정량은 모형 식 3.1 과 같은 것을 알 수 있다. 이렇게 여러 개의 독립변수가 있는 모형에서 하나의 변수를 제거해도 다른 복립변수의 추정에 영향을 미치지 않는 경우는 어떤 경우일까?\n이제 모형 식 3.1 의 설계행렬(\\(\\pmb  X\\), design matrix)를 구해서 \\(\\pmb  X^t \\pmb  X\\)를 구해보자.\nX &lt;- model.matrix(fm1)\nX\n\n  (Intercept) x1 x2\n1           1  1  1\n2           1  1 -1\n3           1  1  1\n4           1  1 -1\n5           1 -1  1\n6           1 -1 -1\n7           1 -1  1\n8           1 -1 -1\nattr(,\"assign\")\n[1] 0 1 2\n\nt(X) %*% X\n\n            (Intercept) x1 x2\n(Intercept)           8  0  0\nx1                    0  8  0\nx2                    0  0  8\n모형 식 3.1 의 설계행렬 \\(\\pmb  X\\)의 각 열들은 서로 직교하는것을 알 수 있다.\n만약 여러 개의 독립변수를 가진 선형모형에서 모든 설명 변수들의 열들이 모두 서로 직교한다면(절편에 대한 열도 포함해서) 회귀계수의 추정값은 독립변수가 줄어든 축소돤 모형에서도 원래의 모형과 같은 것을 알 수 있다.\n선형 모형에서 설계행렬 \\(\\pmb  X\\)의 각 열벡터를 각각 \\(\\pmb  x_1,\\dots, \\pmb  x_{p}\\)라고 하자. 만약 모든 열들이 서로 직교한다면 (즉 \\({\\pmb  x}_i^t \\pmb  x_j =0\\) for \\(i \\ne j\\)) 선형회귀 모형에서 회귀계수의 추정치는 설명 변수의 유무에 관계없이 일정하게 나타난다.\n이러한 상황을 모형식으로 다시 써보자. 만약 다음이 성립하면 \\[\n\\pmb  X^t \\pmb  X =  \n\\begin{bmatrix}\n{\\pmb  x}_1^t \\\\\n{\\pmb  x}_2^t \\\\\n\\vdots \\\\\n{\\pmb  x}_{p}^t \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\pmb  x_{1} &  \\pmb  x_{2} & \\dots & {\\pmb  x}_{p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n{\\pmb  x}_1^t {\\pmb  x}_1 & 0 & 0 & \\cdots & 0 \\\\\n0 & {\\pmb  x}_2^t {\\pmb  x}_2  & 0 & \\cdots & 0 \\\\\n0  & 0 & {\\pmb  x}_3^t {\\pmb  x}_3  & \\cdots & 0 \\\\\n\\vdots  &  \\vdots  & \\vdots  & \\vdots    & 0 \\\\\n0 & 0 & \\cdots & 0 & {\\pmb  x}_{p}^t {\\pmb  x}_{p}\n\\end{bmatrix}\n\\] 회귀계수의 추정량은 다음과 같이 나타난다.\n\\[\n\\left ( {\\pmb  X}^t \\pmb  X \\right )^{-1} {\\pmb  X}^t \\pmb  y =  \n\\begin{bmatrix}\n\\tfrac{1} {{\\pmb  x}_1^t {\\pmb  x}_1} & 0 & 0 & \\cdots & 0 \\\\\n0 & \\tfrac{1}{{\\pmb  x}_2^t {\\pmb  x}_2}  & 0 & \\cdots & 0 \\\\\n0  & 0 & \\tfrac{1}{{\\pmb  x}_3^t {\\pmb  x}_3}  & \\cdots & 0 \\\\\n\\vdots  &  \\vdots  & \\vdots  & \\vdots    & 0 \\\\\n0 & 0 & \\cdots & 0 & \\tfrac{1}{{\\pmb  x}_{p}^t {\\pmb  x}_{p} }\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\pmb  x}_1^t \\pmb  y \\\\\n{\\pmb  x}_2^t \\pmb  y \\\\\n{\\pmb  x}_3^t \\pmb  y \\\\\n\\vdots \\\\\n{\\pmb  x}_{p}^t \\pmb  y\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\tfrac{{\\pmb  x}_1^t \\pmb  y}{{\\pmb  x}_1^t {\\pmb  x}_1 } \\\\\n\\tfrac{{\\pmb  x}_2^t \\pmb  y}{{\\pmb  x}_2^t {\\pmb  x}_2 } \\\\\n\\vdots \\\\\n\\tfrac{{\\pmb  x}_{p}^t \\pmb  y}{{\\pmb  x}_{p}^t {\\pmb  x}_{p} }\n\\end{bmatrix}\n\\]\n위의 결과를 조금 더 일반화해보자. 만약 계획행렬 \\(\\pmb  X\\)를 다음과 같은 \\(p\\)개의 부분 계획행렬 \\(\\pmb  X_1, \\pmb  X_2, \\dots, \\pmb  X_{p}\\)로 나누고\n\\[ \\pmb  y = \\pmb  X \\pmb  \\beta + \\pmb  e = \\sum_{k=1}^{p} \\pmb  X_k \\pmb  \\beta_k + \\pmb  e \\]\n부분 계획행렬들이 다음과 같은 성질을 가지고 있다고 하자.\n\\[  \n\\pmb  X = [\\pmb  X_1~ \\pmb  X_2~ \\dots~ \\pmb  X_{p} ] \\quad \\text{ and } \\quad {\\pmb  X}_i^t {\\pmb  X}_j =\\pmb  0, i \\ne j\n\\tag{3.3}\\]\n이러한 조건 하에서는 회귀계수의 추정량이 다음과 같이 나타난다.\n\\[\n\\begin{aligned}\n\\hat { \\pmb  \\beta} &  = \\left ( {\\pmb  X}^t \\pmb  X \\right )^{-1} {\\pmb  X}^t \\pmb  y \\\\\n& =\n\\begin{bmatrix}\n( {\\pmb  X}_1^t {\\pmb  X}_1 )^{-1} & 0 & 0 & \\cdots & 0 \\\\\n0 & ( {\\pmb  X}_2^t {\\pmb  X}_2 )^{-1}  & 0 & \\cdots & 0 \\\\\n0  & 0 & ( {\\pmb  X}_3^t {\\pmb  X}_3 )^{-1}  & \\cdots & 0 \\\\\n\\vdots  &  \\vdots  & \\vdots  & \\vdots    & 0 \\\\\n0 & 0 & \\cdots & 0 & ( {\\pmb  X}_{p}^t {\\pmb  X}_{p} )^{-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\pmb  X}_1^t \\pmb  y \\\\\n{\\pmb  X}_2^t \\pmb  y \\\\\n{\\pmb  X}_3^t \\pmb  y \\\\\n\\vdots \\\\\n{\\pmb  X}_{p}^t \\pmb  y\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n( {\\pmb  X}_1^t {\\pmb  X}_1 )^{-1} {\\pmb  X}_1^t \\pmb  y \\\\\n( {\\pmb  X}_2^t {\\pmb  X}_2 )^{-1} {\\pmb  X}_2^t \\pmb  y \\\\\n\\vdots \\\\\n( {\\pmb  X}_{p}^t {\\pmb  X}_{p} )^{-1} {\\pmb  X}_{p}^t \\pmb  y\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n\\hat {\\pmb  \\beta}_1 \\\\\n\\hat {\\pmb  \\beta}_2 \\\\\n\\hat {\\pmb  \\beta}_3 \\\\\n\\vdots \\\\\n\\hat {\\pmb  \\beta}_{p}\n\\end{bmatrix}\n\\end{aligned}\n\\]\n위의 결과는 전체 모형에서의 추정량 \\(\\hat {\\pmb  \\beta}\\)의 \\(j\\) 번째 부분 \\(\\hat {\\pmb  \\beta}_j\\)이 \\(({\\pmb  X}_j^t {\\pmb  X}_j )^{-1} {\\pmb  X}_j^t \\pmb  y\\)로 구성되며 다른 \\(\\pmb  X_i\\)와는 관계가 없다.\n이러한 결과를 이용하면 설명변수들이 서로 직교하는 조건 식 3.3 를 만족하면 축소모형\n\\[ \\pmb  y = \\pmb  X_j \\pmb  \\beta_j + \\pmb  e \\]\n에서 계수 추정치 \\(\\hat {\\pmb  \\beta}_j =({\\pmb  X}_j^t {\\pmb  X}_j )^{-1} {\\pmb  X}_j^t \\pmb  y\\) 는 모든 설명 변수를 고려한 완전 모형에서의 추정치와 같은 것을 알 수 있다. 설명변수들이 서로 직교하는 조건 식 3.3 을 만족하면 하나의 축소모형에 대한 추정량는 직교하는 다른 설명변수들의 영향을 받지 않는다.\n더 나아가서 직교하는 설명변수들이 회귀제곱합에 미치는 영향은 각 축소모형들의 기여도를 단순하게 더한 결과와 같다.\n\\[\n\\begin{aligned}\nSSR & = \\pmb  y^t  \\left ( \\pmb  H  -  \\tfrac{1}{n} \\pmb  J \\right ) \\pmb  y \\\\\n  & = \\pmb  y^t  \\pmb  H \\pmb  y - \\tfrac{1}{n} \\pmb  y^t \\pmb  J  \\pmb  y \\\\\n  & = \\pmb  y^t \\pmb  X (\\pmb  X^t \\pmb  X)^{-1} \\pmb  X^t \\pmb  y  - n (\\bar y)^2 \\\\\n  & = \\pmb  y^t \\pmb  X  (\\pmb  X^t \\pmb  X)^{-1} (\\pmb  X^t \\pmb  X) (\\pmb  X^t \\pmb  X)^{-1} \\pmb  X^t \\pmb  y  - n (\\bar y)^2 \\\\\n  & = {\\hat {\\pmb  \\beta}}^t (\\pmb  X^t \\pmb  X) \\hat {\\pmb  \\beta} - n (\\bar y)^2 \\\\\n  & = \\sum_{k=1}^{p} {\\hat {\\pmb  \\beta}}_k^t ({\\pmb  X}_k^t {\\pmb  X}_k) \\hat {\\pmb  \\beta}_k - n (\\bar y)^2\n\\end{aligned}\n\\]\n또한 회귀계수 추정량의 분산을 보면 부분 회귀계수 추정량 \\(\\hat {\\pmb  \\beta}_j\\) 들은 서로 독립이며 축소 모형에서의 분산과 동일함을 알 수 있다.\n\\[\n\\begin{aligned}\nCov(\\hat {\\pmb  \\beta} ) & = \\sigma^2 (\\pmb  X^t \\pmb  X)^{-1} \\\\\n  & =\n  \\begin{bmatrix}\n\\sigma^2 ( {\\pmb  X}_1^t {\\pmb  X}_1 )^{-1} & 0 & 0 & \\cdots & 0 \\\\\n0 & \\sigma^2 ( {\\pmb  X}_2^t {\\pmb  X}_2 )^{-1}  & 0 & \\cdots & 0 \\\\\n0  & 0 & \\sigma^2 ( {\\pmb  X}_3^t {\\pmb  X}_3 )^{-1}  & \\cdots & 0 \\\\\n\\vdots  &  \\vdots  & \\vdots  & \\vdots    & 0 \\\\\n0 & 0 & \\cdots & 0 & \\sigma^2 ( {\\pmb  X}_{p}^t {\\pmb  X}_{p} )^{-1}\n\\end{bmatrix} \\\\\n& =  \n  \\begin{bmatrix}\nCov( \\hat {\\pmb  \\beta}_1 ) & 0 & 0 & \\cdots & 0 \\\\\n0 &  Cov( \\hat {\\pmb  \\beta}_2 )  & 0 & \\cdots & 0 \\\\\n0  & 0 &  Cov( \\hat  {\\pmb  \\beta}_3 )   & \\cdots & 0 \\\\\n\\vdots  &  \\vdots  & \\vdots  & \\vdots    & 0 \\\\\n0 & 0 & \\cdots & 0 &  Cov( \\hat  {\\pmb  \\beta}_p )\n\\end{bmatrix}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>모형의 비교</span>"
    ]
  },
  {
    "objectID": "qmd/inference2.html#설명변수의-추가",
    "href": "qmd/inference2.html#설명변수의-추가",
    "title": "3  모형의 비교",
    "section": "3.2 설명변수의 추가",
    "text": "3.2 설명변수의 추가\n먼저 계획행렬 \\(\\pmb  X_1\\)을 고려한 선형모형을 고려하자.\n\\[\n\\pmb  y  = \\pmb  X_1 {\\pmb  \\beta}_{1*}  + \\pmb e\n\\tag{3.4}\\]\n이 경우 회귀계수의 최소제곱추정량은 \\(\\hat {\\pmb  \\beta}_{1*} = ({\\pmb  X}_1^t {\\pmb  X}_1)^{-1} {\\pmb  X}_1^t \\pmb  y\\) 이다.\n이제 위의 모형 식 3.4 에 설명변수를 추가한 모형을 생각해 보자. 추가된 설명변수로 이루어진 계획행렬을 \\(\\pmb X_2\\)라고 하면 다음과 같이 쓸수 있다.\n\\[\n\\begin{aligned}\n\\pmb  y & = \\pmb  X_1 {\\pmb  \\beta}_1 + \\pmb  X_2 {\\pmb  \\beta}_2  + \\pmb  e  \\\\\n    & = [ \\pmb  X_1 ~ \\pmb  X_2]\n    \\begin{bmatrix}\n    {\\pmb  \\beta}_1 \\\\\n    {\\pmb  \\beta}_2\n    \\end{bmatrix} + \\pmb  e \\\\\n    & =  {\\pmb   X } {\\pmb  \\beta} + \\pmb  e\n\\end{aligned}\n\\tag{3.5}\\]\n위의 식에서\n\\[\n\\pmb X = [ \\pmb X_1 ~ \\pmb X_2] \\quad \\text{ and } \\quad \\pmb  \\beta =\n\\begin{bmatrix}\n{\\pmb  \\beta}_1 \\\\\n{\\pmb  \\beta}_2\n\\end{bmatrix}\n\\]\n설명변수를 추가한 확대 모형 식 3.5 에서 회귀계수의 최소제곱추정량은 \\(\\hat {\\pmb  \\beta} = (\\pmb  X^t \\pmb  X)^{-1} \\pmb  X^t \\pmb  y\\)이다.\n여기서 주의할 점은 식 3.4 의 회귀 계수 \\({\\pmb \\beta}_{1*}\\) 의 추정량과 식 3.5 의 회귀 계수 \\({\\pmb \\beta}_{1}\\) 의 추정량은 일반적으로 같지 않다.\n확대모형 식 3.5 의 회귀계수 추정량을 구하려면 최소제곱법을 다시 확장모형에 적용해야 하지만 원래의 모형 식 3.4 에서 구해진 추정량 \\(\\hat {\\pmb  \\beta}_{1*} = ({\\pmb  X}_1^t {\\pmb  X}_1)^{-1} {\\pmb  X}_1^t \\pmb  y\\) 을 이용하여 유도할 수 있다.\n이제 원래의 모형 식 3.4 에서 모자행렬을\n\\[ \\pmb  H_1 = {\\pmb  X}_1 ({\\pmb  X}_1^t {\\pmb  X}_1)^{-1} {\\pmb  X}_1^t \\]\n라고 하고 확대 모형 식 3.5 에 대하여 다음과 같이 모형을 다시 표현해보자.\n\\[\n\\begin{aligned}\n\\pmb  y & = \\pmb  X_1 {\\pmb  \\beta}_1 + \\pmb  X_2 {\\pmb  \\beta}_2  + \\pmb  e  \\\\\n  & = \\pmb  X_1 {\\pmb  \\beta}_1 + ( \\pmb  H_1  + \\pmb  I -\\pmb  H_1  ) \\pmb  X_2 {\\pmb  \\beta}_2  + \\pmb  e \\\\\n  & = \\pmb  X_1 {\\pmb  \\beta}_1 +  \\pmb  H_1  \\pmb  X_2 {\\pmb  \\beta}_2 + (\\pmb  I -\\pmb  H_1  ) \\pmb  X_2 {\\pmb  \\beta}_2  + \\pmb  e \\\\\n  & = [ \\pmb  X_1 {\\pmb  \\beta}_1 + \\pmb  H_1 \\pmb  X_2 {\\pmb  \\beta}_2 ]  + (\\pmb  I -\\pmb  H_1  ) \\pmb  X_2    {\\pmb  \\beta}_2  + \\pmb  e \\\\\n  & = \\pmb  X_1  [ {\\pmb  \\beta}_1 +  ({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t   \\pmb  X_2 {\\pmb  \\beta}_2]  + \\tilde {\\pmb  X}_2  {\\pmb  \\beta}_2  + \\pmb  e \\\\\n  & = \\pmb  X_1  \\tilde {\\pmb  \\beta}_1  + \\tilde {\\pmb  X}_2  {\\pmb  \\beta}_2  + \\pmb  e\n\\end{aligned}\n\\]\n이제 다음과 같은 변환된 모형을 고려하자.\n\\[\n\\pmb y =\\pmb  X_1  \\tilde {\\pmb  \\beta}_1  + \\tilde {\\pmb  X}_2  {\\pmb  \\beta}_2  + \\pmb  e\n\\tag{3.6}\\]\n위의 식에서 다음과 같이 새로운 계수벡터 \\(\\tilde {\\pmb  \\beta}_1\\) 와 변환된 계획행렬 \\(\\tilde{\\pmb  X}_2\\)를 정의하였다.\n\\[\n\\tilde {\\pmb  \\beta}_1  = {\\pmb  \\beta}_1 +  ({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t   \\pmb  X_2 {\\pmb  \\beta}_2,  \\quad\n\\tilde{\\pmb  X}_2 =   (\\pmb  I -\\pmb  H_1  ) \\pmb  X_2\n\\tag{3.7}\\]\n이제 변환된 모형 식 3.6 에서 두 계획행렬 \\(\\pmb  X_1\\)과 \\(\\tilde{\\pmb  X}_2\\)가 서로 직교하는 것을 알 수 있다.\n\\[  {\\pmb  X_1}^t \\tilde{\\pmb  X}_2 =  {\\pmb  X}_1^t (\\pmb  I -\\pmb  H_1  ) \\pmb  X_2=  {\\pmb  X}_1^t (\\pmb  I -{\\pmb  X}_1 ({\\pmb  X}_1^t {\\pmb  X}_1)^{-1} {\\pmb  X}_1^t ) \\pmb  X_2= \\pmb  0 \\]\n이제 확대된 모형 식 3.6 의 두 계획행렬 \\(\\pmb  X_1\\)과 \\(\\tilde{\\pmb  X}_2\\)가 서로 직교하므로 앞에서 나온 직교하는 계획행렬에 대한 회귀계수에 대한 결과를 이용하면 다음과 같이 회귀계수 추정량을 얻을 수 있다.\n이제 모형 식 3.6 의 회귀계수 \\(\\tilde {\\pmb  \\beta_1}\\) 과 \\(\\pmb  \\beta_2\\)의 추정량을 구해보면 다음과 같다.\n\\[\n\\hat {\\tilde{\\pmb  \\beta_1}} =  ({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t \\pmb  y, \\quad \\hat {\\pmb  \\beta}_2 = ({\\tilde {\\pmb  X}}_2^t \\tilde {\\pmb  X}_2)^{-1} {\\tilde {\\pmb  X}_2}^t \\pmb  y\n\\tag{3.8}\\]\n먼저 식 3.6 에서 \\({\\pmb  \\beta}_2\\) 에 대한 추정량 \\(\\hat {\\pmb  \\beta}_2\\)는 반응변수 \\(\\pmb  y\\) 를 변환된 계획 행렬 \\(\\tilde {\\pmb  X}_2 =  (\\pmb  I -\\pmb  H_1  ) \\pmb  X_2\\)로 적합할 때의 회귀계수이다.\n\\[ y = [(\\pmb  I -\\pmb  H_1  ) \\pmb  X_2]  {\\pmb  \\beta}_2 + \\pmb  e \\]\n식 3.8 에 주어진 추정량 \\(\\hat {\\pmb  \\beta}_2\\) 를 다시 다음과 같이 유도할 수 있다.\n\\[\n\\begin{aligned}\n\\hat {\\pmb  \\beta}_2 & = ({\\tilde {\\pmb  X}}_2^t \\tilde {\\pmb  X}_2)^{-1} {\\tilde {\\pmb  X}_2}^t \\pmb  y \\\\\n   & =   [ {\\pmb  X}_2^t  (\\pmb  I -\\pmb  H_1  )  (\\pmb  I -\\pmb  H_1  )   \\pmb  X_2]^{-1}  {\\pmb  X}_2^t (\\pmb  I -\\pmb  H_1  ) \\pmb  y \\\\\n   & =    [ {\\pmb  X}_2^t  (\\pmb  I -\\pmb  H_1  )  (\\pmb  I -\\pmb  H_1  )   \\pmb  X_2]^{-1}  {\\pmb  X}_2^t (\\pmb  I -\\pmb  H_1  ) (\\pmb  I -\\pmb  H_1  ) \\pmb  y \\\\\n    & = ({\\tilde {\\pmb  X}_2}^t \\tilde {\\pmb  X}_2)^{-1} {\\tilde {\\pmb  X}_2}^t  [(\\pmb  I -\\pmb  H_1  ) \\pmb  y] \\\\\n    & = ({\\tilde {\\pmb  X}_2}^t \\tilde {\\pmb  X}_2)^{-1} {\\tilde {\\pmb  X}_2}^t  \\tilde {\\pmb y}  \n\\end{aligned}\n\\] 위의 유도를 보면 반응변수 \\(\\pmb  y\\) 에서 \\(\\pmb  X_1\\)로 적합한 후에 구한 잔차벡터 \\(\\tilde {\\pmb  y} = (\\pmb  I -\\pmb  H_1  ) \\pmb  y\\) 를 새로운 반응변수로 고려한 후, 추가된 변수에 대한 계획행렬 \\(\\pmb  X_2\\) 에서 먼저 고려한 변수의 계획행렬 \\(\\pmb X_1\\) 의 효과를 제거한 \\((\\pmb  I -\\pmb  H_1  )\\pmb  X_2\\) 로 적합한 경우의 회귀계수로 나타난다.\n\\[ \\tilde {\\pmb  y} = \\tilde {\\pmb  X}_2 \\pmb \\beta_2 + \\pmb e \\quad \\rightarrow \\quad (\\pmb  I -\\pmb  H_1  )y = [(\\pmb  I -\\pmb  H_1  ) \\pmb  X_2]  {\\pmb  \\beta}_2 + \\pmb  e \\]\n또한 식 3.6 의 회귀계수 \\(\\tilde{\\pmb  \\beta_1}\\)의 추정량은 직교성에 의하여 \\(({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t \\pmb  y\\) 으로 주어지며 이는 모형 식 3.4 에서 구한 회귀계수의 추정량과 같다.\n\\[ \\hat {\\tilde{\\pmb  \\beta_1}} = \\hat {{\\pmb  \\beta}}_{1*} = ({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t \\pmb  y\\]\n이제 식 3.7 의 관계를 이용하면 모형 식 3.5 에서 나타난 회귀계수 \\({\\pmb  \\beta}_1\\) 의 추정량을 다음과 같이 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\hat {\\pmb  \\beta}_1 & = \\hat {\\tilde {\\pmb  \\beta}}_1  - ({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t   \\pmb  X_2 \\hat {\\pmb  \\beta}_2 \\\\\n& = \\hat {{\\pmb  \\beta}}_{1*}  - ({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t   \\pmb  X_2 \\hat {\\pmb  \\beta}_2 \\\\\n& =({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t (\\pmb  y - \\pmb  X_2 \\hat {\\pmb  \\beta}_2)\n\\end{aligned}\n\\tag{3.9}\\]\n식 3.9 에 주어진 회귀계수 \\({\\pmb  \\beta}_1\\) 의 추정량은 새로운 변수를 추기하기 전의 모형에서 구한 추정량 \\(\\hat {{\\pmb  \\beta}}_{1*}\\) 을 새로운 변수를 추가한 후의 모형에서 추가된 변수에 대한 회귀계수의 추정량 \\(\\hat {\\pmb  \\beta}_2\\) 으로 보정힌 형태이다.\n이제 간단한 예제를 통하여 위에서 유도한 공식을 적용해 보자.\n먼저 3 개의 설명변수 \\(x_1, x_2, x_3\\) 를 가진 10 개의 자료를 임의로 만들어 보자.\n\nset.seed(23123)\nx1 &lt;- c(1,2,3,4,5,6,7,8,9,9)\nx2 &lt;- c(1,4,2,5,3,2,4,3,1,2)\nx3 &lt;- c(6,3,2,3,1,4,5,3,2,1)\ny &lt;- 2 + 3*x1 + 4*x2 + 5*x3 + rnorm(10)\n\ndf &lt;- data.frame(x1,x2,x3,y)\ndf\n\n   x1 x2 x3        y\n1   1  1  6 38.99584\n2   2  4  3 38.17609\n3   3  2  2 27.13590\n4   4  5  3 49.86576\n5   5  3  1 32.88329\n6   6  2  4 48.43808\n7   7  4  5 63.30271\n8   8  3  3 53.54604\n9   9  1  2 41.86736\n10  9  2  1 43.30445\n\n\n먼저 2개의 독립변수 \\(x_1\\) 과 \\(x_2\\) 가 있는 모형을 적합해 보자.\n\\[ y = \\beta_{0*} + \\beta_{1*} x_1 + \\beta_{2*} x_2 + e  \\tag{3.10}\\]\n\nlm1s &lt;- lm(y ~ x1 + x2, data=df)\nlm1s$coefficients\n\n(Intercept)          x1          x2 \n  22.964557    1.948430    3.802026 \n\n\n또한 모형 식 3.10 에서 사용한 계획행렬 \\(\\pmb X_1\\) 을 구해보자\n\nX1 &lt;- model.matrix(lm1s)\nX1\n\n   (Intercept) x1 x2\n1            1  1  1\n2            1  2  4\n3            1  3  2\n4            1  4  5\n5            1  5  3\n6            1  6  2\n7            1  7  4\n8            1  8  3\n9            1  9  1\n10           1  9  2\nattr(,\"assign\")\n[1] 0 1 2\n\n\n다음으로 모든 독립변수가 있는 모형을 적합해 보자.\n\\[ y = \\beta_{0} + \\beta_{1} x_1 + \\beta_{2} x_2 + \\beta_{3} x_3 + e  \\tag{3.11}\\]\n\nlmAll &lt;- lm(y ~ x1 + x2 + x3, data=df)\nlmAll$coefficients\n\n(Intercept)          x1          x2          x3 \n-0.05735613  3.15491311  4.16172306  5.17857500 \n\n\n또한 모형 식 3.11 에서 모든 독립변수를 사용한 경우 계획행렬 \\(\\pmb X\\) 와 추가된 변수 \\(x_3\\) 에 대한 열만 가지는 계획행렬 \\(\\pmb X_2\\) 를 구해보자\n\nX &lt;- model.matrix(lmAll)\nX\n\n   (Intercept) x1 x2 x3\n1            1  1  1  6\n2            1  2  4  3\n3            1  3  2  2\n4            1  4  5  3\n5            1  5  3  1\n6            1  6  2  4\n7            1  7  4  5\n8            1  8  3  3\n9            1  9  1  2\n10           1  9  2  1\nattr(,\"assign\")\n[1] 0 1 2 3\n\n\n\nX2 &lt;- X[,4]\nX2\n\n 1  2  3  4  5  6  7  8  9 10 \n 6  3  2  3  1  4  5  3  2  1 \n\n\n이제 식 3.7 에 주어진 \\(\\tilde {\\pmb X}_2\\) 를 계산하고 이를 이용하여 \\(\\hat {\\pmb \\beta}_2\\) 를 구해보자.\n\nH1 &lt;- X1 %*% solve(t(X1) %*% X1) %*% t(X1)\nX2t &lt;- (diag(10) - H1) %*% X2\nX2t\n\n         [,1]\n1   1.8568267\n2  -0.7018216\n3  -1.6077630\n4  -0.1664113\n5  -2.0723527\n6   1.0911645\n7   2.4630575\n8   0.6265747\n9  -0.2793667\n10 -1.2099081\n\n\n\nbeta2 &lt;- solve(t(X2t) %*% X2t) %*% t(X2t) %*% y\nbeta2\n\n         [,1]\n[1,] 5.178575\n\n\n위에서 구한 회귀계수 beta2 는 모든 독립 변수가 있는 모형에서의 \\(x_3\\) 에 대한 회귀계수 추정량과 같다.\n이제 절편, \\(x_1\\), \\(x_2\\) 만 있는 모형에서 구한 회귀계수를 위에서 구한 beta2를 이용하여 보정해 보자. 아래 보정된 회귀계수 추정량은 모든 독립변수를 고려한 모형에서의 회귀계수 추정량과 같다.\n\nbeta1 &lt;- lm1s$coefficients - solve(t(X1) %*% X1) %*% t(X1) %*% X2 %*% beta2\nbeta1\n\n                   [,1]\n(Intercept) -0.05735613\nx1           3.15491311\nx2           4.16172306\n\n\n이제 앞에서 본 예제와 같이 특별하게 1개의 설명변수를 추가하는 경우를 알아보자. 이 경우는 추가된 변수에 대한 계획행렬 \\(\\pmb  X_2 = \\pmb  x_{p}\\)는 하나의 벡터이다. 따라서\n\\[  \n  \\pmb  y = \\pmb  X_1  \\pmb  \\beta_1  + \\pmb  X_2 \\pmb  \\beta_2 = \\pmb  X_1  \\pmb  \\beta  + {\\pmb  x}_p  \\beta_p  + \\pmb  e\n\\tag{3.12}\\]\n위의 식 3.8 에서\n\\[\n\\tilde {\\pmb X}_2 =  (\\pmb  I -{\\pmb  H}_1  ) {\\pmb  x}_p \\equiv \\tilde {\\pmb x}_p\n\\] 로 정의하면 회귀식 식 3.12 에서 하나 추가된 설명변수에 대한 회귀계수의 추정량은 다음과 같다.\n\\[\n\\begin{aligned}\n\\hat { \\beta}_p & = ({\\tilde {\\pmb  X}_2}^t \\tilde {\\pmb  X}_2)^{-1} {\\tilde {\\pmb  X}_2}^t   \\pmb  y  \\\\\n  & = [ \\tilde {\\pmb  x}_p^t  \\pmb  x_p ]^{-1}  \\tilde {\\pmb  x}_p^t \\pmb  y \\\\\n  & = [ \\tilde {\\pmb  x}_p^t  \\pmb  x_p ]^{-1}  {\\pmb  x}_p^t ( \\pmb  I - \\pmb  H_1)  \\pmb  y \\\\\n  & = [ \\tilde {\\pmb  x}_p^t  \\pmb  x_p ]^{-1}  {\\pmb  x}_p^t ( \\pmb  I - \\pmb  H_1) (\\pmb  I -\\pmb  H_1  ) \\pmb  y \\\\\n  & = [ \\tilde {\\pmb  x}_p^t  \\pmb  x_p ]^{-1} \\tilde {\\pmb  x}_p^t ( \\pmb  I - \\pmb  H_1) \\pmb  y \\\\\n  & = [ \\tilde {\\pmb  x}_p^t  \\pmb  x_p ]^{-1} \\tilde {\\pmb  x}_p^t \\tilde {\\pmb  y} \\\\\n  & = \\frac{ \\tilde {\\pmb  x}_p^t \\tilde {\\pmb  y} }{ \\tilde {\\pmb  x}_p^t \\tilde {\\pmb  x}_p }\n\\end{aligned}\n\\]\n위의 식에서 \\(\\tilde {\\pmb  y} =  ( \\pmb  I - \\pmb  H_1){\\pmb  y}\\) 이다.\n또한 식 3.12 과 같이 새로운 변수 \\(x_p\\) 가 추가되면 새로운 변수가 추가된 후에는 회귀계수 추정량이 다음과 같아 보정된다.\n\\[\n\\begin{aligned}\n\\hat {\\pmb  \\beta}_1 & =({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X_1}^t [\\pmb  y -  \\pmb  X_2 \\hat {\\pmb  \\beta}_2  ] \\\\\n  & = ({\\pmb  X_1}^t \\pmb  X_1)^{-1} {\\pmb  X_1}^t \\pmb  y -  ({\\pmb  X_1}^t \\pmb  X)_1^{-1} {\\pmb  X}_1^t \\pmb  x_p \\frac{ \\tilde {\\pmb  x}_p^t \\tilde {\\pmb  y} }{ \\tilde {\\pmb  x}_p^t \\tilde {\\pmb  x}_p }  \\\\\n  & = ({\\pmb  X}_1^t \\pmb  X)_1^{-1} {\\pmb  X}_1^t \\pmb  y -  ({\\pmb  X}_1^t \\pmb  X)_1^{-1} {\\pmb  X}_1^t [\\pmb  x_p (\\tilde {\\pmb  x}_p^t \\tilde {\\pmb  x}_p )^{-1} \\tilde {\\pmb  x}_p^t ] \\tilde {\\pmb  y} \\\\\n  & = \\hat {\\pmb \\beta}_{1*} -  ({\\pmb  X}_1^t \\pmb  X)_1^{-1} {\\pmb  X}_1^t [\\pmb  x_p (\\tilde {\\pmb  x}_p^t \\tilde {\\pmb  x}_p )^{-1} \\tilde {\\pmb  x}_p^t ] \\tilde {\\pmb  y}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>모형의 비교</span>"
    ]
  },
  {
    "objectID": "qmd/inference2.html#부분-f-검정과-가능도비-검정",
    "href": "qmd/inference2.html#부분-f-검정과-가능도비-검정",
    "title": "3  모형의 비교",
    "section": "3.3 부분 F-검정과 가능도비 검정",
    "text": "3.3 부분 F-검정과 가능도비 검정\n앞 절에서 회귀모형에 새로운 독립변수를 1개 이상 추가할 경우 회귀계수 추정량과 제곱합의 변화를 살펴보있다.\n실제 자료를 분석하여 회귀 모형식을 만드는 경우 일반적으로 중요한 몇 개의 설명변수부터 모형에 포함시키고 다른 변수들을 추가한다. 반대로 중요한 변수에 대한 사전 정보가 없다면 가능한 모든 변수를 모두 포함시킨 후에 중요하지 않은 변수들을 제거하기도 한다. 이런 두 가지 방법 모두 축차적으로 변수를 추가 또는 제거하는 방법으로 고려하는 모형들이 포함 관계를 가진다.\n이렇게 포함관계를 가지는 두 모형을 고려해 보자. 먼저 설명변수의 수가 많은 모형을 최대 모형(full model)이라고 부르자.\n\\[ y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots \\beta_{p-1} x_{i,p} + \\beta_p x_{i,p} + \\dots + \\beta_{p+q} + e_i \\]\n위의 식은 모두 절편을 제외하면 모두 \\(p+q\\) 개의 설명변수를 가진 선형 모형이다. 위의 최대모형을 다음과 같은 행렬식으로 써보자\n아래에 정의된 최대\n\\[\n\\pmb  y= \\pmb  X {\\pmb  \\beta} + \\pmb  e, , \\quad \\pmb  e \\sim N(\\pmb  0, \\sigma^2 \\pmb  I_n)  \\quad \\text{ Full model}\n\\tag{3.13}\\]\n이제 축소된 모형으로 최대모형에서 마자막 \\(q\\)개의 설명변수가 모형에 포함되지 않은 경우를 생각하자.\n\\[ y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots \\beta_{p-1} x_{i,p} + \\beta_p x_{i,p} + e_i \\]\n축소모형은 다음과 같은 행렬식으로 표시한다.\n\\[\n\\pmb  y= \\pmb  X_1 {\\pmb  \\beta_1} + \\pmb  e, , \\quad \\pmb  e \\sim N(\\pmb  0, \\sigma^2 \\pmb  I_n)  \\quad \\text{ Reduced Model}\n\\tag{3.14}\\]\n참고로 최대모형 식 3.13 의 계획행렬 \\(\\pmb  X = [\\pmb  X_1, \\pmb  X_2]\\) 의 차원은 \\(n \\times (p+q+1)\\)이고 축소 모형 식 3.14 의 계획행렬 \\(\\pmb  X_1\\) 의 차원은 \\(n \\times (p+1)\\) 이다.\n여기서 유의할 점은 최대 모형은 축소모형을 포함한다는 것이다. 만약 최대모형에서 마지막 \\(q\\)개의 설명변수들에 대한 회귀계수들이 모두 0이면, 즉 \\(\\beta_{p+1} = \\cdots \\beta_{p+q}=0\\) 이면 축소모형이 된다.\n교재에서는 추가제곱합을 이용한 부분 F-검정(교과서 p.158-161)을 설명한다. 즉, 다음과 같은 가설을 검정하고자 한다.\n\\[\nH_0: \\beta_{p+1} = \\beta_{p+2} =\\cdots = \\beta_{p+q}=0 \\text{ (Reduced)} \\quad \\text{ vs.} \\quad H_1: \\text{ not } H_0 \\text{  (Full)}  \n\\tag{3.15}\\]\n위와 같은 가설을 검정하기 위한 부분 F-검정의 검정 통계량 \\(F_0\\) 는 다음과 같이 주어진다 (교과서 식 4.4).\n\\[\nF_0 = \\frac{[SSE(R) - SSE(F)]/(df_R - df_F)}{SSE(F)/df_F}   \n\\]\n만약 \\(H_0\\) 가 참이면 검정 통계량 \\(F_0\\) 는 자유도가 각각 \\(df_R-df_F\\)와 \\(df_F\\) 를 가지는 F-분포를 따르므로 이를 이용하여 모형에 새로운 변수를 추가하는 검정을 수행하는 부분 F-검정을 실시할 수 있다.\n참고로 위의 식들에서 자유도 \\(df_R\\) 과 \\(df_F\\)는 다음과 같이 주어진다.\n\\[ df_R = n - (p+q+1), \\quad df_F = n - (p+1)\\]\n선형모형에 대한 최대 가능도 추정법은 장 1 에서 설명하였다. 위의 최대 모형과 축소모형에 대한 최대가능도 추정법을 설명하기 위하여 다음과 같은 식을 사용할 것이다.\n임의의 벡터 \\(\\pmb  v\\)에 대하여 노름 \\(\\norm{\\pmb  v}^2\\) 을 다음과 같아 정의한다. \\[ \\norm{\\pmb  v}^2 = \\pmb  v^t \\pmb  v \\] 최대모형에 대한 계획행렬 \\(\\pmb  X\\) 에 대한 모자행렬을 이용하여 사영행렬 \\(\\pmb  P\\) 와 \\(\\pmb  Q\\)를 다음과 같이 정의한다.\n\\[ \\pmb  P \\equiv \\pmb  H(\\pmb  X) = \\pmb X ({\\pmb X}^t {\\pmb X} )^{-1} {\\pmb X}^t , \\quad \\pmb  Q = \\pmb  I - \\pmb  P  \\]\n또한 축소모형의 계획행렬 \\(\\pmb  X_1\\)에 대한 모자행렬을 이용하여 사영행렬 \\(\\pmb  P_1\\) 와 \\(\\pmb  Q_1\\)를 다음과 같이 정의한다.\n\\[ \\pmb  P_1 \\equiv \\pmb  H_1(\\pmb  X_1) = \\pmb X_1 ({\\pmb X}_1^t {\\pmb X}_1 )^{-1} {\\pmb X}_1^t,  \\quad \\pmb  Q_1 = \\pmb  I - \\pmb  P_1 \\]\n분산에 대한 모수를 \\(\\tau=\\sigma^2\\) 과 같이 쓰고 편의상 반응 벡터의 평균을 다음과 같이 표시하자\n\\[ \\pmb  \\mu = \\pmb  X  \\pmb  \\beta, \\quad  \\pmb  \\mu_1 = \\pmb  X_1  \\pmb  \\beta_1  \\]\n이제 최대모형 식 3.13 에 대한 최대 가능도 추정을 생각해 보자.\n\\[\n\\begin{aligned}\n\\ell_n( \\pmb  \\theta_F; \\pmb  y)\n   &= -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac{1}{2\\tau} ( \\pmb  y- \\pmb  X  \\pmb  \\beta)^t ( \\pmb  y- \\pmb  X  \\pmb  \\beta)   \\\\\n   & =  -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac {1}{2\\tau} \\norm{ \\pmb  y- \\pmb  X  \\pmb  \\beta }^2   \\\\\n   & =  -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac {1}{2\\tau} \\norm{ \\pmb  y- \\pmb  \\mu }^2    \\\\\n   & =  -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac {1}{2\\tau} \\norm{ \\pmb  y- \\pmb  P \\pmb  y + \\pmb  P \\pmb  y - \\pmb  \\mu }^2   \\\\\n   & =  -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac {1}{2\\tau} \\left [ \\norm{ \\pmb  y- \\pmb  P \\pmb  y}^2 + \\norm{\\pmb  P \\pmb  y - \\pmb  \\mu }^2 \\right ]   \\\\\n      & =  -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac {1}{2\\tau} \\left [ \\norm{ \\pmb  Q \\pmb  y}^2 + \\norm{\\pmb  P \\pmb  y - \\pmb  \\mu }^2 \\right ]   \n\\end{aligned}\n\\]\n위의 최대 모형에 대한 로그 가능도 함수 \\(\\ell_n( \\pmb  \\theta; \\pmb  y)\\)에서 \\(\\mu = \\pmb  X \\pmb  \\beta\\)에 대한 최대가능도 추정량과 모분산 \\(\\tau\\) 에 대한 추정량은 다음과 같아 주어진다(장 1 참조)\n\\[ \\hat \\mu = \\pmb  X \\hat {\\pmb  \\beta} = \\pmb  P \\pmb  y, \\quad \\hat \\tau_F = \\frac{1}{n} \\norm{ \\pmb  Q \\pmb  y}^2  = \\frac{1}{n} SSE(F)\\]\n따라서 최대 가능도 추정량 \\(\\hat {\\pmb  \\mu}\\) 와 \\(\\hat {\\tau}_F\\)를 로그 가능도 함수에 넣으면 다음과 같은 결과가 주어진다.\n\\[\n\\underset{\\pmb  \\mu, \\pmb  \\tau }{\\max} \\ell_n( \\pmb  \\theta_F; \\pmb  y)  = -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\hat \\tau_F - \\frac{n}{2}\n\\]\n위의 결과를 가능도 함수로 표시하면\n\\[\n\\underset{\\pmb  \\mu, \\pmb  \\tau }{\\max} L_n( \\pmb  \\theta_F; \\pmb  y)  =(2 \\pi e)^{-n/2} [SSE(F)/n]^{-n/2}\n\\tag{3.16}\\]\n축소모형 식 3.14 에 대해서도 같은 방법으로 최대 가능도 춛정량을 구하면 다음과 같이 주어지며\n\\[ \\hat \\mu_1 = \\pmb  X_1 \\hat {\\pmb  \\beta_1} = \\pmb  P_1 \\pmb  y, \\quad \\hat \\tau_R = \\frac{1}{n} \\norm{ \\pmb  Q_1 \\pmb  y}^2 = \\frac{1}{n} SSE(R)   \\]\n로그 가능도 함수에 넣어면 다음과 같은 결과가 주어진다.\n\\[\n\\underset{\\pmb  \\mu_1, \\pmb  \\tau }{\\max} \\ell_n( \\pmb  \\theta_R; \\pmb  y)  = -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\hat \\tau_R - \\frac{n}{2}\n\\]\n위의 결과를 가능도 함수로 표시하면 \\[\n\\underset{\\pmb  \\mu, \\pmb  \\tau }{\\max} L_n( \\pmb  \\theta_R; \\pmb  y)  =(2 \\pi e)^{-n/2} [SSE(R)/n]^{-n/2}\n\\tag{3.17}\\]\n최대 가능도 추정에서 만약 최대 모형 \\(F\\) 가 축소모형 \\(R\\) 을 포함하면 가설 식 3.15 에 대한 검정이 가능하다.\n위의 가설 검정에서 두 모형의 가능도 함수 식 3.16 와 식 3.17 의 비, 즉 가능도 비(Likelihood Ratio) \\(\\Lambda\\) 는 다음과 같이 주어진다.\n\\[\n\\Lambda= \\frac{ \\underset{\\pmb  \\mu_1, \\pmb  \\tau }{\\max} L_n( \\pmb  \\theta_R; \\pmb  y)}{\\underset{\\pmb  \\mu, \\pmb  \\tau }{\\max} L_n( \\pmb  \\theta_F; \\pmb  y) } =  \\left [ \\frac{SSE(F)}{SSE(R)} \\right ]^{n/2}\n\\tag{3.18}\\]\n위의 가능도 비 \\(\\Lambda\\)가 작으면 귀무가설 \\(H_0\\)을 기각한다.\n\\[ \\text{if } \\Lambda &lt; c', \\text{ then reject } H_0 \\]\n식 식 3.18 에 나타난 가능도 비를 다시 표현해 보자\n\\[ \\Lambda = \\left [ \\frac{SSE(F)}{SSE(R)} \\right ]^{n/2} = \\left [ 1 +\\frac{SSE(R) - SSE(F)}{SSE(F)} \\right ]^{-n/2} \\equiv (1 + F^*)^{-n/2} \\] 위의 식을 보면 \\(\\Lambda\\)가 \\(F^*\\)에 반비례하므로 다음과 같이 \\(F^*\\)가 크면 \\(H_0\\)를 기각할 수 있다. 이제 회귀분석에서 주로 쓰이는 부분 F-검정 통계량을 위의 식에서 나타난 \\(F^*\\)로 표시해보면 다음과 같이 쓸수 있다. \\[\n\\text{if } F = \\frac{n-(p+q+1)}{q} F^* = \\frac{[SSE(R) - SSE(F)]/q}{SSE(F)/[n-(p+q+1)]} &gt; c, \\text{ then reject } H_0  \n\\tag{3.19}\\]\n위의 식 3.19 에 있는 가설검정의 절차는 추가제곱합을 이용한 부분 F-검정(교과서 p.158-161)과 동일한 검정이다. 따라서 부분 F-검정은 가능도비 검정이다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>모형의 비교</span>"
    ]
  },
  {
    "objectID": "qmd/modeleval2.html",
    "href": "qmd/modeleval2.html",
    "title": "4  모형의 진단",
    "section": "",
    "text": "4.1 등분산성 가정의 위반\n일반적인 회귀분석모형에서 \\[ \\pmb y = \\pmb X \\pmb \\beta + \\pmb e \\]\n오차항이 다음과 같이 서로 독립이고 등분산성을 만족한다면 \\[ var(\\pmb e) =\\sigma^2 \\pmb I_n \\]\n최소제곱법에 의한 회귀계수 추정량 \\(\\hat  {\\pmb \\beta}\\) 다음과 같고\n\\[  \\hat {\\pmb \\beta}  = (\\pmb X^t\\pmb X)^{-1}\\pmb X^t\\pmb y \\]\n이는 최소분산선형추정량(BLUE)이다. 만약에 오차항에 대한 가정이 만족하지 않는다면 최소제곱법 추정량 \\(\\hat {\\pmb \\beta}\\)의 최적성이 유지되는가에 대한 질문이 생기게 된다.\n여기서 오차항의 분산에 대하여 좀더 일반적인 모형을 생각해보자. 가장 일반적인 모형은 다음과 같은 임의의 양정치행렬(positive definite matrix) \\(\\pmb V\\)가 오차항의 공분산 행렬인 경우이다.\n\\[ Var(\\pmb e) = \\pmb V \\] 가장 일반적인 경우를 고려하기 전에 전형적인 가정을 약간 벗어나면서 실제 문제에서 흔히 접하는 경우를 생각해 보자.\n일단 오차항이 서로 독립이지만 분산이 다른 경우이다.\n\\[ Var(\\pmb e) = \\text{diag} [\\sigma_1^2,\\sigma_2^2,\\dots, \\sigma_n^2 ]  \\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>모형의 진단</span>"
    ]
  },
  {
    "objectID": "qmd/modeleval2.html#등분산성-가정의-위반",
    "href": "qmd/modeleval2.html#등분산성-가정의-위반",
    "title": "4  모형의 진단",
    "section": "",
    "text": "4.1.1 가중 최소제곱법\n이러한 경우에 가중 최소제곱법(Weighted Leadt Square Estimator; WLSE)을 사용하면 최소제곱법 추정량의 최적성을 유지할 수 있다.\n오차항의 분산 \\(\\sigma_1^2,\\sigma_2^2,\\dots, \\sigma_n^2\\)을 안다고 가정하면 각 관측값 \\(y_i\\)를 해당 오차의 표분편차 \\(\\sigma_i\\)로 나누면 등분산성을 다시 얻을 수 있다.\n\\[ Var (y_i) = Var (\\pmb x_i^t \\pmb \\beta + e_i)=Var ( e_i)=\\sigma_i^2 \\]\n\\[ \\Rightarrow Var (y_i/\\sigma_i) = Var (\\pmb x_i^t \\pmb \\beta' + e_i/ \\sigma_i)=Var ( e_i/\\sigma_i) = 1 \\]\n이 떄 새로운 관측치 \\(y'_i = y_i/\\sigma_i\\)를 사용하여 최소제곱법을 적용하면\n\\[ \\min_{\\pmb \\beta'} \\sum_{i=1}^n (y'_i - \\pmb x_i^t \\pmb \\beta')^2 = \\min_{\\pmb \\beta}  \\sum_{i=1}^n \\left [\\frac{1}{\\sigma_i^2} \\right ](y_i - \\pmb x_i^t \\pmb \\beta)^2 \\equiv \\min_{\\pmb \\beta}  \\sum_{i=1}^n w_i (y_i - \\pmb x_i^t \\pmb \\beta)^2\\]\n여기서 \\(w_i=1/\\sigma^2_i\\)이다. 이러한 가중최소제곱법은 각 관측치에 대하여 서로 다른 가중치를 적용하여 최소제곱 추정량을 구하며 위의 경우에는 가중치가 반응값의 분산에 반비례한다. 따라서 분산이 큰 오차항을 가진 반응값의 가중치는 분산이 작은 반응값에 비해 상대적으로 작다. 이러한 가중치와 분산의 관계는 변이가 적은 반응값 근방에서 오차를 더욱 줄이려고 하는 직관적인 생각과 일치한다. 가중치를 적용한 최소제곱 추정량은 다음과 같이 나타낼 수 있다.\n\\[ \\hat  {\\pmb \\beta}= (\\pmb X^t \\pmb W \\pmb X)^{-1}\\pmb X^t \\pmb W \\pmb y \\] 여기서\n\\[ \\pmb W = \\text{diag} \\left [\\frac{1}{\\sigma_1^2},\\frac{1}{\\sigma_2^2},\\dots, \\frac{1}{\\sigma_n^2} \\right ] = [Var(\\pmb e)]^{-1} =\\pmb V^{-1} \\]\n위에서 본 가중최소제곱법을 오차항이 일반적인 공분산 행렬 \\(\\pmb V\\)를 가질 때 적용하면 다음과 같이 목적 함수를 나타낼 수 있으며\n\\[ \\min_{\\pmb \\beta} (\\pmb y - \\pmb X \\pmb \\beta)^t \\pmb V^{-1} (\\pmb y - \\pmb X \\pmb \\beta) \\]\n가중최소제곱추정량은 다음과 같이 나타낼 수 있다.\n\\[\n\\hat  {\\pmb \\beta}_* = (\\pmb X^t \\pmb V^{-1} \\pmb X)^{-1} \\pmb X^t \\pmb V^{-1} \\pmb y\n\\tag{4.1}\\]\n가중최소제곱추정량은 다음과 같은 성질은 만족한다.\n\n가중최소제곱추정량은 불편추정량이다: \\(E(\\hat  {\\pmb \\beta}_*) =\\pmb \\beta\\)\n가중최소제곱추정량 \\(\\hat  {\\pmb \\beta}_*\\)는 분산이 가장 작은 불편선형 추정량이다 (Gauss-Markov Theorem)\n가중최소제곱추정량 \\(\\hat  {\\pmb \\beta}_*\\)의 분산:\n\n\\[ Var(\\hat  {\\pmb \\beta}_*) =  (\\pmb X^t \\pmb V^{-1} \\pmb X)^{-1} \\]\n\n가중최소제곱추정량 \\(\\hat  {\\pmb \\beta}_*\\)은 \\(\\pmb y\\)가 평균이 \\(\\pmb X \\pmb \\beta\\)이고 분산이 \\(V\\)인 정규분포에서 \\(\\pmb \\beta\\)에 대한 최대우도추정량이다.\n일반최소제곱추정량도 불편추정량이다: \\(E(\\hat  {\\pmb \\beta}) =\\pmb \\beta\\)\n\n가중최소제곱법에서 유의할 점은 오차항의 공분산 행렬 \\(\\pmb V\\)에 대하여 모르는 경우 이를 추정해야 하며 가중최소제곱추정량에 공분산 행렬의 추정치\n\\(\\hat {\\pmb V}\\)을 사용할 경우 위에서 언급한 최적성은 더 이상 성립하지 않는다.\n\\[ \\hat {\\pmb \\beta}_* = (\\pmb X^t \\hat {\\pmb V}^{-1} \\pmb X)^{-1}\\pmb X^t \\hat {\\pmb V}^{-1} \\pmb y \\]\n더 나아가 공분산행렬 \\(\\pmb V\\)에 대한 추정은 어려운 문제이므로 그 추정 방법과 통계적 성질을 잘 고려하여 사용해야 한다. 정확한 추정을 위해서 또는 자료의 특성을 이용하여 공분산 행렬 \\(\\pmb V\\)에 대한 모형을 어느 정ㅇ도 단순화하는 것이 바람직하다. 예를 들어 공분산 행렬 \\(\\pmb V\\)를 다음과 같이 나타낼 수 있다면 유용할 것이다.\n\\[ \\pmb V = \\sigma^2 \\text{diag} [v_1, v_2,\\dots, v_n ] \\] 여기서 \\((v_1,v_2,\\dots,v_n)\\)은 알려진 값이고 \\(\\sigma^2\\)는 추정해야하는 모수이다.\n오차항의 등분산성에 대한 가정을 검토하기 위한 방법중 가장 유용하고 간단한 방법은 잔차그림을 이용하는 것이다. 잔차 \\(r_i\\)와 적합된 값 \\(\\hat y_i\\)에 대한 잔차그림을 그려서 잔차의 퍼진 정도가 적합된 값 \\(\\hat y_i\\)에 따라 변하면 등분산성에 대한 가정을 의심해봐야 한다. 실제 자료에서 반응값의 분산이 독립변수에 비례하여 나타나는 경우가 많다. 단순회귀모형을 고려하고\n\\[ y_i= \\beta_0 + \\beta_1 x_i + e_i \\] 오차항이 서로 독립이며 그 분산이 독립변수에 비례한다고 가정하자.\n\\[ Var (e_i) = x_i \\sigma^2 \\] 이러한 경우는 독립변수의 값이 양인 경우이며 독립변수의 값이 커지면 반응값의 분산도 커진다. 이러한 경우 독립변수와 종속변수의 관계, 회귀식, 잔차그림은 다음과 같이 나타난다.\n\n\n\n\n\n오차항의 등분산성이 위반된 경우",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>모형의 진단</span>"
    ]
  },
  {
    "objectID": "qmd/modeleval2.html#변수변환",
    "href": "qmd/modeleval2.html#변수변환",
    "title": "4  모형의 진단",
    "section": "4.2 변수변환",
    "text": "4.2 변수변환\n변수변환(Variable transformation)은 독립변수와 종속변수를 변환함으로서 회귀식의 적합도를 향상시켜 예측력을 높일 수 있을뿐 아니라 최소제곱법에서의 등분산성 가정에 대한 만족도를 높일 수 있는 유용한 방법이다 (variance stabilization). 이 절에서는 변수변환의 종류와 그 효과를 단순회귀식에서 살펴본다. 중회귀의 경우에는 변수변환의 적용을 복합적으로 고려해야 할 것이다.\n\n4.2.1 지수모형과 멱함수: 로그변환\n회귀식에 대한 모형이 지수함수 모형인 경우, 즉 독립변수와 종속변수가 다음과 같은 경우 \\[ y = \\beta_0 \\exp (\\beta_1 x) \\] 종속변수에 대한 로그 변환(log transformation)을 하면 선형관계에 매우 가깝게 된다 ( 아래 그림 참조). \\[ \\log(y_i) = \\beta,_0 + \\beta_1 x_i + e_i \\] 여기서 주의할 점은 원래의 지수모형에 오차항의 지수함수가 곱해지는 형태가 되어야 로그 변환후에 등분산성의 가정을 만족하게 된다. 즉 오차항 \\(e_i\\)를 서로 독립이고 평균이 0, 분산이 \\(\\sigma^2\\)이라고 하면 다음과 같은 관계가 성립된다. \\[ y_i = \\beta_0 \\exp (\\beta_1 x_i)\\exp(e_i) \\quad \\Rightarrow \\quad \\log(y_i) = \\beta'_0 + \\beta_1 x_i + e_i \\]\n\n\n\n\n\n지수모형과 로그변환\n\n\n\n\n회귀식에 대한 모형이 멱함수모형인 경우, 즉 독립변수와 종속변수가 다음과 같은 경우 \\[ y = \\beta_0  x^\\beta_1 \\] 독립변수와 종속변수에 대한 로그 변환을 하면 선형관계에 매우 가깝게 된다. \\[ \\log(y_i) = \\beta,_0 + \\beta_1 \\log(x_i) + e_i \\] 여기서 주의할 점도 원래의 멱함수모형에 오차항이 곱해지는 형태가 되어야 로그 변환후에 등분산성의 가정을 만족하게 된다. 즉 오차항 \\(e_i\\)를 서로 독립이고 평균이 0, 분산이 \\(\\sigma^2\\)이라고 하면 다음과 같은 관계가 성립된다. \\[ y_i = \\beta_0  x^\\beta_1 \\exp(e_i) \\quad \\Rightarrow \\quad \\log(y_i) = \\beta'_0 + \\beta_1 \\log(x_i) + e_i \\]\n회귀식에 대한 모형이 역지수함수 모형인 경우, 즉 독립변수와 종속변수가 다음과 같은 경우 \\[ y = \\beta_0 \\exp (\\beta_1/ x) \\] 종속변수에 대한 로그 변환과 독림변수에 대한 역변환을 하면 선형관계에 매우 가깝게 된다. \\[ \\log(y_i) = \\beta,_0 + \\beta_1 \\left ( \\frac{1}{x_i} \\right ) + e_i \\] 여기서 주의할 점은 원래의 역지수모형에 오차항의 지수함수가 곱해지는 형태가 되어야 로그 변환후에 등분산성의 가정을 만족하게 된다.\n\n\n\n\n\n역지수모형과 로그/역변환\n\n\n\n\n\n\n4.2.2 쌍곡선과 역변환\n생물학, 경제학등의 문야에서 독립변수와 종속변수의 관계가 쌍곡선(Hyperbola) 형태인 경우가 많다. 독립변수가 증가함에 따라 종속변수의 값이 수렴하는 경우에 이러한 관계가 매우 유용하다. \\[ y = \\frac{x}{\\beta_0 + \\beta_1 x} \\] 이러한 경우에 독립변수와 종속변수에 모두 역변환(Inverse transformation)을 취하면 선형관계에 매우 가깝게 된다 (아래 그림 참조). \\[ \\frac{1}{y_i} = \\beta_0 + \\beta_1 \\left ( \\frac{1}{x_i} \\right ) + e_i \\]\n\n\n\n\n\n쌍곡선과 역변환\n\n\n\n\n위에서 살펴본 회귀모형들에서 변수변환후에 등분산성에 대한 가정을 만족하려면 대부분 변환전의 함수관계식에 오차항의 지수함수가 곱해지는 형태가 되어야 한다 (multiplicative error model). 이렇게 오차항이 함수관계식에 곱해지는 형태가 아니 다른 형태라면, 예를 들어 오차항이 원래의 함수관계식에 더해지는 형태 (additive error model), 등분산성의 가정이 상당히 위배될 수 있음을 주의해야 한다. 예를 들어 회귀식에 대한 모형이 지수함수 모형인 경우 서로 독립이고 평균이 0, 분산이 \\(\\sigma^2\\)인 오차항 \\(e_i\\)를 함수 관계식에 더해졌다면 로그변환된 종속 변수와 독립변수는 선형관계를 보이지만 등분산성 가정은 만족하지 못하게된다.\n\\[ y_i = \\beta_0 \\exp (\\beta_1 x_i) +e_i \\quad \\Rightarrow \\quad \\log(y_i) \\cong \\beta'_0 + \\beta_1 x_i + e^*_i \\]\n\n\n\n\n\nAdditive error model 과 로그변환\n\n\n\n\n\n\n4.2.3 Box-Cox 변환\n앞 절에서 보았듯이 종속변수에 로그변환 등을 적용하면 여러 가지 유용한 점이 많다. 위에서 살펴본 종속변수에 변환은 여러 가지 비선형모형을 선형모형에 가깝게 만들어 주며 Multiplicative error model과 같이 반응값이 분산이 독립변수의 크기에 영향을 받는 모형을 등분산성을 가진 형태의 모형으로 버꾸어 준다 (Variance stabilization). 이렇게 종속변수에 대한 여러 가지 변환을 하나의 체계적인 형태로 결합한것을 Box-Cox 변환으로 부르며 다음과 같아 정의한다.\n\\[\ny^{(\\lambda)} =\n\\begin{cases}\n\\log(y) & \\text{ if } \\lambda=0 \\\\\n\\frac{y^\\lambda -1}{\\lambda} & \\text{ if } \\lambda \\ne 0\n\\end{cases}\n\\tag{4.2}\\]\nBox-Cox 변환은 로그변환과 멱변환을 모두 포함하고 있다. 또한 Box-Cox 변환된 \\(y^{(\\lambda)}\\)가 정규분포를 따른다는 가정 하에 자료가 주어졌을때 \\(\\lambda\\)에 대한 최대우도추정량을 구할 수 있다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>모형의 진단</span>"
    ]
  },
  {
    "objectID": "qmd/modeleval2.html#다중공선성",
    "href": "qmd/modeleval2.html#다중공선성",
    "title": "4  모형의 진단",
    "section": "4.3 다중공선성",
    "text": "4.3 다중공선성\n회귀분석에서 독립변수들간의 강한 선형 관계의 경향이 있을 때 이를 다중공선성(multicollinearity)라고 한다. 즉, \\(p\\)개의 독립변수 \\(x_1,x_2,\\dots,x_p\\)의 관계가 다음과 같은 선형관계에 가깝다면 다중공선성이 존재한다고 한다.\n\\[ c_1 x_1 + c_2 x_2 + \\dots + c_p x_p \\cong 0 \\]\n다중공선성에 의해 발생하는 여러 가지 문제점들을 기술적으로 독립변수들의 강한 선형관계때문에 행렬 \\(X^tX\\)가 ill-conditioned 행렬이 되어 그 역행렬이 불안정하게 구해지는 결과 때문에 생기게 된다. 여기서 회귀계수의 공분산 행렬은 다음과 같이 주어짐에 유의하자.\n\\[ Var(\\pmb b) = \\sigma^2 (X^t X)^{-1} \\]\n예를 들어서 두 개의 독립변수가 있는 회귀 모형을 생각해 보자.\n\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e \\]\n그림 4.1 에서 (a)의 경우는 두 개의 독립변수 \\(x_1\\)과 \\(x_2\\)의 관계가 독립적이어서 (linearly independent) 적합된 회귀식이(그림에서 2차원 평면) 안정적이다. 반면에 그림 4.1 에서 (b)의 경우는 두 개의 독립변수 \\(x_1\\)과 \\(x_2\\)가 완벽한 선형관계가 있기 떄문에 (linearly dependent)\n\\[ c_1 x_1 + c_2 x_2 = 0 \\] 적합된 회귀식이 여러가지 존재한다. 이러한 경우는 매우 드물지만 그림 4.1 에서 (c)의 경우는 두 개의 독립변수 \\(x_1\\)과 \\(x_2\\)가 선형관계에 매우 가깝기 때문에 적합된 회귀식이 불안정하다.\n\n\n\n\n\n\n\n\n그림 4.1: 회귀분석에서의 다중공선성의 정도(Fox 2008, p310)\n\n\n\n\n\n회귀분석에서 다중공선성의 정도를 측정할 수 있는 통계량은 다음과 같은 것들이 있다.\n\n4.3.1 독립변수간의 상관계수\n독립변수간의 상관계수를 보아 강한 상관관계를 가지는 변수들이 있다면 다중공선성의 가능성이 크다.\n\n\n4.3.2 \\(\\pmb X^t \\pmb X\\) 의 고유값(Eigenvalues)\n행렬 \\(\\pmb X^t \\pmb X\\)의 고유값를 구하여 큰 순서대로 나열했을 때 가장 작은 값이 0에 매우 가까우면 다중공선성의 가능성이 크다. 만약에 독립변수들간의 선형 관계가 있다면 행렬 \\(\\pmb X^t \\pmb X\\)은 최대 계수(full rank) 행렬이 아니므로 하나 이상의 고유값이 0이 되게 된다.\n이제 \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p\\)를 \\(\\pmb X^t \\pmb X\\)의 고유값라고 하자. 이런 가정 하에서 \\(1/\\lambda_i\\)는 \\((\\pmb X^t \\pmb X)^{-1}\\)의 고유값이다. \\(\\pmb X^t \\pmb X\\)의 고유값에 대한 고유벡터를 \\(\\pmb p_1, \\pmb p_2,\\dots,\\pmb p_p\\)라고 하고 행렬 \\(\\pmb P=[\\pmb p_1, \\pmb p_2,\\dots,\\pmb p_p]\\)로 정의하자. 이때 다음과 같이 스펙트렇 분해를 이용하여 \\(\\pmb X^t \\pmb X\\)를 나타낼 수 있다.\n\\[ \\pmb P^t (\\pmb X^t \\pmb X) \\pmb P = \\pmb \\Lambda = \\text{diag}(\\lambda_1 , \\lambda_2 , \\dots , \\lambda_p) \\] 또한\n\\[ (\\pmb X^t \\pmb X)^{-1} =  \\pmb P \\pmb \\Lambda^{-1}\\pmb P^t  \\] 따라서 가장 작은 고유값 \\(\\lambda_p\\)가 매우 0에 가까우면 \\((\\pmb X^t \\pmb X) \\pmb p_p =\\lambda_p \\pmb p_p \\approx 0\\)이 성립하고 이는 \\(\\pmb X \\pmb p_p \\approx 0\\)을 의미하여 독립변수간에 선형관계가 있다는 것을 암시한다.\n일반적으로 최소제곱추정량의 공분산은 다음과 같아 나타내어지고\n\\[ Var(\\hat {\\pmb \\beta}) = \\sigma^2 (\\pmb X^t \\pmb X)^{-1} = \\sigma^2 \\pmb P \\pmb \\Lambda^{-1}\\pmb P^t  =\n\\sigma^2  \\sum_{i=1}^p \\frac{1}{\\lambda_i} \\pmb p_i \\pmb p^t_i \\]\n최소제곱추정량의 분산의 합(total variance)은 다음과 같다.\n\\[ \\sum_{i=1}^p Var(\\hat \\beta_i) = tr[\\sigma^2 (\\pmb X^t \\pmb X)^{-1}]= \\sigma^2 tr[ (\\pmb X^t \\pmb X)^{-1}]=\n\\sigma^2 \\sum_{i=1}^p \\frac{1}{\\lambda_i} \\]\n따라서 고유값의 값이 0에 가까와 지면 추정량의 분산의 합은 매우 크게 된다.\n\n\n4.3.3 조건지수 (condition number)\n다중공선성의 판별을 위하여 행렬 \\(\\pmb X^t \\pmb X\\)의 고유값이 중요한 측도라고 했다. 고유값을 상대적으로 비교하면 다중공선성을 더 명확하게 알 수 있다. 조건지수는 가장 튼 고유값과 다른 고유값의 비율의 제곱근으로 나타내어진다.\n\\[ \\kappa_i = \\sqrt{\\frac {\\lambda_1}{\\lambda_i}}, \\quad i=2,3,\\cdots,p \\]\n중요한 역활을 하는 조건지수는 가장 작은 고유값에 의한 것이다.\n\\[ \\kappa_p =\\kappa(\\pmb X) = \\sqrt{\\frac {\\lambda_1}{\\lambda_p}} \\]\n\\(\\kappa(\\pmb X)\\)의 값이 크면 클수록 다중공선성의 가능성이 높다. 일반적으로 \\(\\kappa(\\pmb X)\\) 가 30 이상이면 다중공선성의 가능성이 크다고 본다.\n\n\n4.3.4 분산팽창계수 (Variance Inflation Factor ;VIF)\n하나의 독립변수 \\(x_i\\)를 나머지 다른 \\(p-1\\)개의 독립변수 \\(x_1,\\dots, x_{i-1}, x_{i+1},\\dots, x_p\\)를 이용하여 회귀식을 적합시킬 수 있다. 이때 다중공선성이 존재한다면 적합된 회귀식의 결정계수 \\(R^2_i\\)는 1에 매우 가까울 것이다\n\\[ x_i = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_{i-1} x_{i-1} +\\beta_{i+1} x_{i+1} + \\dots + \\beta_p x_p +e \\]\n이때 독립변수 \\(x_i\\)에 대한 VIF는 다음과 같이 정의되며 그 값이 5 또는 10보다 크면 다중공선성의 가능성이 크다.\n\\[ VIF_i = \\frac{1}{1-R_i^2} \\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>모형의 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html",
    "href": "qmd/residual.html",
    "title": "5  관측값에 대한 진단",
    "section": "",
    "text": "5.1 서론\n회귀분석을 포함한 통계적 자료분석에서 흔하게 접하는 문제는 자료 중의 일부가 통계적 모형에 의해 추정된 평균적인 경향에서 매우 벗어나 있는 점을 발견하게 되는 경우이다.\n이러한 경우 평균적인 경향에서 매우 벗어난 자료를 분석에서 제외시킬 것인지에 대한 논의도 필요할 수 있으며 이러한 자료들이 모형의 모수에 대한 추정에 어떤 영향을 미칠 것인자에 대한 검토도 필요할 수 있다.\n평균적인 경향에서 매우 벗어난 자료를 흔히 이상점(outlier)라고 부른다. 회귀분석에서 이러한 이상점은 회귀계수 추정과 그에 따른 여러 가지 통계적 추론에 많은 영향을 미친다. 따라서 이상점들이 회귀분석의 계수 추정에 어떤 영향을 얼만큼 끼치는가에 대한 검토는 매우 중요하다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#이상점의-유형",
    "href": "qmd/residual.html#이상점의-유형",
    "title": "5  관측값에 대한 진단",
    "section": "5.2 이상점의 유형",
    "text": "5.2 이상점의 유형\n일차원 자료에서는 평균적인 경향에서 매우 벗어난 자료의 식별이 단순하고 쉽다. 예를 들어 다음과 같이 일변량 자료 \\(\\pmb x\\)만 고려하면 이상점이 어떤 점인지는 쉽게 찾을 수 있다.\n\\[ \\pmb x^t = (1,2,3,4,5,10) \\]\n그러나 반응변수와 독립변수들을 고려해야 하는 회귀분석에서 이상점은 간단하게 파악하기 힘들고 상황에 따라 그 의미가 매우 다르다.\n회귀분석에서 이상점의 다양한 종류와 그 영향을 알아보기 위하여 단순회귀분석을 고려하고 다음 그림을 보자.\n\n\n\n\n\n여러가지 종류의 이상점과 그 영향\n\n\n\n\n위의 그림에서 실선은 검정색 점을 포함해서 적합한 회귀직선이고 점선은 검정색 점을 제외했을 때의 회귀직선이다\n그림의 (a)에서 검정색 점은 설명변수 \\(x\\)에 대해서는 이상점이 아니지만 \\(x\\)가 주어진 경우 반응변수 \\(y\\)에 대해서는 평균적인 경향에서 많이 벗어나 있기 때문에 이상점이다. 이러한 이상점을 회귀이상점(regression outlier)라고 부르기도 한다. (a)의 회귀이상점의 유무는 회귀계수의 추정에 크게 영향을 주지 않는다. 이렇게 어떤 관측점이 있고 없음에 따라 회귀계수의 값이 크게 변하지 않는다면 그 자료의 영향력(leverage)이 작다고 한다.\n(b)에서의 검은 점은 설명변수 \\(x\\)에 대하여 이상점이며 또한 회귀이상점이다. 더 나아가 이 이상점을 제외하고 적합한 회귀계수는 이상점을 포함했을 때 적합한 회귀계수와 매우 다르다. 이 경우 이 이상점은 큰 영향력을 가졌다고 말한다.\n(c)에서의 검은 점은 설명변수 \\(x\\)에 대해서 이상점이지만 회귀이상점은 아니다. 이러한 경우 이상점의 유무에 따라 회귀계수의 값이 크게 변하지 않으므로 이상점은 작은 영향력을 가졌다고 말한다. 하지만 (c)에서의 검은 점이 설명변수 \\(x\\)의 중심점으로부터 크게 멀어져있으므로 \\(y\\)의 값이 조그만 변해도 그림 (d)와 같이 큰 영향력을 가진다.\n관측치 \\(y_i\\) 를 제외할 때와 포함할 때의 회귀계수 추정치가 매우 다르면 그 관측치를 영향점(influential point)라고 하며 그 영향의 크기는 그 점이 가진 영향력(leverage)의 크기와 평균에서 떨어진 정도에 비례한다.\n\n자료가 선형모형의 계수 추정에 미치는 영향 \\(\\propto\\) 영향력의 크기 \\(\\times\\) 이상치의 특이한 정도\n\n위에서 살펴보았듯이 회귀분석에서 이상점의 종류와 그 영향은 매우 다양하며 복잡하다. 이러한 이상점의 종류와 회귀계수의 영향에 대하여 분석할 때 유용하게 쓰이는 통계량이 잔차(residual)이다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#지렛값",
    "href": "qmd/residual.html#지렛값",
    "title": "5  관측값에 대한 진단",
    "section": "5.3 지렛값",
    "text": "5.3 지렛값\n\\(y\\)가 반응변수이고 \\(p-1\\)개의 설명변수 \\(x_1,x_2,\\dots,x_{p-1}\\)가 있을 때 회귀식은 다음과 같이 표현된다.\n\\[ \\pmb y = \\pmb X \\pmb \\beta + \\pmb e \\]\n회귀계수 \\(\\pmb \\beta\\)의 최소제곱 추정치 \\(\\hat{ \\pmb \\beta}\\)는 다음과 같이 주어지며\n\\[ \\hat{ \\pmb \\beta} = (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y \\]\n관측값 \\(\\pmb y\\) 의 추정치 \\(\\hat {\\pmb y}\\)는 다음과 같다.\n\\[  \\hat {\\pmb y} = \\pmb X \\hat{ \\pmb \\beta} =  \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y \\equiv \\pmb H \\pmb y \\]\n여기서 \\(\\pmb H= \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t\\)를 사영행렬(hat matrix 또는 projection matrix)라고 부르며 사영행렬 \\(\\pmb H\\) 의 \\(i\\) 번째 대각원소를 \\(h_{ii}\\)라고 하며 이는 이상치 또는 영향치 분석에 중요한 역할을 한다.\n\\(i\\)번째 관측치의 설명변수 벡터를 다음과 같이 표시하면\n\\[\\pmb x_{i}^t=(1, x_{i1},x_{i2},\\dots, x_{i,p-1}) \\]\n\\(\\pmb H\\) 의 \\(i\\) 번째 대각원소를 \\(h_{ii}\\)는 다음과 같이 표현된다.\n\\[\nh_{ii} = \\pmb x_{i}^t (\\pmb X^t \\pmb X)^{-1}  \\pmb x_{i}\n\\tag{5.1}\\]\n\\(h_{ii}\\)는 \\(i\\) 번째 관측치의 설명변수 \\((x_{i1},x_{i2},\\dots, x_{i,p-1})\\) 들이 모든 관측치의 평균 \\((\\bar x_1, \\bar x_2,\\dots, \\bar x_{p-1})\\) 에서 얼마나 멀리 떨어져 있는가에 대한 상대적인 양을 나타낸다. 따라서 \\(h_{ii}\\) 를 지렛점(leverage point)라고 부른다.\n지렛점 값이 클수로 영향점일 가능성이 크며 큰 값을 높은 지렛값(high leverage point)라고 부른다.\n보통 \\(h_{ii}\\)값이 \\(p/n\\)보다 크면 영향력이 크다고 말한다. 참고로 단순 회귀식에서 \\(h_{ii}\\)는 다음과 같이 주어진다.\n\\[ h_{ii} = \\frac{1}{n} + \\frac{ ( x_i-\\bar x)^2 }{\\sum (x_i-\\bar x)^2} \\]\n또한 \\(h_{ii}\\)값을 모두 더하면 설명변수의 개수와 같다. \\[ tr(\\pmb H)=\\sum_{i=1}^k h_{ii} = p \\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#내-표준화-잔차",
    "href": "qmd/residual.html#내-표준화-잔차",
    "title": "5  관측값에 대한 진단",
    "section": "5.4 내 표준화 잔차",
    "text": "5.4 내 표준화 잔차\n잔차 \\(r_i\\)는 \\(y\\) 의 실제 관측값과 그 추정치의 차이이며\n\\[\nr_i = y_i -\\hat y_i  = y_i - {\\pmb x}_i^t \\hat {\\pmb \\beta}\n\\tag{5.2}\\]\n잔차벡터에 대한 식은 다음과 같다.\n\\[\n\\pmb r = \\pmb y - \\hat {\\pmb y} = \\pmb y - \\pmb H \\pmb y = (\\pmb I-\\pmb H)\\pmb y\n\\tag{5.3}\\]\n잔차의 공분산 행렬을 살펴보면 다음과 같이 주어진다. 따라서 \\(i\\) 번째 잔차의 분산은 \\(Var(r_i) = (1-h_{ii})\\sigma^2\\) 이다.\n\\[\nVar(\\pmb r) = \\sigma^2 (\\pmb I-\\pmb H)\n\\tag{5.4}\\]\n위에서 언급한 잔차 \\(r_i\\) 를 보통 잔차(ordinary residual)이라고 하며 그 크기가 단위에 따라 바뀌므로 잔차분석에서는 표준화 잔차(standardized residual)을 더 많이 사용한다.\n아래와 같이 잔차를 그 표준편차로 나눈 값을 내 표준화 잔차(internally studentized residual) 이라고 부른다.\n\\[\nr^s_i = \\frac{r_i}{s \\sqrt{1-h_{ii}}}\n\\tag{5.5}\\]\n위의 식에서 \\(s\\)는 오차항의 표준편차 \\(\\sigma\\)의 추정량이며 \\(h_{ii}\\)는 사영행렬 \\(\\pmb H\\) 의 \\(i\\) 번째 대각원소(즉 지렛값)이다.\n잔차분석에서는 척도(scale)에 영향이 없는 표준화 잔차를 이용하는 것이 좋다. 그 값이 클수로 이상치일 가능성이 크다. 보통 내표준화 잔차의 절대값이 2보다 크면 이상치일 가능성이 크다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#관측값의-영향-계수-추정",
    "href": "qmd/residual.html#관측값의-영향-계수-추정",
    "title": "5  관측값에 대한 진단",
    "section": "5.5 관측값의 영향: 계수 추정",
    "text": "5.5 관측값의 영향: 계수 추정\n회귀분석에서 하나의 관측치가 회귀계수의 추정에 영향을 미치는 정도를 알아볼 때 유용한 방법은 그 관측치를 제외했을 때의 최소제곱추정량과 포함했을 때의 추정량을 비교하는 것이다.\n\\(i\\)번째 관측치에 대한 반응값과 설명변수들이 다음과 같은 때\n\\[ y_i, \\quad \\pmb x_{i}^t=(1, x_{i1},x_{i2},\\dots, x_{i,p-1}) \\]\n\\(i\\)번째 관측치를 제외한 자료에서 반응변수와 설명변수의 벡터식을 다음과 같이 표시한다.\n\\[ \\pmb y_{-i}, \\quad \\pmb X_{-i} \\]\n\\(i\\)번째 관측치를 제외했을 때 회귀계수의 최소제곱추정량을 \\(\\hat{ \\pmb \\beta}_{-i}\\)라 하면 모든 관측치를 이용한 최소제곱추정량을 \\(\\hat{ \\pmb \\beta}\\)와의 관계는 다음과 같이 나타낼 수 있다. 아래 식 세번째 중의 결과는 우드베리 공식 식 A.3 을 이용하였다.\n\\[\n\\begin{aligned}\n\\hat{ \\pmb \\beta}_{-i} & =  (\\pmb X_{-i}^t \\pmb X_{-i})^{-1} \\pmb X_{-i}^t \\pmb y_{-i}\\\\\n& = (\\pmb X^t \\pmb X - \\pmb x_{i} \\pmb x_{i}^t)^{-1} (\\pmb X^t \\pmb y -  \\pmb x_{i} y_{i}) \\\\\n& = \\left [ (\\pmb X^t \\pmb X)^{-1} - \\frac { (\\pmb X^t \\pmb X)^{-1}  \\pmb x_{i} \\pmb x_{i}^t\n  (\\pmb X^t \\pmb X)^{-1}  }{ 1- \\pmb x_{i}^t (\\pmb X^t \\pmb X)^{-1}  \\pmb x_{i} } \\right ]\n(\\pmb X^t \\pmb y -  \\pmb x_{i} y_{i})  \\\\\n& = \\hat{ \\pmb \\beta} + \\frac{1}{1-h_{ii}} (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i \\left [ \\pmb x_i^t \\hat{ \\pmb \\beta} - (1-h_{ii}) y_i - h_{ii} y_i \\right ] \\\\\n& =  \\hat{ \\pmb \\beta} - \\frac{1}{1-h_{ii}} (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i ( y_i - \\pmb x_i^t \\hat{ \\pmb \\beta}) \\\\\n& =  \\hat{ \\pmb \\beta} - \\frac{r_i}{1-h_{ii}} (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i\n\\end{aligned}\n\\tag{5.6}\\]\n또한 \\(i\\)번째 관측치를 제외했을 때 오차항 분산의 추정량을 \\(s^2_{-i}\\)로 나타낸다.\n\\[\ns^2_{-i} = \\frac{1}{n-p-1} \\sum_{j \\ne i} (y_j - {\\pmb x}_j^t \\hat{ \\pmb \\beta}_{-i} )^2\n\\tag{5.7}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#외-표준화-잔차와-press-잔차",
    "href": "qmd/residual.html#외-표준화-잔차와-press-잔차",
    "title": "5  관측값에 대한 진단",
    "section": "5.6 외 표준화 잔차와 PRESS 잔차",
    "text": "5.6 외 표준화 잔차와 PRESS 잔차\n잔차를 표준화 할 떄 \\(i\\)번째 관측치를 제외했을 때 분산의 추정량을 \\(s^2_{-i}\\)을 이용하는 것이 합리적이다. 이는 반응값이 이상점인 경우 분산의 추정량이 커지게 된다. 식 식 5.5 에서 정의된 내 표준화 잔차에서는 이상점이 분산의 추정량에 영향을 주어 잔차의 크기가 작아지게 된다. 따라서 내 표본화 잔차는 이상점을 구별할 수 있는 능력이 떨어진다. 이러한 점을 보완하기 위하여 이상점의 영향을 약화시킬 수 있도록 \\(s^2_{-i}\\)를 이용하여 표준화 한 양이 아래와 같이 정의된 표준화 잔차이다.\n\\[\nr^*_i = \\frac{r_i}{s_{-i} \\sqrt{1-h_{ii}}}\n\\tag{5.8}\\]\n식 식 5.8 에서 정의된 차를 표준화 잔차(studentized residual) 또는 외 표준화 잔차(externally studentized residual)라고 부른다.\n외 표준화 잔차는 \\(i\\)번째 관측치가 회귀식 적합에 미치는 영향을 내 표분화 잔차보다 더 민감하게 탐색할 수 있다. 보통 외 표준화 잔차의 절대값이 2보다 크면 이상치일 가능성이 크다.\nPRESS 잔차 \\(r_{i,-i}\\)는 \\(i\\) 번쨰 관측값을 빼고 적합한 회귀식으로 부터 얻은 \\(E(y| \\pmb x_i)\\)의 추정치 \\(\\hat y_{i,-i}\\)를 이용하여 만든 잔차이다. PRESS 잔차는 다음과 같이 정의된다.\n\\[\nr_{i,-i}  =   y_i - \\hat y_{i,-i} = y_i - \\pmb x^t_i \\hat{ \\pmb \\beta}_{-i}\n\\tag{5.9}\\]\n실제 PRESS 잔차를 구할 경우 관측값을 제외하지 않고도 원래의 회귀식을 이용하여 아래와 같이 쉽게 구할 수 있다. 그 값이 클수로 이상치 또는 영향점일 가능성이 크다.\n\\[\n\\begin{aligned}\nr_{i,-i} & =   y_i - \\hat y_{i,-i} \\\\\n& =y_i - \\pmb x^t_i \\hat{ \\pmb \\beta}_{-i}  \\\\\n& = y_i - \\pmb x^t_i  \\left [ \\hat{ \\pmb \\beta} - \\frac{1}{1-h_{ii}} (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i r_i \\right ] \\\\\n&= (y_i - \\pmb x^t_i \\hat{ \\pmb \\beta}) +   r_i  \\frac{\\pmb x^t_i (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i}{1-h_{ii}} \\\\\n&= \\frac{r_i}{1-h_{ii}}\n\\end{aligned}\n\\tag{5.10}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#관측값의-영향-분산-추정",
    "href": "qmd/residual.html#관측값의-영향-분산-추정",
    "title": "5  관측값에 대한 진단",
    "section": "5.7 관측값의 영향: 분산 추정",
    "text": "5.7 관측값의 영향: 분산 추정\n참고로 식 식 5.7 에서 정의된 \\(s^2_{-i}\\) 과 \\(s^2 = SSE/(n-p)\\)의 관계를 살펴보자. 먼저 \\(SSE\\)의 정의와 식 식 5.6 과 식 5.10 를 이용하여 다음과 같은 분해가 가능하다.\n\\[\n\\pmb y - {\\pmb X} \\hat{ \\pmb \\beta}_{-i}  = ( \\pmb y - {\\pmb X} \\hat{ \\pmb \\beta}) +\n\\frac{r_i}{1-h_{ii}} \\pmb X (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i\n\\]\n따라서\n\\[\n\\begin{aligned}\n& \\sum_{j \\ne i} (y_j - {\\pmb x}_j^t \\hat{ \\pmb \\beta}_{-i} )^2   + (y_i - \\hat {y}_{i,-i} )^2 \\\\\n& = \\sum_{j \\ne i} (y_j - {\\pmb x}_j^t \\hat{ \\pmb \\beta}_{-i} )^2   + (y_i - {\\pmb x}_i^t \\hat{ \\pmb \\beta}_{-i} )^2  \\\\\n  & = ( \\pmb y - \\pmb X \\hat {\\pmb \\beta}_{-i})^t ( \\pmb y - \\pmb X \\hat {\\pmb \\beta}_{-i}) \\\\\n  & = ( \\pmb y - \\pmb X \\hat {\\pmb \\beta})^t ( \\pmb y - \\pmb X \\hat {\\pmb \\beta})\n   -2 \\frac{r_i}{1-h_{ii}} ( \\pmb y - \\pmb X \\hat {\\pmb \\beta})^t  \\pmb X (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i  \\\\\n   & \\quad + \\frac{r^2_i}{(1-h_{ii})^2} \\pmb x_i^t  (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb X (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i  \\\\\n   & = ( \\pmb y - \\pmb X \\hat {\\pmb \\beta})^t ( \\pmb y - \\pmb X \\hat {\\pmb \\beta})\n   -2 \\frac{r_i}{1-h_{ii}}  \\pmb y^t(\\pmb I - \\pmb H)  \\pmb X (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i  \\\\\n   & \\quad  + \\frac{r^2_i}{(1-h_{ii})^2} \\pmb x_i^t (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i  \\\\\n    & = SSE + 0 + \\frac{r^2_i}{(1-h_{ii})^2} h_{ii}  \\\\\n    & = SSE  + \\frac{r^2_i h_{ii}}{(1-h_{ii})^2}   \n\\end{aligned}\n\\tag{5.11}\\]\n이제 위의 식의 결과와 식 식 5.10 를 이용하면 다음과 같은 결과를 얻는다.\n\\[\n\\begin{aligned}\n\\sum_{j \\ne i} (y_j - {\\pmb x}_j^t \\hat{ \\pmb \\beta}_{-i} )^2   \n& =  SSE  + \\frac{r^2_i h_{ii}}{(1-h_{ii})^2}   -  (y_i - \\hat {y}_{i,-i} )^2 \\\\\n& = SSE  + \\frac{r^2_i h_{ii}}{(1-h_{ii})^2}   -  (y_i - \\hat {y}_{i,-i} )^2 \\\\\n  & = SSE  + \\frac{r^2_i h_{ii}}{(1-h_{ii})^2}   -   \\frac{r^2_i}{(1-h_{ii})^2} \\\\\n  & = SSE  - \\frac{r^2_i }{1-h_{ii}}    \n\\end{aligned}\n\\tag{5.12}\\]\n따라서 다음 식을 이용하면 \\(s^2_{-i}\\)은 모든 관측값을 이용한 \\(s^2\\)으로부터 쉽게 유도할 수 있다.\n\\[\n(n-p-1) s^2_{-i} = (n-p)s^2 + - \\frac{r^2_i }{1-h_{ii}}    \n\\tag{5.13}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#영향력의-측도",
    "href": "qmd/residual.html#영향력의-측도",
    "title": "5  관측값에 대한 진단",
    "section": "5.8 영향력의 측도",
    "text": "5.8 영향력의 측도\n하나의 관측값이 있는 경우 회귀계수 추정치와 없는 경우의 추정치의 차이가 크면 그 관측값이 큰 영향력을 가진다. 이러한 영향력을 측정할 수 있는 측조에 대하여 알아보자.\n쿡의 거리(COOK’s distance) \\(C_i\\)는 \\(i\\)번째 관측치가 회귀식 적합의 계수에 미치는 영향을 나타내는 양으로서 다음과 같이 정의된다.\n\\[\nC_i = \\frac{  (\\hat{ \\pmb \\beta} -\\hat{ \\pmb \\beta}_{-i})^t [ \\widehat {Cov}(\\hat {\\pmb \\beta}]^{-1}\n(\\hat{ \\pmb \\beta} -\\hat{ \\pmb \\beta}_{-i}) } {p} = \\frac{  (\\hat{ \\pmb \\beta} -\\hat{ \\pmb \\beta}_{-i})^t (\\pmb X^t \\pmb X) (\\hat{ \\pmb \\beta} -\\hat{ \\pmb \\beta}_{-i}) } {p s^2}\n\\tag{5.14}\\]\n여기서 \\(\\hat{ \\pmb \\beta}_{-i}\\)는 \\(i\\)번째 관측치를 제외하고 적합한 회귀식에 의한 회귀계수이며 \\(p\\)는 설명변수의 개수이다. 그 값이 클수로 영향점일 가능성이 크다.\n쿡의 거리 \\(C_i\\)과 내 표준화 잔차와의 관계는 다음과 같다.\n\\[ C_i = \\frac{ (r^s_i)^2}{p} \\left ( \\frac{h_{ii} }{1-h_{ii}} \\right ) \\]\nDFFITS는 \\(n\\)개의 모든 자료를 이용했을 때의 \\(i\\) 번째 관측값의 평균 \\(E(y|\\pmb x_i)\\)의 추정치 \\(\\hat y_i\\)와 \\(i\\) 번쨰 관측값을 빼고 적합한 회귀식에 의한 추정치 \\(\\hat y_{i,-i}\\)의 표준화된 차이을 말한다.\n즉, \\(\\hat y_{i,-i}\\)를 \\(i\\)번째 관측치를 제외하고 적합한 회귀식에 의한 예측치라고 한다면 두 예측치의 차이 \\(\\hat y_i - \\hat y_{i,-i}\\)를 표준화시키면 다음과 같다.\n\\[\nDFFITS_i = \\frac{\\hat y_i - \\hat y_{i,-i}}{s_{-i}\\sqrt{h_{ii}}}  \n\\tag{5.15}\\]\nDFFITS 는 그 값이 클수로 영향점일 가능성이 크다.\n여기서 식 식 5.6 를 이용하면 다음 식를 얻고\n\\[ {\\pmb x}_i^t \\hat{ \\pmb \\beta}_{-i} =\\pmb x_i^t \\hat{ \\pmb \\beta} -\\frac{r_i h_{ii}}{1-h_{ii}}\\]\nDFFITS과 잔차와의 관계를 알 수 있다.\n\\[\n\\begin{aligned}\nDFFITS_i & = \\frac{\\hat y_i - \\hat y_{i,-i}}{s_{-i}\\sqrt{h_{ii}}}  \\\\\n& =  \\frac{ [h_{ii}/(1-h_{ii})]r_i } {s_{-i}\\sqrt{h_{ii}}} \\\\\n& =  \\frac{ r_i } {s_{-i}\\sqrt{1-h_{ii}}} [h_{ii}/(1-h_{ii})]^{1/2} \\\\\n&= r^*_i \\left [\\frac{h_{ii}}{1-h_{ii}} \\right ]^{1/2}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/modelselection.html",
    "href": "qmd/modelselection.html",
    "title": "6  모형의 선택",
    "section": "",
    "text": "6.1 서론\n모형의 선택은 자료의 분석에서 고려하는 다수의 모형들(a family of models) 중에서 가장 적합한 모형(best model)을 찾는 것이다. 여기서 가장 적합하다는 의미는 다양한 기준이 있지만 일반적으로 선택된 모형의 예측력 또는 설명력이 다른 모든 모형들보다 더 좋다는 의미이다.\n분석에서 고려하는 모형들의 집합을 모형 공간(model space)이라고 하며 이 공간에서 가장 적합한 모형을 찾는 것이 모형 선택(model selection)이다. 이 장에서 모형의 예측력 또는 설명력 을 정의하고 비교하는 방법에 대하여 배울 것이다.\n주어진 모형 공간에서 가장 좋은 모형을 선택했을 때 다음과 같은 질문이 가능하다.\n선택된 모형보다 더 좋은 모형이 있지 않을까? 더 좋은 모형이 주어진 모형공간에 포함되지 않을 수도 있다.\n주어진 자료에서 반응변수와 설명변수의 관계를 더욱 잘 설명할 수 있는모형을 계속 찾는다면 결국에는 예측력을 높이기 위하여 더 많은 설명변수를 포함하는 모형을 찾게 될 것이다. 궁국적으로는 반응변수의 관측값 \\(y\\) 와 예측값 \\(\\hat y\\)의 차이가 가장 작은 모형, 즉 설명력이 가장 좋은 모형을 선택하려는 노력을 계속한다면 과적합(overfitting) 이 발생할 수 있다.\n과적합은 모형의 복잡도가 증가함에 따라 주어진 자료에 대한 모형의 예측력은 증가하지만 모형의 일반적인 예측의 효율은 오히려 감소하는 현상을 말한다.\n이러한 과적합을 피하려면 모형의 복잡도와 예측력 사이의 적절한 균형을 찾아야 한다.\n현실 세계의 상황에서는 진정한 모형이 알려지지 않거나 자료의 정확한 분포와 관계를 기술할 수 있는 모형을 파악하는 것은 매우 어렵다. 하지만 실제로 데이터를 생성하는 과정이나 현상을 정확하게 기술하는 가상의 모형이 존재한다고 가정할 수는 있다. 이렇게 자료의 분포와 관계를 정확하게 기술하는 가상의 모형을 참모형(true model)이라고 한다. 다시 강조하지만 가상의 모형이라고 말한 의미는 자료의 생성 과정을 정확하게 기술할 수 있는 모형을 구체화하여 표현하는 것이 매우 힘들기 떄문이다.\n모형의 선택하는 또 다른 기준은 가상의 참모형에 제일 가까운 모형 을 선택하는 것이다. 우리가 생각할 수 있는 대부분의 모형 공간은 참모형을 포함하지 않는 다고 가정할 수 있다. 이러한 경우 고려하는 다수의 모형들 중에서 참모형에 가장 가까운 모형을 최적의 모형이라고 할 수 있다.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>모형의 선택</span>"
    ]
  },
  {
    "objectID": "qmd/modelselection.html#모형선택의-측도",
    "href": "qmd/modelselection.html#모형선택의-측도",
    "title": "6  모형의 선택",
    "section": "6.2 모형선택의 측도",
    "text": "6.2 모형선택의 측도\n회귀분석모형의 구축을 시작할 때는 될 수 있는 한 많은 독립변수들을 고려하고 그 중에 모형에 적합한 변수들과 그렇지 않은 변수들을 구별하여 최선의 모형을 찾으려고 많은 노력을 기울인다.\n이 절에서는 설명변수의 조합으로 만들 수 있는 다양한 모형들을 비교할 수 있는 기준과 통계적 방법에 대하여 알아보고자 한다.\n일반적인 회귀분석모형에서 다음과 같은 선형 회귀모형을 가정한다.\n\\[ {\\pmb y} = {\\pmb X_p} {\\pmb \\beta}_p + \\pmb e \\]\n오차항이 다음과 같이 서로 독립이고 등분산성을 만족한다면\n\\[ V(\\pmb e) =\\sigma^2 \\pmb I_n \\]\n최소제곱법에 의한 회귀계수 추정량 \\(\\hat {\\pmb \\beta}_p\\) 다음과 같고\n\\[  \\hat {\\pmb \\beta}_p = ({\\pmb X}_p^t{\\pmb X}_p)^{-1}{\\pmb X}_p^t{\\pmb y} \\]\n\n\n\n\n\n\n중요\n\n\n\n이 절의 선형 회귀모형에서는 독립변수의 개수가 \\(p\\)개인 것을 강조하기 위하여 \\({\\pmb X}_p\\) 와 \\({\\pmb \\beta}_p\\) 를 사용하였다.\n\n\n모든 가능한 회귀모형의 개수는 \\(2^p-1\\)개이므로 \\(p\\)가 크지 않다면 가능한 모든 회귀모형을 비교하여 하나의 모형을 선택하는 것이 좋을 것이다. 여러가지 모형들을 비교할 수 있는 모형 선택의 측도들을 알아보자.\n\n6.2.1 결정계수\n총제곱합에서 회귀모형으로 설명할 수 있는 변동 모형 제곱합이 차지하는 부분의 비율, 즉 모형제곱합 \\(SSR\\)을 총제곱합 \\(SST\\)으로 나눈 비율을 결정계수(coefficient of determination)라 하며 \\(R^2\\)으로 표현한다.\n\\[\nR^2 = \\frac{SSR}{SST} =  1 -\\frac{SSE}{SST}  =1- \\frac{\\sum^n_{i=1}(y_i-\\hat y_i)^2}{ \\sum^n_{i=1}(y_i - \\bar y)^2}\n\\tag{6.1}\\]\n결정계수 \\(R^2\\)는 언제나 0 이상 1 이하의 값을 갖는다. 회귀모형이 데이터에 아주 잘 적합되면 결정계수의 값은 1 에 가깝게 된다.\n주의할 점은 회귀식에 독립변수를 추가하면 결정계수는 언제나 증가한다. 즉 반응변수와 관련이 없는 변수도 회귀식에 추가하면 결정계수의 값이 증가하기 때문에 결정계수로 모형을 선택하면 언제나 모든 독립변수가 모형에 들어간 가장 큰 모델이 선택된다.\n\n\n6.2.2 결정계수의 수정\n수정 결정계수 \\(\\tilde R^2\\)는 독립변수의 개수가 증가함에 따라 증가하는 결정계수 \\(R^2\\)를 보정한 모형 선택의 척도이다.\n\\[\n\\begin{aligned}\n\\tilde R^2 & = 1-  \\frac{SSE_p/(n-p)}{SST/(n-1)} \\\\ \\notag\n  & = 1 - \\frac{s_p^2}{SST/(n-1)}  \n\\end{aligned}\n\\tag{6.2}\\]\n여기서 \\(p\\)는 회귀모형에 포함된 독립변수의 개수이다.\n\n\n6.2.3 Mallow’s \\(C_p\\)\n모형의 적합도를 측정하기 위한 여러 가지 통계량중 가장 중요하고 자주 쓰이는 통계량이 평균제곱오차(mean squared error; MSE)이다 이 책에서는 평균제곱오차를 \\(\\Delta_p^2\\) 으로 표시할 것이다.\n반응변수 \\(y_i\\)의 평균을 \\(\\mu_i=E(y_i)\\)로 하고 독립 변수의 개수가 \\(p\\)개인 선형회귀 모형에서 최소제곱법에 의한 예측값을 \\(\\hat y_{ip} = {\\pmb x}_{ip}^t \\hat {\\pmb \\beta}_p\\)라고 하면 MSE는 다음과 같이 주어진다.\n\\[\n\\begin{aligned}\nE [ (\\hat y_{ip} -\\mu_i)^2 ] &= E[( {\\pmb x}_{ip}^t \\hat {\\pmb \\beta}_p-\\mu_i)^2 ] \\\\\n  &=  E[( {\\pmb x}_{ip}^t \\hat {\\pmb \\beta}_p -E({\\pmb x}_{ip}^t \\hat {\\pmb \\beta}_p) + E({\\pmb x}_{ip}^t \\hat {\\pmb \\beta}_p)-\\mu_i)^2 ] \\\\\n  &= Var({\\pmb x}_{ip}^t \\hat {\\pmb \\beta}_p)+ [E({\\pmb x}_{ip}^t \\hat {\\pmb \\beta}_p)-\\mu_i]^2 \\\\\n  &= \\sigma^2 {\\pmb x}_{ip}^t({\\pmb X}_p^t {\\pmb X}_p^t)^{-1} {\\pmb x}_{ip} + ( \\eta_{ip}-\\mu_i)^2\n\\end{aligned}\n\\]\n여기서 \\(\\eta_{ip} = E({\\pmb x}_{ip}^t \\hat {\\pmb \\beta}_p)\\)이다. 여기서 유의할 점은 반응변수 \\(y_i\\)의 평균을 \\(\\mu_i\\)와 \\(\\eta_{ip}\\) 는 다를 수도 있으며 그 차이를 모형에 의한 편이(bias)라고 한다.\n\\[ E({\\pmb x}_{ip}^t \\hat {\\pmb \\beta}_p ) -\\mu_i =  \\eta_{ip} -\\mu_i \\]\n이제 평균제곱오차를 구하기 위하여 각각의 관측값 \\(y_1, y_2,\\dots,y_n\\)에 대한 제곱합을 구해보자\n\\[\n\\begin{aligned}\n\\Delta_p^2 &= \\sum_i E(\\hat y_{ip} -\\mu_i)^2   \\\\\n  &= \\sigma^2 \\sum_i {\\pmb x}_{ip}^t({\\pmb X}_p^t {\\pmb X}_p^t)^{-1} {\\pmb x}_{ip} + \\sum_i (\\eta_{ip}-\\mu_i)^2 \\\\ \\notag\n  & = \\sigma^2 tr( {\\pmb X}_p ({\\pmb X}_p^t {\\pmb X}_p^t)^{-1} {\\pmb X}_p^t ) + \\sum_i (\\eta_{ip}-\\mu_i)^2 \\\\ \\notag\n  &= p \\sigma^2 + SSB_p\n\\end{aligned}\n\\tag{6.3}\\]\n여기서 \\(SSB_p=\\sum_i (\\eta_{pi}-\\mu_i)^2\\)이며 예측값의 편이들의 제곱합이다.평균제곱오차 \\(\\Delta_p^2\\) 은 모형에서 추정된 값이 실제 평균과 가까운 정도를 나타내는 측도이지만 실제로 자료를 이용하여 구할 수는 없는 양이다.\n여기서 중요한 점은 평균제곱오차 \\(\\Delta_p^2\\) 는 분산과 편차 제곱들의 합이다.\n실제 평균제곱오차 \\(\\Delta_p^2\\)는 계산할 수 있는 값이 아니므로 이를 적절히 추정할 수 있는 통계량으로 잔차제곱합 (SSE)를 생각해 보자. 독립 변수의 개수가 \\(p\\)개인 선형회귀 모형에 의한 잔차제곱합을 고려하고 그 기대값을 구해보면\n\\[\n\\begin{aligned}\nE(SSE_p) &= E[ {\\pmb y}^t ({\\pmb I} - {\\pmb X}_p ({\\pmb X}_p^t {\\pmb X}_p)^{-1}{\\pmb X}_p^t) {\\pmb y} ] \\\\ \\notag\n       &= tr( \\sigma^2 ({\\pmb I} - {\\pmb X}_p ({\\pmb X}_p^t {\\pmb X}_p)^{-1}{\\pmb X}_p^t)) +\n         E({\\pmb y})^t  ({\\pmb I} - {\\pmb X}_p ({\\pmb X}_p^t {\\pmb X}_p)^{-1}{\\pmb X}_p^t) E({\\pmb y}) \\\\ \\notag\n      &= \\sigma^2 tr(  ({\\pmb I} - {\\pmb X}_p ({\\pmb X}_p^t {\\pmb X}_p)^{-1}{\\pmb X}_p^t)) +\n             E({\\pmb y})^t ({\\pmb I} -\\pmb H_p)({\\pmb I} -\\pmb H_p ) E({\\pmb y}) \\\\ \\notag\n         &= \\sigma^2 (n-p) + [E({\\pmb y}) - E(\\hat {\\pmb y}_p)]^t [E({\\pmb y}) - E(\\hat {\\pmb y}_p)] \\\\\n       &= \\sigma^2(n-p) + SSB_p\n\\end{aligned}\n\\tag{6.4}\\]\n위의 결과는 \\({\\pmb I} - {\\pmb X}_p ({\\pmb X}_p^t {\\pmb X}_p)^{-1}{\\pmb X}_p^t = {\\pmb I} -{\\pmb H}_p\\)가 멱등행렬인 사실과 아래의 식을 이용하였다.\n\\[ \\eta_p = E(\\hat {\\pmb y}_p) = E({\\pmb X}_p^t \\hat {\\pmb \\beta}_p) =  {\\pmb X}_p ({\\pmb X}_p^t {\\pmb X}_p)^{-1}{\\pmb X}_p^t E( {\\pmb y})  = {\\pmb H}_p E( {\\pmb y})\\] 만약 \\(\\sigma^2\\)의 불편추정량을 \\(\\hat \\sigma^2\\)라 하면 식 식 6.3 와 식 6.4 를 이용하여 다음과 같은 결과를 얻는다.\n\\[\n\\begin{aligned}\nE[SSE_p -(n-2p)\\hat \\sigma^2] & = \\sigma^2(n-p) + SSB_p -(n-2p) E(\\hat \\sigma^2) \\\\ \\notag  \n       & = p\\sigma^2 +SSB_p  \\\\ \\notag\n       & = \\Delta_p^2\n\\end{aligned}\n\\tag{6.5}\\]\n따라서 평균제곱오차 \\(\\Delta_p^2\\)의 추정량으로 \\(SSE_p -(n-2p)\\hat \\sigma^2\\) 을 사용할 수 있다. Mallow(1973)가 제안한 Mallow’s \\(C_p\\)는 평균제곱오차를 분산의 추정량으로 나눈값 \\(\\Delta_p^2/\\sigma^2\\)이며 이를 최소화는 모형을 선택할 것을 Mallow가 제안하였다.\n\\[\nC_p = \\frac{SSE_p}{\\hat \\sigma^2 } + (2p-n)\n\\tag{6.6}\\]\n식 식 6.6 에서 주어진 \\(C_p\\) 에서 \\(\\hat \\sigma^2\\)은 고려하는 모든 변수를 포함하는 모형(full model)에서 구한 오차항 분산의 추정량이다. Mallow(1973)는 \\(\\Delta_p^2\\)이 \\(SSB_p\\)가 0일 때, 즉 \\(E(\\hat {\\pmb y}_p)=E({\\pmb y})\\)일 때 최소값 \\(p \\sigma^2\\)를 같는다는 사실에 의거하여 \\(C_p\\)와 \\(p\\)에 대한 그림을 그리고 \\(C_p\\)의 값이 해당하는 \\(p\\)값에 가깝거나 작은 모형을 선택하는 탐색적 방법을 제안하였다.\n여기서 주목할 점은 Mallow’s \\(C_p\\) 에서 설명변수의 개수 \\(p\\)의 개수를 크게 하면 \\(SSE_p\\) 는 작아지지만 항 \\(2p-n\\)은 증가하게 된다. 따라서 \\(SSE_p\\)에 더해주는 항 \\(2p-n\\) 은 설명변수의 증가에 따른 벌칙항(penalty term)으로 볼 수 있다.\n\n\n6.2.4 PRESS\nPRESS는 prediction error sum of square의 약자로 Cross-validation에 의거한 모형선택을 위한 척도이다. 전차분석에서 보았던 처럼 \\(i\\)번째 관측치 \\((y_i,{\\pmb x}_i)\\)를 제외한 반응변수 벡터, 계획행렬, 회귀계수를 각각 \\({\\pmb y}_{-i}\\), \\({\\pmb X}_{-i}\\), \\(\\hat {\\pmb \\beta}_{-i}\\)와 같이 표시하고 그에 해당하는 예측값을 \\(\\hat y_{ip,-i}\\)라 하면 RESS는 다음과 같이 정의된다.\n\\[\nPRESS_p  = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat y_{ip,-i})^2\n\\tag{6.7}\\]\n여기서 잔차분석에서 유도한 것처럼\n\\[ y_i - \\hat y_{ip,-i} = \\frac{r_i}{1-h_{ii}} \\]\n를 이용하면 PRESS를 다음과 같이 표현할 수 있다.\n\\[\nPRESS_p = \\frac{1}{n} \\sum_{i=1}^n  \\left [ \\frac{y_i - \\hat y_{ip}}{1-h_{ii}} \\right ]^2\n\\approx \\frac{ SSE_p }{n(1-p/n)^2}\n\\]\n위의 식 마지막 근사는 모든 \\(h_{ii}\\)가 그 평균값 \\(p/n\\)에 가깝다는 가정 하에 세워진 식이다.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>모형의 선택</span>"
    ]
  },
  {
    "objectID": "qmd/modelselection.html#aic-와-bic",
    "href": "qmd/modelselection.html#aic-와-bic",
    "title": "6  모형의 선택",
    "section": "6.3 AIC 와 BIC",
    "text": "6.3 AIC 와 BIC\n통계모형을 선택하는 척도로서 가능도함수이론에 근거한 AIC(Akaike information criteria)와 베이지안 검정이론에 기초한 BIC(bayesian or schwartz information criteria)가 있다.\nAIC와 BIC는 회귀분석뿐 아니라 일반적인 통계 모형에서 자주 사용하는 모형의 선택에 대한 척도이다. AIC와 BIC의 정의는 다음과 같다.\n\\[\nAIC =  -2 \\log \\ell(\\hat {\\pmb \\theta}) + 2k\n\\tag{6.8}\\]\n\\[\nBIC =  -2 \\log \\ell(\\hat {\\pmb \\theta}) + (\\log n) k\n\\tag{6.9}\\]\n여기서 \\(k\\)는 모형에 포함된 모수의 총 개수 이다. \\(\\ell(\\hat {\\pmb \\theta})\\)은 최대가능도추정량 \\(\\hat {\\pmb \\theta}\\)에서 계산된 로그 가능도함수이다.\n선형모형에 대한 가능도 추정에서 식 식 1.19 에서 보았듯이 정규분포 가정 하에서 회귀모형에 대한 로그 가능도함수는 다음과 같으므로\n\\[\nl_n(\\hat { \\pmb \\theta} ) = l_n(\\hat { \\pmb \\beta} ,\\hat \\sigma^2 )\n= -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}  - \\frac{n}{2} \\log \\frac{SSE_p}{n}\n\\]\n따라서 선형회귀 모형에서의 AIC와 BIC는 다음과 같이 주어진다.\n\\[\n\\begin{aligned}\nAIC &= n\\log(2\\pi) + n + n \\log  \\frac{SSE_p}{n} + 2(p+1) \\\\\nBIC &= n\\log(2\\pi) + n + n \\log  \\frac{SSE_p}{n} + (\\log n) (p+1)\n\\end{aligned}\n\\tag{6.10}\\]\n여기서 \\(p\\)는 회귀모형에 포함된 독립변수의 개수이며 오차항의 분산까지 포함하여 모수의 총 개수는 \\(p+1\\) 이다.\n이제 잔차제곱합 \\(SSE_p\\) 가 작아지면 AIC 와 BIC의 \\(SSE_p\\) 부분이 작아지지만 각 측도의 벌칙항은 증가하게 된다. 여러 개의 모형을 비교하 때 AIC, BIC 의 값이 작은 모형이 좋은 모형이라고 할 수 있다. 또한 주목할 점은 AIC, BIC 의 벌칙항이 다르며 특히 BIC 의 벌칙항에 표본의 개수 \\(n\\) 이 로그스케일로 포함되어 있다.\nAIC 와 BIC 에 대한 이론적인 설명은 부록 G 에 제시되어 있으니 참고하자.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>모형의 선택</span>"
    ]
  },
  {
    "objectID": "qmd/modelselection.html#변수-선택법",
    "href": "qmd/modelselection.html#변수-선택법",
    "title": "6  모형의 선택",
    "section": "6.4 변수 선택법",
    "text": "6.4 변수 선택법\n주어진 설명변수들 중에 반응변수에 유의한 영향을 미치는 변수들을 단계적으로 선택하는 방법(variable selection procedure)은 다음과 같이 세 종류의 방법이 있다.\n\nForward selection: Forward selection 방법은 회귀모형에 독립변수를 하나 씩 추가하는 방법이다. 첫 번째 추가하는 변수는 설명변수가 한 개인 모형 중에 결정계수 \\(R^2\\)(또는 다른 측도)이 가장 큰 변수를 선택하며 두번째 부터는 추가되었을 때 \\(R^2\\)의 증가가 가장 큰 값을 선택하게 된다. 변수의 추가가 멈추는 조건은 추가된 변수가 주어진 신뢰수준에서 유의하지 않을 때이다.\nBackward elimination: Backward elimination 방법은 모든 설명변수를 포함한 가장 큰 회귀모형(full model)에서 설명변수를 하나 씩 제거하는 방법이다. 제거하는 변수의 선택은 변수가 제거되었을 때 \\(R^2\\)의 감소가 가장 작은 값을 선택하게 된다.\nStepwise: Stepwise는 Forward selection과 Backward elimination을 조합하여 변수의 추가와 제거가 모두 가능한 방법이다.\n\n변수선택법은 이 방법이 제안되었을 당시 매우 유용한 방법으로 여겨졌다. 그러나 변수선택법의 무리한 남용 등 여러 가지 단점들로 인하여 조심해서 사용해야 한다는 것이 현재의 공통된 의견이다. 변수선택법의 이용과 그 유의사항은 다음과 같이 요약할 수 있다.\n\n미숙한 이용자에 의해 남용될 수 있다.\n다중공선성이 존재할 때 불안정하다.\nStepwise는 주어진 추가와 제거 시 사용되는 유의수준에 따라 최적의 모형이 다를 수 있다.\n모든 가능한 회귀 모형(All possible regressions)을 사용하는 것이 대안이 될 수 있다.\n과적합(overfitting)의 위험성이 크다.\n변수의 추가나 제거에 통계적 검정법을 쓰는데 여러 가지 위험성이 존재한다 (예로 다중비교 문제)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>모형의 선택</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "강근석, and 유형조. 2016. R을 활용한 선형회귀분석.\n1st ed. 교우사. https://github.com/regbook/regbook.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html",
    "href": "qmd/math_mat_basic.html",
    "title": "부록 A — 행렬의 기초",
    "section": "",
    "text": "A.1 벡터와 행렬\n다음 \\(p\\)-차원 벡터(vector) 또는 열벡터(column vector) \\(\\pmb a\\) 는 \\(p\\)개의 원소 \\(a_1, a_2, \\dots, a_p\\) 를 하나의 열(column)에 배치한 형태를 가진 개체이다.\n\\[\n\\pmb a =\n\\begin{bmatrix}\na_1 \\\\\na_2 \\\\\n\\vdots \\\\\na_p\n\\end{bmatrix}\n\\tag{A.1}\\]\n차원이 \\(n \\times p\\) 인 행렬 \\(\\pmb A\\) 는 다음과 같이 \\(n\\)개의 행과 \\(p\\) 개의 열에 원소 \\(a_{ij}\\)를 다음과 같이 배치한 형태를 가진다.\n\\[\n\\pmb A =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{n1} & a_{n2} & \\dots & a_{np}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#두-행렬의-덧셈",
    "href": "qmd/math_mat_basic.html#두-행렬의-덧셈",
    "title": "부록 A — 행렬의 기초",
    "section": "A.2 두 행렬의 덧셈",
    "text": "A.2 두 행렬의 덧셈\n두 행렬 \\(\\pmb A\\) 와 \\(\\pmb B\\) 를 더하는 규칙은 다음과 같다.\n\n두 행렬 \\(\\pmb A\\) 와 \\(\\pmb B\\) 는 행과 열의 갯수가 같아야 한다.\n\\(\\pmb A + \\pmb B = \\pmb C\\) 라고 하면, 덧셈의 결과로 만들어진 행렬 \\(\\pmb C\\)는 두 행렬과 같은 수의 행과 열을 가지면 각 원소는 다음과 같다.\n\n\\[ \\pmb A + \\pmb B = \\pmb C \\quad \\rightarrow \\quad c_{ij} = a_{ij} + b_{ij} \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#스칼라곱",
    "href": "qmd/math_mat_basic.html#스칼라곱",
    "title": "부록 A — 행렬의 기초",
    "section": "A.3 스칼라곱",
    "text": "A.3 스칼라곱\n임의의 실수 \\(\\lambda\\) (스칼라)가 주어졌을 때, \\(\\lambda\\) 와 행렬 \\(\\pmb A\\)의 스칼라곱(scalar product) 는 행렬의 모든 원소에 \\(\\lambda\\) 를 곱해준 행렬로 정의된다.\n예를 들어 \\(\\lambda=2\\), \\(\\pmb A \\in \\RR^{2\\times 3}\\) 인 경우\n\\[\n\\lambda \\pmb A =\n2\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n-1 & 0 & 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 & 4 & 6 \\\\\n-2 & 0 & 4\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#벡터와-행렬의-곱셈",
    "href": "qmd/math_mat_basic.html#벡터와-행렬의-곱셈",
    "title": "부록 A — 행렬의 기초",
    "section": "A.4 벡터와 행렬의 곱셈",
    "text": "A.4 벡터와 행렬의 곱셈\n\\(n \\times p\\) 인 행렬 \\(\\pmb A\\) 와 \\(p\\)-차원 벡터(vector) \\(\\pmb b\\)는 다음과 같이 두 개의 서로 다른 형태로 나타낼 수 있다.\n\nA.4.1 행과 열의 내적\n먼저 행렬과 벡터의 곱셈은 행렬 \\(\\pmb A\\) 의 행벡터와 벡터 \\(\\pmb b\\) 의 내적(inner product)로 나타낼 수 있다.\n\\[\n\\begin{aligned}\n{\\pmb A} {\\pmb b} & =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{n1} & a_{n2} & \\dots & a_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n{\\pmb r}^t_1 \\\\\n{\\pmb r}^t_2 \\\\\n\\vdots \\\\\n{\\pmb r}^t_n\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix}\n\\quad\n\\text{ where }\n{\\pmb r}^t_i =\n\\begin{bmatrix}\na_{i1} & a_{i2} & \\dots & a_{ip}\n\\end{bmatrix}  \\\\\n& =\n\\begin{bmatrix}\n{\\pmb r}^t_1 {\\pmb b} \\\\\n{\\pmb r}^t_2 {\\pmb b} \\\\\n\\vdots \\\\\n{\\pmb r}^t_n {\\pmb b}\n\\end{bmatrix}  \n=\n\\begin{bmatrix}\n\\sum_{j=1}^p a_{1j} b_j \\\\\n\\sum_{j=1}^p a_{2j} b_j \\\\\n\\vdots \\\\\n\\sum_{j=1}^p a_{nj} b_j\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n&lt;\\pmb r_1, \\pmb b&gt;  \\\\\n&lt;\\pmb r_2, \\pmb b&gt; \\\\\n\\vdots \\\\\n&lt;\\pmb r_n, \\pmb b&gt;\n\\end{bmatrix}\n\\end{aligned}\n\\]\n위에서 \\(&lt; \\pmb a, \\pmb b&gt;\\) 는 다음과 같은 두 벡터의 내적(inner product)을 의미한다.\n\\[ &lt; \\pmb a, \\pmb b&gt; = {\\pmb a}^t {\\pmb b} = \\sum_{i=1}^p a_i b_i \\]\n\n\nA.4.2 열벡터의 선형조합\n이제 행렬과 벡터의 곱셈을 행렬을 구성하는 열벡터들의 선형조합(linear combination)으로 나타낼 수 있다.\n\\[\n\\begin{aligned}\n{\\pmb A} {\\pmb b} & =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{n1} & a_{n2} & \\dots & a_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n{\\pmb c}_1 & {\\pmb c}_2 & \\dots & {\\pmb c}_p\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix}\n\\quad\n\\text{ where }\n{\\pmb c}_j =\n\\begin{bmatrix}\na_{1j} \\\\\na_{2j} \\\\\n\\vdots \\\\\na_{nj}\n\\end{bmatrix} \\\\\n& =\nb_1\n\\begin{bmatrix}\na_{11} \\\\\na_{21} \\\\\n\\vdots \\\\\na_{n1}\n\\end{bmatrix}\n+\nb_2\n\\begin{bmatrix}\na_{12} \\\\\na_{22} \\\\\n\\vdots \\\\\na_{n2}\n\\end{bmatrix}\n+ \\cdots +\nb_p\n\\begin{bmatrix}\na_{1p} \\\\\na_{2p} \\\\\n\\vdots \\\\\na_{np}\n\\end{bmatrix}  \\\\\n& =\nb_1 {\\pmb c}_1 + b_2 {\\pmb c}_2 + \\cdots + b_p {\\pmb c}_p \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#행렬의-전치",
    "href": "qmd/math_mat_basic.html#행렬의-전치",
    "title": "부록 A — 행렬의 기초",
    "section": "A.5 행렬의 전치",
    "text": "A.5 행렬의 전치\n\\(\\pmb A^t\\)는 행렬의 전치(transpose)를 나타낸다. 행렬의 전치는 원소의 행과 열을 바꾸어 만든 행렬이다.\n\\[\n{\\pmb A}  =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{n1} & a_{n2} & \\dots & a_{np}\n\\end{bmatrix}\n= \\{ a_{ij} \\}_{ n \\times p}\n\\quad\n\\rightarrow\n\\quad\n{\\pmb A}^t  =\n\\begin{bmatrix}\na_{11} & a_{21} & \\dots & a_{n1} \\\\\na_{12} & a_{22} & \\dots & a_{n2} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{1p} & a_{2p} & \\dots & a_{np}\n\\end{bmatrix}\n= \\{ a_{ji} \\}_{ p \\times n}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#행렬의-곱셈",
    "href": "qmd/math_mat_basic.html#행렬의-곱셈",
    "title": "부록 A — 행렬의 기초",
    "section": "A.6 행렬의 곱셈",
    "text": "A.6 행렬의 곱셈\n먼저 두 행렬 \\(\\pmb A\\) 와 \\(\\pmb B\\) 의 곱셈\n\\[ \\pmb A \\times \\pmb B \\equiv \\pmb A \\pmb B \\]\n을 정의하려면 다음과 같은 조건이 만족되어야 한다.\n\n행렬 \\(\\pmb A\\) 의 열의 갯수와 행렬 \\(\\pmb B\\) 의 행의 갯수가 같아야 한다\n\n따라서 두 행렬의 곱셈은 순서를 바꾸면 정의 자체가 안될 수 있다.\n이제 두 행렬 \\(\\pmb A \\in \\RR^{m \\times n}\\) 와 \\(\\pmb B \\in \\RR^{n \\times k}\\)의 곱셈은 다음과 같이 정의된다.\n\\[ \\pmb A \\pmb B =  \\pmb C\\]\n행렬 \\(\\pmb C\\) 는 \\(m\\) 개의 행과 \\(k\\)개의 열로 구성된 행렬이며(\\(\\pmb C \\in \\RR^{m \\times k}\\)) 각 원소 \\(c_{ij}\\)는 다음과 같이 정의된다.\n\\[  c_{ij} = \\sum_{l=1}^n a_{il} b_{lk}, \\quad i=1,2,\\dots,m; j=1,2,\\dots,k \\]\n먼저 간단한 예제로 다음과 같은 두 개의 행렬의 곱을 생각해 보자.\n\\[\n\\pmb A \\pmb B =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 1 \\\\\n-1 & 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(1)(0) + (2)(-1) & (1)(1) + (2)(2) \\\\\n(3)(0) + (4)(-1) & (3)(1) + (4)(2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-2 & 5 \\\\\n-4 & 11\n\\end{bmatrix}\n\\]\n곱하는 순서를 바꾸어 계산해 보자.\n\\[\n\\pmb B \\pmb A =\n\\begin{bmatrix}\n0 & 1 \\\\\n-1 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(0)(1) + (1)(3) & (0)(2) + (1)(4) \\\\\n(-1)(1) + (2)(3) & (-1)(2) + (2)(4)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix}\n\\]\n위 두 결과를 보면 행렬의 곱셈에서는 교환법칙이 성립하지 않음을 알 수 있다.\n이제 차원이 다른 두 행렬의 곱셈을 살펴보자.\n\\[\n\\pmb A =\n\\begin{bmatrix}\n1 & 2 & 3\\\\\n3 & 2 & 1\n\\end{bmatrix},\n\\quad\n\\pmb B =\n\\begin{bmatrix}\n0 & 2 \\\\\n1 & -1 \\\\\n0 & 1\n\\end{bmatrix}\n\\]\n두 행렬의 곱셈은 다음과 같이 계산할 수 있다.\n\\[\n\\pmb A \\pmb B =\n\\begin{bmatrix}\n1 & 2 & 3\\\\\n3 & 2 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 2 \\\\\n1 & -1 \\\\\n0 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 & 3 \\\\\n2 & 5\n\\end{bmatrix}\n\\]\n두 행렬의 곱하는 순서를 바꾸면 차원이 전혀 다른 행렬이 얻어진다.\n\\[\n\\pmb B \\pmb A =\n\\begin{bmatrix}\n0 & 2 \\\\\n1 & -1 \\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 & 3\\\\\n3 & 2 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n6 & 4 & 2 \\\\\n-2 & 0 & 2 \\\\\n3 & 2 & 1\n\\end{bmatrix}\n\\]\n행렬의 곱셈은 교환법칙이 성립하지 않는다.\n\\[  \\pmb A \\pmb B \\ne  \\pmb B \\pmb A \\tag{A.2}\\]\n\n\n\n\n\n\n주의\n\n\n\n교환법칙이 성립하지 않는다는 의미는 식 A.2 이 언제나 성립한다는 의미는 아니다. 아래와 같이 특별한 경우 교환법칙이 성립하는 경우도 있다.\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 3\n\\end{bmatrix}\n\\]\n\n\n\n행렬의 곱셈은 결합법칙과 배분법칙은 성립한다.\n\n\\[ (\\pmb A \\pmb B) \\pmb C = \\pmb A (\\pmb B \\pmb C) \\]\n\\[ (\\pmb A + \\pmb B) \\pmb C = \\pmb A \\pmb C +  \\pmb B \\pmb C \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#단위벡터와-항등행렬",
    "href": "qmd/math_mat_basic.html#단위벡터와-항등행렬",
    "title": "부록 A — 행렬의 기초",
    "section": "A.7 단위벡터와 항등행렬",
    "text": "A.7 단위벡터와 항등행렬\n\\(i\\)번째 단위벡터 \\(\\pmb e_i\\)를 정의하자. 단위벡터 \\(\\pmb e_i\\)는 \\(n\\)- 차원 벡터로서 \\(i\\)번째 원소만 1이고 나머지는 0인 벡터이다.\n\\[ \\pmb e_i =\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\  \n1 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix}\n\\]\n즉 \\(n\\)-차원 항등행렬 \\(\\pmb I\\)는 n개의 단위벡터들을 모아놓은 것이다. 단위행렬은 대각원소가 1이고 나머지는 0인 정방행렬이다.\n\\[  \\pmb I = [ \\pmb e_1 ~~ \\pmb e_2 ~~ \\dots ~~ \\pmb e_n ] \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#대각합",
    "href": "qmd/math_mat_basic.html#대각합",
    "title": "부록 A — 행렬의 기초",
    "section": "A.8 대각합",
    "text": "A.8 대각합\n\\(\\pmb A = \\{ a_{ij} \\}\\)를 \\(n \\times n\\) 정방행렬(square matrix)인 경우, 행렬의 대각 원소(diagonal element)들의 합(trace)을 \\(tr(\\pmb A)\\)로 표시한다.\n\\[ tr(\\pmb A) = \\sum_{i=1}^n a_{ii} \\]\n두 행렬의 덧셈(뺄셈)에 대한 대각합에 대한 성질들은 다음과 같다.\n\\[ tr( {\\pmb A} \\pm {\\pmb B}) = tr({\\pmb A}) \\pm tr({\\pmb B}) \\]\n\n\n\n\n\n\n주의\n\n\n\n행렬의 곱셈은 일반적으로 교환법칙이 성립하지 않지만 대각합의 연산은 교환법칙이 성립한다.\n\\[  tr(\\pmb A \\pmb B)  = tr( \\pmb B \\pmb A) \\]\n\n\n대각합은 교환법칙이 성립히기 떄문에 다음과 같은 성질이 성립한다.\n\\[\n\\operatorname{tr}(\\pmb {A} \\pmb {K} \\pmb {L})=\\operatorname{tr}(\\pmb {K} \\pmb {L} \\pmb {A})\n\\]\n벡터의 연산에서도 대각합의 교환법칙이 성립되어 다음과 같은 유용한 식이 성립한다.\n\\[\n\\operatorname{tr}\\left(\\pmb {x} \\pmb {y}^t \\right)=\\operatorname{tr}\\left(\\pmb {y}^t  \\pmb {x}\\right)=\\pmb {y}^t  \\pmb {x} \\in \\mathbb{R} .\n\\]\n대각합의 교환법칙때문에 어떤 행렬의 앞에 특정 행렬을 곱하고, 뒤에 역행렬을 곱해도 대각합은 변하지 않는다.\n\\[\n\\operatorname{tr}\\left(\\pmb {S}^{-1} \\pmb {A} \\pmb {S}\\right) = \\operatorname{tr}\\left(\\pmb {A} \\pmb {S} \\pmb {S}^{-1}\\right)=\\operatorname{tr}(\\pmb {A})\n\\]\n대각합에 대한 그 밖의 성질들은 다음과 같다.\n\n\\(\\operatorname{tr}(\\alpha \\pmb {A})=\\alpha \\operatorname{tr}(\\pmb {A}), \\alpha \\in \\mathbb{R}\\) for \\(\\pmb {A} \\in \\mathbb{R}^{n \\times n}\\)\n\\(\\operatorname{tr}\\left(\\pmb {I}_n\\right)=n\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#행렬식",
    "href": "qmd/math_mat_basic.html#행렬식",
    "title": "부록 A — 행렬의 기초",
    "section": "A.9 행렬식",
    "text": "A.9 행렬식\n\\(\\pmb A\\)의 행렬식(determinant)을 \\(det(\\pmb A)=|\\pmb A|\\)로 표기한다.\n이차원 행렬 \\(\\pmb A\\) 의 행렬식은 다음과 같이 계산한다.\n\\[\n\\operatorname{det}( \\pmb {A})=\\left|\\begin{array}{ll}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{array}\\right|=a_{11} a_{22}-a_{12} a_{21} .\n\\]\n만약 행렬 \\(\\pmb A\\)가 대각행렬(diagonal matrix)이면 \\(|\\pmb A|\\)는 행렬의 대각원소의 곱이다 (\\(| \\pmb A| =\\prod a_{ii}\\)).\n두 행렬의 곱의 행렬식은 각 행렬의 행렬식의 곱이다.\n\\[ |\\pmb A \\pmb B | = | \\pmb A| |\\pmb B| \\]\n행렬식에 대한 유용한 공식들은 다음과 같다.\n\n\\(|{\\pmb A}^t| = |{\\pmb A}|\\)\n\\(|c {\\pmb A}| = c^n |{\\pmb A}|\\)\n\n만약 행렬 \\(\\pmb A\\)가 다음과 같은 분할행렬(partitioned matrix) 의 형태를 가지면\n\\[\n\\pmb A =\n\\begin{bmatrix}\n{\\pmb A}_{11} & {\\pmb A}_{12} \\\\\n{\\pmb 0} & {\\pmb A}_{22}\n\\end{bmatrix}\n\\]\n행렬 \\(\\pmb A\\)의 행렬식은 다음과 같이 주어진다.\n\\[ |{\\pmb A}| = |{\\pmb A}_{11}| |{\\pmb A}_{22} | \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#직교행렬",
    "href": "qmd/math_mat_basic.html#직교행렬",
    "title": "부록 A — 행렬의 기초",
    "section": "A.10 직교행렬",
    "text": "A.10 직교행렬\n만약 정방행렬 \\(\\pmb P\\)가 다음과 같은 조건을 만족하면 직교행렬(orthogonal matrix)라고 부른다.\n\\[  \\pmb P \\pmb P^t = \\pmb P^t \\pmb P = \\pmb I \\]\n직교행렬의 정의에서 주의할 점은 $P P^t = I $ 와 \\(\\pmb P^t \\pmb P = \\pmb I\\) 이 모두 성립하해야 한다는 점이다.\n행렬 \\(\\pmb P\\) 의 역행렬은 \\(\\pmb P^t\\) 이다.\n\\[ \\pmb P^{-1} = \\pmb P^t\\]\n만약 \\(\\pmb P\\)가 직교행렬이면 다음과 같은 성질을 가진다.\n\n\\(| \\pmb P | = \\pm 1\\) , 왜냐하면 \\[  | \\pmb P \\pmb P^t | = | \\pmb P | |\\pmb P^t |  = | \\pmb P|^2 = |\\pmb I| =1 \\]\n임의의 정방행렬 \\(\\pmb A\\)에 대하여 다음이 성립한다. \\[ tr(\\pmb P \\pmb A \\pmb P^t) = tr(\\pmb A \\pmb P^t \\pmb P) = tr(\\pmb A) \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#우드베리-공식",
    "href": "qmd/math_mat_basic.html#우드베리-공식",
    "title": "부록 A — 행렬의 기초",
    "section": "A.11 우드베리 공식",
    "text": "A.11 우드베리 공식\n다음은 우드베리공식(Woodbury formula) 과 파생된 유용한 공식들이다.\n\\[\n(\\pmb A+\\pmb U\\pmb C\\pmb V)^{-1} = \\pmb A^{-1}-\\pmb A^{-1} \\pmb U (\\pmb C^{-1} + \\pmb V \\pmb A^{-1}\\pmb U)^{-1} \\pmb V \\pmb A^{-1}\n\\]\n\\[\n(\\pmb I+\\pmb U \\pmb C\\pmb V)^{-1} = \\pmb I - \\pmb U (\\pmb C^{-1} + \\pmb V \\pmb U)^{-1} \\pmb V\n\\]\n\\[ (\\pmb A+\\pmb u\\pmb v^t)^{-1} = \\pmb A^{-1} - \\frac{ \\pmb A^{-1} \\pmb u\\pmb v^t \\pmb A^{-1}}{1+\\pmb v^t \\pmb A^{-1}\\pmb u}  \\tag{A.3}\\]\n\\[\n(a \\pmb I_n + b \\pmb 1_n \\pmb 1_n^t)^{-1} = \\frac{1}{a} \\left [ \\pmb I_n - \\frac{b}{a+nb} \\pmb 1 \\pmb 1^t \\right ]\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html",
    "href": "qmd/math_vector_space.html",
    "title": "부록 B — 벡터공간",
    "section": "",
    "text": "B.1 벡터공간의 정의와 의미\n먼저 지금까지 우리가 배운 벡터의 개념을 일반화하여 다루기 위해서 벡터공간의 일반적 개념을 정의하고자 한다.\n벡터는 숫자를 모아놓은 형태인 식 A.1 로 주로 나타내지만 이러한 벡터를 모아놓은 집합은 실벡터 공간(real vector space)이라고 한다. 즉, 식 A.1 의 벡터는 \\(p\\)-차원 실벡터(real vector)라고 한다.\n지금부터 논의할 추상적인 벡터 공간(abstract vector space)은 어떤 집합이든 원소에 대한 더하기와 스칼라곱이 정의되어 있는 공간을 말한다.\n이제부터 \\(\\RR\\) 을 실수 전체 집합이라고 하자. 또한 \\(\\RR^n\\) 을 \\(n\\)-차원 실벡터(real vector)의 집합이라고 하자. 또한 \\(\\RR^{n \\times p}\\) 을 \\(n \\times p\\)-차원 행렬의 집합이라고 하자.\n벡터공간(vector space) 은 어떤 집합 \\(S\\) 에 다음과 같은 두 개의 연산이 정의된 공간을 말한다.\n위에서 더하기 연산이 정의되어 있다는 의미는 다음에 주어진 규칙이 성립한다는 의미이다.\n\\[  s_1 + b \\in S \\quad \\forall s_1,b \\in S \\]\n\\[  (s_1 + s_2) + s_3 = s_1 + (s_2 +s_3)  \\quad \\forall s_1,s_2,s_3 \\in S \\]\n\\[  s + e = e + s = s \\quad \\exists e ~~\\forall s \\in S \\]\n\\[ s + i = i  + s = 0 \\quad \\exists i ~~\\ \\forall s \\in S \\]\n일반적으로 항등원(\\(e\\)) 는 \\(0\\) 으로 표시하며 역원(\\(i\\)) 는 \\(-s\\) 로 표시한다.\n\\[  s_1 + s_2 = s_2 + s_1  \\quad \\forall s_1,s_2 \\in S \\]\n또한 위에서 스칼라곱 연산이 정의되어 있다는 의미는 다음에 주어진 규칙이 성립한다는 의미이다.\n\\[  r_1(s_1+s_2) = r_1 s_1 + r_2 s_2,~~~ (r_1+r_2)s = r_1 s + r_2 s  \\quad \\forall s_1,s_2 \\in S, ~~ \\forall r_1,r_2 in \\RR \\]\n\\[  r_1(r_2s) = (r_1 r_2) s  \\quad \\forall s \\in S, ~~ \\forall r_1,r_2 in \\RR \\]\n\\[  1 \\cdot s  = s \\quad \\forall s \\in S \\]\n이 강의에서는 스칼라로 실수만 사용하고 벡터공간은 실벡터공간(real vector space)만 고려할 것이다. 하지만 벡터공간은 실벡터가 아닌 다른 일반적인 집합에 대해서도 정의할 수 있음을 유의하자. 예를 들어 \\(n\\)-차원 다힝식들을 모두 모아 놓은 집합은 벡터공간이다. 또한 연속인 함수들을 모아 놓은 집합도 벡터공간이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#벡터공간의-정의와-의미",
    "href": "qmd/math_vector_space.html#벡터공간의-정의와-의미",
    "title": "부록 B — 벡터공간",
    "section": "",
    "text": "두 개의 원소에 대한 더하기(addition, \\(+\\)) 연산의 정의되어 있다.\n\\[+ ~ ~ : S + S \\rightarrow  S \\tag{B.1}\\]\n하나의 실수와 한 개의 원소에 대한 스칼라곱(scalar product, \\(\\cdot\\)) 연산이 정의되어 있다.\n\\[\\cdot ~ ~ : \\RR \\cdot S \\rightarrow  S \\tag{B.2}\\]\n\n\n\n집합 \\(S\\) 가 연산에 대하여 닫혀있다 (closure).\n\n\n\n결합법칙이 성립한다 (Associativity).\n\n\n\n항등원이 존재한다 (Neutral element).\n\n\n\n역원이 존재한다 (Inverse element).\n\n\n\n\n교환법칙이 성립한다 (Commutativity).\n\n\n\n\n스칼라곱 연산의 분배법칙이 성립한다 (Distributivity).\n\n\n\n스칼라곱 연산의 결합법칙이 성립한다\n\n\n\n스칼라곱 연산의 항등원이 존재한다 (Neutral element).\n\n\n\n\n\n\n\n\n주의\n\n\n\n벡터 공간에서 주의할 점은 두 벡터의 곱하기 가 정의되어 있다는 것이 아니라 하나의 스칼라와 하나의 벡터에 대한 스칼라 곱하기가 정의되어 있다는 것이다.\n\\[\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n=?\n\\quad {but} \\quad\n3 \\cdot\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n3 \\\\\n6\n\\end{bmatrix}\n\\]\n두 벡터의 곱하기 는 내적(inner product) 란 이름으로 따로 정의한다. 또한 두 벡터의 곱셈이 유일하게 정의되지 않는다는 점에 유의하자. 예를 들어 벡터의 곱셈은 외적(cross product) 이라는 이름으로 정의된다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#벡터의-선형독립",
    "href": "qmd/math_vector_space.html#벡터의-선형독립",
    "title": "부록 B — 벡터공간",
    "section": "B.2 벡터의 선형독립",
    "text": "B.2 벡터의 선형독립\n벡터공간에 속한 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 의 선형결합(또는 선형결합, linear combination)이란 각 벡터에 스칼라를 곱하여 더한 것들이다.\n즉 다음과 같은 형태의 식을 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\)의 선형결합(linear combination)이라고 한다:\n\\[ r_1 \\pmb v_1 + r_2 \\pmb v_2 + \\cdots + r_n \\pmb v_n, \\quad r_1,r_2,\\dots, r_n \\in \\RR  \\tag{B.3}\\]\n\n정의 B.1 (벡터의 선형독립과 선형종속) 벡터공간에 속한 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 가 있다고 하자. 만약 다음 식이 만약 모두 \\(0\\)인 \\(n\\)개의 스칼라 \\(x_1,x_2,\\dots,x_n\\)에 대해서만 성립하면 \\(n\\)개 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 들은 선형독립(linearly independent)라고 한다.\n\\[\nx_1 \\pmb v_1 + x_2 \\pmb v_2 + \\dots + x_n \\pmb v_n = \\pmb 0 \\quad \\Longleftrightarrow\nx_1 = x_2 = \\dots = x_n =0\n\\tag{B.4}\\]\n또한 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 가 선형독립이 아니면 선형종속(linear dependent)라고 한다. 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 가 선형종속이면 모두 0이 아닌 \\(x_1,x_2,\\dots,x_n\\) 이 존재하여 다음이 성립한다는 것이다.\n\\[\n\\exists~ x_1,x_2,\\dots,x_n \\in \\RR \\text{ s.t. } (x_1,x_2,\\dots,x_n) \\ne \\pmb 0,\\quad  \\pmb v_1 + x_2 \\pmb v_2 + \\dots + x_n \\pmb v_n = \\pmb 0\n\\tag{B.5}\\]\n\\(\\blacksquare\\)\n\n예를 들어 다음과 같이 주어진 3개의 3-차원 벡터들은 선형종속이다.\n\\[\n\\pmb v_1 =\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\n\\end{bmatrix},\n\\quad\n\\pmb v_2 =\n\\begin{bmatrix}\n1\\\\\n0\\\\\n1\n\\end{bmatrix},\n\\quad\n\\pmb v_3 =\n\\begin{bmatrix}\n3\\\\\n2\\\\\n5\n\\end{bmatrix}\n\\tag{B.6}\\]\n왜냐하면 다음과 같이 모두 0이 아닌 스칼라에 의해서 다음 식이 성립하기 떄문이다. 즉 벡터 \\(\\pmb v_3\\)는 \\(\\pmb v_2\\) 에 2를 곱하여 \\(\\pmb v_1\\)에 더한 값과 같다.\n\\[\n\\pmb v_3 = \\pmb v_1 + 2 \\pmb v_2 \\quad \\Longleftrightarrow \\quad    \\pmb v_1 + 2 \\pmb v_2 -\\pmb v_3 = 0\n\\]\n이제 다음과 같이 주어진 3개의 3-차원 벡터들은 선형독립이다. 즉 3개 벡터의 선형 조합이 0이 될 수 있도록 만드는 스칼라는 모두 0인 경우 밖에 없다.\n\\[\n\\pmb v_1 =\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\n\\end{bmatrix},\n\\quad\n\\pmb v_2 =\n\\begin{bmatrix}\n1\\\\\n0\\\\\n1\n\\end{bmatrix},\n\\quad\n\\pmb v_3 =\n\\begin{bmatrix}\n3\\\\\n2\\\\\n4\n\\end{bmatrix}\n\\tag{B.7}\\]\n이제 다음과 같이 주어진 4개의 3-차원 벡터들은 선형종속이다.\n\\[\n\\pmb v_1 =\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\n\\end{bmatrix},\n\\quad\n\\pmb v_2 =\n\\begin{bmatrix}\n1\\\\\n0\\\\\n1\n\\end{bmatrix},\n\\quad\n\\pmb v_3 =\n\\begin{bmatrix}\n3\\\\\n2\\\\\n4\n\\end{bmatrix}\n\\quad\n\\pmb v_4 =\n\\begin{bmatrix}\n0\\\\\n0\\\\\n1\n\\end{bmatrix}\n\\tag{B.8}\\]\n\\(\\pmb v_3\\) 가 다음과 같이 다른 벡터의 선형결합으로 나타난는 것을 보여준다.\n\\[ \\pmb v_3 = \\begin{bmatrix}\n3\\\\\n2\\\\\n4\n\\end{bmatrix}\n= (1)\\pmb v_1 +  (2)\\pmb v_2 +  (-1)\\pmb v_4\n= (1)\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\n\\end{bmatrix}\n+\n(2)\n\\begin{bmatrix}\n1\\\\\n0\\\\\n1\n\\end{bmatrix}\n+\n(-1)\\begin{bmatrix}\n0\\\\\n0\\\\\n1\n\\end{bmatrix}\n\\]\n식 B.8 와 같이 3차원 벡터가 4개인 경우 벡터의 값에 관계없이 선형종속으로 나타난다. 이러한 사실은 \\(\\RR^n\\)의 \\(n+1\\) 개의 벡터는 항상 선형종속이라는 정리의 결과이다.\n즉, \\(\\RR^n\\)에서 \\(n\\)개보다 더 많은 벡터들은 항상 선형종속이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#역행렬",
    "href": "qmd/math_vector_space.html#역행렬",
    "title": "부록 B — 벡터공간",
    "section": "B.3 역행렬",
    "text": "B.3 역행렬\n정방행렬 \\(\\pmb A\\) 의 역행렬(inverse matrix) \\(\\pmb A^{-1}\\)는 다음과 같은 성질을 만족하는 행렬이다.\n\\[ \\pmb A \\pmb A^{-1} = \\pmb A^{-1} \\pmb A = \\pmb I \\]\n역행렬은 언제나 존재하는 것은 아니다. 만약 행렬 \\(\\pmb A\\)가 역행렬을 가지면 이를 \\(\\pmb A^{-1}\\)로 표시한다. 또한 역행렬이 존재하면 정칙행렬(non-singular matrix)이라고 한다.\n역행렬은 존재하는 조건은 행렬식(determinant)이 0이 아니어야 한다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#행렬의-계수",
    "href": "qmd/math_vector_space.html#행렬의-계수",
    "title": "부록 B — 벡터공간",
    "section": "B.4 행렬의 계수",
    "text": "B.4 행렬의 계수\n행렬의 계수(rank)란 일차 독립인 열들의 최대 수 또는 일차 독립인 행들의 최대 수로 정의된다\n\\[ rank(\\pmb A) = rk(\\pmb A) = dim(Col(\\pmb A)) = dim(Row(\\pmb A)) \\] 꼭 기억해야 할 것은 행렬의 계수는 열들을 이용하여 구한 계수와 행들을 이용하여 구한 계수가 같다는 것이다. 즉, 행렬의 계수는 열의 계수와 행의 계수 중 하나만 구해도 된다는 것이다.\n예를 들어 식 B.6 에 주어진 3 개의 벡터를 열로 하는 행렬의 계수는 2이다. 왜냐하면 선형종속인 벡터가 하나 있기 때문이다.\n\\[\n\\pmb A =\n\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 0 & 2\\\\\n3 & 1 & 5\n\\end{bmatrix} \\quad \\rightarrow \\quad rank(\\pmb A) = 2\n\\]\n위에 주어진 행렬 \\(\\pmb A\\)의 행들을 고려하면 첫 번째 행과 두 번째 행의 합이 세 번째 행으로 나타난다. 즉, 서로 독립인 행의 최대 개수는 2 이며 이는 서로 독립인 열의 최대 개수와 같다. 따라서 행렬 \\(\\pmb A\\)의 계수는 2이다.\n다음으로 식 B.7 에 주어진 3 개의 벡터를 열로 하는 행렬의 계수는 3이다. 3개의 열벡터와 3개의 행벡터들은 모두 선형독립이다.\n\\[\n\\pmb A =\n\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 0 & 2\\\\\n3 & 1 & 4\n\\end{bmatrix} \\quad \\rightarrow \\quad rank(\\pmb A) = 3\n\\] 이제 식 B.8 주어진 4개의 벡터로 이루어진 행렬의 계수는 3이다. 왜냐하면 4개의 열벡터 중 3개의 열벡터는 선형독립이지만 4번째 열벡터는 선형종속이기 때문이다.\n\\[\n\\pmb A =\n\\begin{bmatrix}\n1 & 1 & 3 & 0\\\\\n2 & 0 & 2 & 0\\\\\n3 & 1 & 4 & 1\n\\end{bmatrix} \\quad \\rightarrow \\quad rank(\\pmb A) = 3\n\\]\n행렬의 열과 행의 개수가 다를 때 행렬의 계수는 열의 개수와 행의 개수 중 작은 값보다 같거나 작다 예를 들어 \\(n \\times p\\) 행렬의 계수는 \\(min(n,p)\\) 와 같거나 작다.\n\\[ A \\in \\RR^{n \\times p} \\quad \\rightarrow \\quad rank(\\pmb A) \\le min(n,p) \\]\n다음은 행렬의 계수에 관련된 주요 공식이다.\n\n\\(rank(\\pmb A ) = rank(\\pmb A^t)\\)\n행렬 \\(\\pmb A\\) 가 정방행렬이고 계수가 \\(n\\) 이면 역행렬이 존재한다(정칙행렬).\n또한 더 나아가 \\(\\pmb A\\) 가 정칙행렬이라는 사실은 아래 나열된 조건들과 동치(equivalance)이다.\n\\(\\quad \\Leftrightarrow\\) \\(\\pmb A\\) 의 열들이 일차독립이다.\n\\(\\quad \\Leftrightarrow\\) \\(\\pmb A\\) 의 행들이 일차독립이다.\n\\(\\quad \\Leftrightarrow\\) \\(\\pmb A\\) 의 계수가 \\(n\\) 이다.\n\\(\\quad \\Leftrightarrow\\) \\(\\pmb A\\) 의 행렬식이 0이 아니다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#생성집합과-기저",
    "href": "qmd/math_vector_space.html#생성집합과-기저",
    "title": "부록 B — 벡터공간",
    "section": "B.5 생성집합과 기저",
    "text": "B.5 생성집합과 기저\n벡터공간 \\(V\\) 의 벡터 \\(\\pmb v_1,\\pmb v_n, \\dots, \\pmb v_m\\) 의 선형결합을 모두 모은 집합\n\\[ W = span\\{\\pmb v_1,\\pmb v_2, \\dots, \\pmb v_m \\} = \\{r_1\\pmb v_1 + r_2 \\pmb v_2 + \\cdots+ r_m \\pmb v_m:\nr_1,r_2,\\dots,r_m \\in \\RR \\}\\]\n을 벡터 \\(\\pmb v_1,\\pmb v_n, \\dots, \\pmb v_m\\) 의 생성(span)이라고 하며 \\(W\\) 의 생성집합(generating set, spanning set) 이라고 한다.\n또한 어떤 벡터공간의 생성집합에 속한 벡터들이 선형독립일 때 이 생성집합을 기저 (basis)라고 한다.\n만약 주어진 벡터 공간의 부분집합이 다시 벡터공간의 정의를 만족한다면 이를 부분공간(subspace)이라고 한다. 위에서 정의한 생성집합 \\(W\\)는 벡터공간 \\(V\\)의 부분공간이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#벡터공간의-차원",
    "href": "qmd/math_vector_space.html#벡터공간의-차원",
    "title": "부록 B — 벡터공간",
    "section": "B.6 벡터공간의 차원",
    "text": "B.6 벡터공간의 차원\n\n\\(\\RR^n\\) 의 모든 기저는 \\(n\\)개의 원소를 갖는다.\n임의의 벡터공간 \\(V\\)에 대해서 \\(V\\)의 부분집합 \\(B = \\{\\pmb b_1,\\dots,\\pmb b_n\\}\\) 가 \\(V\\)의 기저라고 하면 다음을 보일 수 있다.\n\n\\(V\\) 의 모든 벡터들은 \\(\\pmb b_1,\\dots,\\pmb b_n\\) 의 선형결합으로 나타낼 수 있으며 유일하다.\n\\(V\\) 의 부분집합이 \\(n\\) 개보다 많은 벡터를 포함하면 이 부분집합의 벡터들은 선형종속이다.\n\\(V\\) 의 또 다른 기저 \\(C=\\{\\pmb c_1,\\dots,\\pmb c_m \\}\\) 가있다면 \\(m=n\\) 이다.\n\n벡터공간 \\(V\\)의 차원(dimension) 은 기저의 개수로 정의되며 \\(dim(V)\\)로 표시한다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#행렬의-열공간과-행공간",
    "href": "qmd/math_vector_space.html#행렬의-열공간과-행공간",
    "title": "부록 B — 벡터공간",
    "section": "B.7 행렬의 열공간과 행공간",
    "text": "B.7 행렬의 열공간과 행공간\n\\(n \\times p\\) 행렬 \\(\\pmb A\\) 에 의하여 생성되는 열공간(column space) \\(C(\\pmb A)\\)는 행렬 \\(\\pmb A\\)를 구성하는 열벡터의 선형조합으로 나타낼 수 있는 모든 벡터들의 집합을 말한다.\n\\[ C(\\pmb A) = \\{ {\\pmb y } | {\\pmb y} = {\\pmb A} {\\pmb x}, {\\pmb x} \\in \\RR^p \\}  \\subset \\RR^n\\]\n\\(n \\times p\\) 행렬 \\(\\pmb A\\) 에 의하여 생성되는 영공간(null space) \\(N(\\pmb A)\\) 는 다음과 같이 정의되는 벡터들의 집합을 말한다.\n\\[ N(\\pmb A) = \\{ {\\pmb x} | {\\pmb A} {\\pmb x} = {\\pmb 0} \\text{ for } {\\pmb x} \\in \\RR^p \\} \\subset \\RR^n\\]\n벡터공간과 영공간은 다음과 같은 성질을 가진다.\n\n\\(rank(\\pmb A) = \\text{ dimension of } C(\\pmb A) = dim[C(\\pmb A)]\\)\n\\(dim[C(\\pmb A)] + dim[N(\\pmb A)] = n\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#두-벡터의-사영",
    "href": "qmd/math_vector_space.html#두-벡터의-사영",
    "title": "부록 B — 벡터공간",
    "section": "B.8 두 벡터의 사영",
    "text": "B.8 두 벡터의 사영\n선형독립인 두 벡터 \\(\\pmb a_1\\)과 \\(\\pmb a_2\\) 가 있다고 하자. 벡터 \\(\\pmb a_1\\)과 같은 방향을 가지면서 벡터 \\(\\pmb a_2\\)에 가장 가까운 벡터를 \\(proj_{\\pmb a_1} (\\pmb a_2)\\) 라고 정의하고 이를 벡터 \\(\\pmb a_1\\) 방향으로 벡터 \\(\\pmb a_2\\) 의 사영(projection)이라고 부른다.\n그러면 이러한 사영은 어떻게 구할 수 있나? 벡터 \\(\\pmb a_2\\) 의 사영은 벡터 \\(\\pmb a_1\\) 방향에 있으므로 어떤 실수 \\(c\\) 가 있어서 다음과 같이 표시할 수 있다.\n\\[ proj_{\\pmb a_1} (\\pmb a_2) =  c \\pmb a_1 \\]\n이제 사영 \\(c \\pmb a_1\\)과 벡터 \\(\\pmb a_2\\)의 거리 \\(d(c)\\) 를 생각하면 다음과 같다.\n\\[\n\\begin{aligned}\nd^2(c) & = \\norm{\\pmb a_2 - c \\pmb a_1}^2 \\\\\n   & = (\\pmb a_2 - c \\pmb a_1)^t(\\pmb a_2 - c \\pmb a_1) \\\\\n   & = \\pmb a^t_2 \\pmb a_2 -2 c \\pmb a_2^t \\pmb a_1 + c^2 \\pmb a^t_1 \\pmb a_1\n\\end{aligned}\n\\]\n위에서 \\(\\norm{\\pmb a}\\) 는 벡터 \\(\\pmb a\\)의 길이를 나타낸다.\n\\[ d(\\pmb a) = \\norm{\\pmb a} = \\sqrt{\\pmb a^t \\pmb a} \\]\n상수 \\(c\\) 는 거리 \\(d(c)\\)를 최소로 만드는 수이다. \\(d^2(c)\\)은 \\(c\\) 에 대하여 미분 가능한 2차 함수이며 아래로 볼록한 함수이므로 이를 미분하여 \\(c\\) 를 구할 수 있다.\n\\[ \\pardiff{d^2(c)}{c} = - 2\\pmb a_2^t \\pmb a_1 + 2c \\pmb a^t_1 \\pmb a_1 =0 \\]\n위의 방적식으로 부터 \\(c\\)를 얻고 \\[ c= \\frac{\\pmb a_2^t \\pmb a_1  }{\\pmb a^t_1 \\pmb a_1} \\]\n다음과 같이 벡터 \\(\\pmb a_1\\) 방향으로 벡터 \\(\\pmb a_2\\) 의 사영을 나타낼 수 있다.\n\\[\nproj_{\\pmb a_1} (\\pmb a_2) = \\frac{ \\pmb a_2^t \\pmb a_1} {\\pmb a_1^t \\pmb a_1} \\pmb a_1\n\\tag{B.9}\\]\n이제 위의 두 벡터의 사영을 이용하면 벡터 \\(\\pmb a_1\\) 과 직교하는 벡터 \\(\\tilde {\\pmb q}_2\\)를 다음과 같이 찾을 수 있다.\n\\[\n\\tilde {\\pmb q}_2 = \\pmb a_2 - proj_{\\pmb a_1} (\\pmb a_2) = \\pmb a_2 -  \\frac{\\pmb a_2^t \\pmb a_1} {\\pmb a_1^t \\pmb a_1} \\pmb a_1\n\\]\n\n\n\n\n\n벡터의 사영\n\n\n\n\n두 벡터 \\(\\pmb a_1\\)와 \\(\\tilde {\\pmb q}_2\\)의 직교성은 다음과 같이 보일 수 있다.\n\\[\n\\begin{aligned}\n\\pmb a_1^t  \\tilde {\\pmb q}_2 & =\n\\pmb a_1^t  \\left ( \\pmb a_2 -  \\frac{ \\pmb a_2^t \\pmb a_1} {\\pmb a_1^t \\pmb a_1} \\pmb a_1 \\right ) \\notag \\\\\n& = \\pmb a_1^t \\pmb a_2 - \\frac{ \\pmb a_2^t \\pmb a_1} {\\pmb a_1^t \\pmb a_1}  \\pmb a_1^t \\pmb a_1 \\notag \\\\\n& = \\pmb a_1^t \\pmb a_2 - \\pmb a_2^t \\pmb a_1 \\notag \\\\\n& = 0\n\\end{aligned}\n\\tag{B.10}\\]\n이제 두 벡터 \\(\\pmb q_1\\)과 \\(\\pmb q_2\\) 를 다음과 같이 정규직교벡터로 만들 수 있다.\n\\[\n\\begin{aligned}\n\\pmb q_1 & =  \\pmb a_1 / \\norm{\\pmb a_1 } \\\\\n\\pmb q_2 & =  \\tilde {\\pmb q}_2 / \\norm{\\tilde {\\pmb q}_2}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#최소제곱법과-사영",
    "href": "qmd/math_vector_space.html#최소제곱법과-사영",
    "title": "부록 B — 벡터공간",
    "section": "B.9 최소제곱법과 사영",
    "text": "B.9 최소제곱법과 사영\n회귀계수벡터의 값을 구하는 최소제곱법의 기준을 다시 살펴보자.\n\\[   \n\\min_{\\pmb \\beta } ( \\pmb y -  \\pmb X \\pmb \\beta )^t( \\pmb y -  \\pmb X \\pmb \\beta )  \n\\]\n위에서 \\(\\pmb X \\pmb \\beta\\)는 행렬 \\(\\pmb X\\)의 열벡터 \\(\\pmb x_1, \\pmb x_2, \\dots, \\pmb x_p\\) 로 이루어진 선형조합이다.\n\\[ \\pmb X \\pmb \\beta = [\\pmb x_1~ \\cdots \\pmb x_p]\\pmb \\beta\n=  \\beta_1 \\pmb x_1 + \\cdots + \\beta_p \\pmb x_p \\]\n행렬 \\(\\pmb X\\)의 열벡터로 생성돤 공간을 \\(C(\\pmb X)\\) 라고 하자\n\\[ C(\\pmb X) = span \\{ \\pmb x_1, \\dots, \\pmb x_p \\}  = \\{ \\pmb X \\pmb \\beta ~|~ \\pmb \\beta \\in \\RR^p \\}\\]\n따라서 최소제곱법으로 구한 회귀계수 벡터 \\(\\hat {\\pmb \\beta}\\)는 반응값 벡터 \\(\\pmb y\\) 와 \\(\\pmb X {\\pmb \\beta}\\)의 거리가 최소가 되도록 만들어 준다.\n\\[\n\\min_{\\pmb \\beta } ( \\pmb y -  \\pmb X \\pmb \\beta )^t( \\pmb y -  \\pmb X \\pmb \\beta )  =\n( \\pmb y -  \\pmb X  \\hat {\\pmb \\beta} )^t( \\pmb y -  \\pmb X  \\hat {\\pmb \\beta} )  \n\\] \\[\n\\hat {\\pmb \\beta} = (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y\n\\]\n따라서 예측값 벡터 \\(\\hat {\\pmb y}\\) 는 행렬 \\(\\pmb X\\)의 열벡터로 생성한 열공간 \\(C(\\pmb X)\\) 방향으로 반응값 벡터 \\(\\pmb y\\)를 사영한 벡터이다.\n\\[ \\hat {\\pmb y} = \\pmb X \\hat {\\pmb \\beta} = \\pmb  X(\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y \\]\n위에서 행렬 \\(\\pmb X(\\pmb X^t \\pmb X)^{-1} \\pmb X^t\\)를 열공간 \\(C(\\pmb X)\\)의 사영행렬(projection matrix)라고 부른다. 사영행렬의 정의는 부록 F 에서 공부한다.\n\\[\n\\pmb H = \\pmb X(\\pmb X^t \\pmb X)^{-1} \\pmb X^t\n\\tag{B.11}\\]\n\n\n\n\n\n최소제곱법을 설명한 그림",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_eigen_value.html",
    "href": "qmd/math_eigen_value.html",
    "title": "부록 C — 고유값과 고유벡터",
    "section": "",
    "text": "C.1 특성다항식\n특성다항식(Characteristic polynomial)은 다음과 같이 정의된다\n실수 \\(\\lambda \\in \\mathbb{R}\\) 와 정방행렬(square matrix) \\(\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}\\) 에 대하여\n\\[\n\\begin{aligned}\np_{\\boldsymbol{A}}(\\lambda) & :=\\operatorname{det}(\\boldsymbol{A}-\\lambda \\boldsymbol{I}) \\\\\n& =c_0+c_1 \\lambda+c_2 \\lambda^2+\\cdots+c_{n-1} \\lambda^{n-1}+(-1)^n \\lambda^n,\n\\end{aligned}\n\\tag{C.1}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>고유값과 고유벡터</span>"
    ]
  },
  {
    "objectID": "qmd/math_eigen_value.html#고유값과-고유벡터",
    "href": "qmd/math_eigen_value.html#고유값과-고유벡터",
    "title": "부록 C — 고유값과 고유벡터",
    "section": "C.2 고유값과 고유벡터",
    "text": "C.2 고유값과 고유벡터\n\nC.2.1 정의\n\\(n\\)-차원 정방행렬 \\(\\pmb A\\) 이 있을 때, 다음 식을 만족하는 \\(\\lambda\\) 와 벡터 \\(\\pmb x\\)가 존재하면 \\(\\lambda\\) 를 행렬 \\(\\pmb A\\) 의 고유값(eigenvalue), \\(\\pmb x\\) 를 행렬 \\(\\pmb A\\) 의 고유벡터(eigenvector)라고 한다 (부교재 definition 4.6)\n\\[ \\pmb A \\pmb x = \\lambda \\pmb x \\]\n\n고유벡터는 유일하지 않다. 즉, 벡터 \\(\\pmb x\\) 가 고유벡터이면 \\(c \\pmb x\\) 도 고유벡터이다.\n\n\\[ \\pmb A (c \\pmb x) = c \\pmb A \\pmb x = c \\lambda \\pmb x = \\lambda (c \\pmb x) \\]\n\n\nC.2.2 계산\n다음 3개의 문장은 동치이다\n\n\\(\\lambda\\) 는 행렬 \\(\\pmb A\\) 의 고유값이다.\n방정식 \\((\\pmb A - \\lambda \\pmb I)\\pmb x = \\pmb 0\\) 은 영벡터이외의 해를 가진다(nontrivial solution)\n\\(\\lambda\\) 는 행렬 \\(\\pmb A - \\lambda \\pmb I\\) 의 행렬식이 0이다.\n\n\\[ \\operatorname{det}(\\pmb A - \\lambda \\pmb I) = 0  \\tag{C.2}\\]\n\n\\(\\lambda\\) 는 행렬 \\(\\pmb A - \\lambda \\pmb I\\) 의 rank가 \\(n\\) 보다 작다.\nTheorem 4.8 에 의하면 위에서 행렬식이 0 인 방정식 식 C.2 을 푸는 것은 식 C.1 의 이 0 을 푸는 것과 동일하다는 것이다.\n\n\n\nC.2.3 중복도와 고유공간\n\n대수적 중복도(algebraic multiplicity) 는 특성다항식 식 C.1 이 0인 방정식을 푸는 경우 다항식에서 고유값이 중근(multiple root)의 해로 나타나는 차수를 의미한다.\n기하적 중복도(geometric multiplicity) 는 고유값에 대응하는 고유벡터들 중 선형독립인 고유벡터들의 최대 개수를 의미한다.\n고유 공간(eigenspace)은 고유값에 대응하는 고유벡터들이 생성하는 벡터공간을 의미한다.\n\n\n\n예제 C.1 3차원 행렬 \\(\\pmb A\\) 가 다음과 같을 때\n\\[\\pmb A=\\left[\\begin{array}{ccc}0 & 0 & -2 \\\\ 1 & 2 & 1 \\\\ 1 & 0 & 3\\end{array}\\right]\\]\n행렬 \\(\\pmb A\\)의 특성다항식은 다음과 같다.\n\\[\n\\operatorname{det}(\\lambda \\pmb I -\\pmb A)=\n\\left|\\begin{array}{ccc}\n\\lambda & 0 & 2 \\\\\n-1 & \\lambda-2 & -1 \\\\\n-1 & 0 & \\lambda-3\n\\end{array}\\right|=(\\lambda-1)(\\lambda-2)^2\n\\] 참고로 특성방정식을 푸는 경우, 방정식 \\(\\operatorname{det}(\\pmb A - \\lambda \\pmb I)=0\\) 이나 \\(\\operatorname{det}(\\lambda \\pmb I -\\pmb A)= 0\\) 중 어느 것을 사용해도 상관없다.\n첫번째 고유값은 \\(\\lambda_1=1\\) 이다. 고유벡터를 구하기 위해서는 다음과 같은 방정식을 풀면 된다.\n\\[ (\\lambda_1 \\pmb I -\\pmb A )\\pmb x = \\pmb 0  \\]\n위의 방정식을 풀면\n\\[\n(\\lambda_1 \\pmb I -\\pmb A )\\pmb x= (\\pmb I -\\pmb A )\\pmb x\n=\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n-1 & -1 & -1 \\\\\n-1 & 0 & -2\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\n아래와 같이 간단히 할 수 있으며\n\\[ x_1 = -2x_3, \\quad x_2 = x_3 \\] 다음과 같은 고유값과 고유벡터를 얻을 수 있다.\n\\[ \\lambda_1=1 \\quad \\rightarrow \\quad  {\\pmb x}_1=\\begin{bmatrix}-2 \\\\ 1 \\\\ 1\\end{bmatrix} \\]\n첫번째 고유값은 \\(\\lambda_1=1\\) 이며 대수적 중복도는 1이고 기하적 중복도도 1이다. 이 경우 고유공간 \\(E_1\\) 은 한 개의 고유벡터 \\(\\pmb x_1\\) 이 생성하는 부분공간을 의미한다.\n\\[\nE_1 = \\text{span}\\left\\{\\begin{bmatrix}-2 \\\\ 1 \\\\ 1\\end{bmatrix} \\right\\}\n\\]\n다음으로 두번째 고유값에 대한 방정식 \\((\\lambda_2 \\pmb I -\\pmb A )\\pmb x = \\pmb 0\\) 을 풀면 다음과 같다.\n\\[\n(\\lambda_2 \\pmb I -\\pmb A )\\pmb x= (2\\pmb I -\\pmb A )\\pmb x =\n\\begin{bmatrix}\n2 & 0 & 2 \\\\\n-1 & 0 & -1 \\\\\n-1 & 0 & -1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\n이 방정식은 아래와 같이 간단히 할 수 있으며\n\\[ x_1 = -x_3 \\] 다음과 같은 두 개의 고유벡터를 얻을 수 있다.\n\\[\n\\lambda_2=2\\quad \\rightarrow \\quad  {\\pmb x}_2=\\begin{bmatrix}-1 \\\\ 0 \\\\ 1\\end{bmatrix}\n\\quad {\\pmb x}_3=\\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix}\n\\]\n위에서 두번째 고유값은 \\(\\lambda_2=2\\) 이며 대수적 중복도는 2이다. 또한 선형독립인 2개의 고유벡터를 구할 수 있으므로 기하적 중복도는 2이다.\n이 경우 \\(E_2\\) 는 두 개의 고유벡터 \\(\\pmb x_2, \\pmb x_3\\) 가 생성하는 부분공간을 의미한다.\n\\[\nE_2 = \\text{span}\\left\\{\\begin{bmatrix}-1 \\\\ 0 \\\\ 1\\end{bmatrix}, \\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix}\\right\\}\n\\]\n\\(\\blacksquare\\)\n\n\n이제 대수적 중복도와 기하적 중복도가 다른 경우에 대한 예제를 들어보자.\n\n\n예제 C.2 3차원 행렬 \\(\\pmb A\\) 가 다음과 같을 때\n\\[\\pmb A=\\left[\\begin{array}{ccc}1 & 0 & 2 \\\\ -1 & 1 & 3 \\\\ 0 & 0 & 2\\end{array}\\right]\\]\n행렬 \\(\\pmb A\\)의 특성다항식은 다음과 같다.\n\\[\n\\operatorname{det}(\\lambda \\pmb I -\\pmb A)=\n\\left|\\begin{array}{ccc}\n\\lambda-1 & 0 & -2 \\\\\n1 & \\lambda-1 & -3  \\\\\n0 & 0 & \\lambda-2\n\\end{array}\\right|=(\\lambda-1)^2(\\lambda-2)\n\\] 첫번째 고유값은 \\(\\lambda_1=1\\) 이다. 고유벡터를 구하기 위해서는 다음과 같은 방정식을 풀면 된다.\n\\[ (\\lambda_1 \\pmb I -\\pmb A )\\pmb x = \\pmb 0  \\]\n위의 방정식을 풀면\n\\[\n(\\lambda_1 \\pmb I -\\pmb A )\\pmb x= (\\pmb I -\\pmb A )\\pmb x =\n\\begin{bmatrix}\n0 & 0 & -2 \\\\\n1 & 0 & -3 \\\\\n0 & 0 & -1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\n아래와 같이 간단히 할 수 있으며\n\\[ \\quad x_1 = x_3 =0 \\] 다음과 같은 하나의 고유벡터를 얻을 수 있다.\n\\[ \\lambda_1=1 \\quad \\rightarrow \\quad  x_1=\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\]\n첫번째 고유값은 \\(\\lambda_1=1\\) 이며 대수적 중복도는 2이지만 기하적 중복도는 1이다. 이 경우 고유공간 \\(E_1\\) 은 한 개의 고유벡터 \\(\\pmb x_1\\) 이 생성하는 부분공간을 의미한다.\n\\[\nE_1 = \\text{span}\\left\\{\\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix} \\right\\}\n\\]\n다음으로 두번째 고유값에 대한 방정식 \\((\\lambda_2 \\pmb I -\\pmb A )\\pmb x = \\pmb 0\\) 을 풀면 다음과 같다.\n\\[\n(\\lambda_2 \\pmb I -\\pmb A )\\pmb x= (2\\pmb I -\\pmb A )\\pmb x =\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n1 & 1 & -3 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\n이 방정식은 아래와 같이 간단히 할 수 있으며\n\\[ x_1 = -2x_3, \\quad x_2=5x_3  \\] 다음과 같은 한 개의 고유벡터를 얻을 수 있다.\n\\[\n\\lambda_2=2\\quad \\rightarrow \\quad  x_2=\\begin{bmatrix}-2 \\\\ 5 \\\\ 1\\end{bmatrix}\n\\]\n위에서 두번째 고유값은 \\(\\lambda_2=2\\) 이며 대수적 중복도는 1이다. 또한 선형독립인 1개의 고유벡터를 구할 수 있으므로 기하적 중복도는 1이다.\n이 경우 \\(E_2\\) 는 한 개의 고유벡터 \\(\\pmb x_2\\) 가 생성하는 부분공간을 의미한다.\n\\[\nE_2 = \\text{span}\\left\\{\\begin{bmatrix}-2\\\\ 5 \\\\ 1\\end{bmatrix}\\right\\}\n\\]\n\\(\\blacksquare\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>고유값과 고유벡터</span>"
    ]
  },
  {
    "objectID": "qmd/math_eigen_value.html#대칭행렬의-대각화",
    "href": "qmd/math_eigen_value.html#대칭행렬의-대각화",
    "title": "부록 C — 고유값과 고유벡터",
    "section": "C.3 대칭행렬의 대각화",
    "text": "C.3 대칭행렬의 대각화\n\\(n\\)차원 대칭행렬 \\(\\pmb  A\\) 에 대하여 직교행렬 \\(\\pmb  P\\)가 존재하여 다음과 같은 분해가 가능하다.\n\\[\n\\pmb  P^t \\pmb  A \\pmb  P = \\pmb  \\Lambda = diag(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)\n\\tag{C.3}\\]\n식 F.2 의 분해에서 \\(\\lambda_i\\)는 행렬 \\(\\pmb  A\\)의 고유치이며 행렬 \\(\\pmb  P\\)의 \\(i\\) 번째 열은 대응하는 고유벡터 \\(\\pmb  p_i\\) 로 구성되어 있다.\n\\[\n\\pmb  P = [ \\pmb  p_1~~ \\pmb  p_2 ~~ \\dots ~~ \\pmb  p_n ]\n\\] 이제 위의 분해를 증명해 보자. 고유치 \\(\\lambda_i\\) 와 대응하는 고유벡터 \\(\\pmb  p_i\\)의 정의에 따라서 다음과 같은 \\(n\\)개의 식을 얻을 수 있고\n\\[ \\pmb  A \\pmb  p_i = \\lambda_i \\pmb  p_i , \\quad i=1,2,3\\dots, n \\]\n위의 식을을 합쳐서 표기하면 다음과 같은 식을 얻으며 이는 식 F.2 를 의미한다.\n\\[ \\pmb  A \\pmb  P = \\pmb  P \\pmb  \\Lambda \\]\n식 F.2 를 다시 쓰면 다음과 같은 스펙트럴 분해(spectral decomposition)를 얻는다.\n\\[\n\\pmb  A  = \\pmb  P \\pmb  \\Lambda \\pmb  P^t  = \\sum_{i=1}^n \\lambda_i \\pmb  p_i \\pmb  {p}_i^t\n\\tag{C.4}\\]\n참고로 다음의 유용한 두 식을 기억하자.\n\\[ tr(\\pmb  A) = \\sum_i \\lambda_i ,\\quad |\\pmb  A| = \\prod_i \\lambda_i \\]\n대칭행렬의 분해 (ref?)(eq:symmdecomp1)를 이용하면 다음과 같은 이차형식의 분해를 얻을 수 있다.\n\\[\\begin{equation}\nQ(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x = \\pmb  x^t \\pmb  P \\pmb  \\Lambda \\pmb  P^t \\pmb  x = \\pmb  y^t \\pmb  \\Lambda \\pmb  y= \\sum_{i=1}^n \\lambda_i y_i^2\n(\\#eq:quaddecomp)\n\\end{equation}\\]\n이차형식의 분해식 @ref(eq:quaddecomp) 를 보면 행렬 \\(\\pmb  A\\)의 모든 고유치가 0보다 크면 양정치 임을 알 수 있다. 또한 모든 고유치가 0보다 크거나 같으면 양반정치 임을 알 수 있다.\n또한 \\(rank(\\pmb  A) = rank(\\pmb  \\Lambda)\\)이며 이는 0이 아닌 고유치의 개수가 행렬 \\(\\pmb  A\\)의 계수(rank)임을 알 수 있다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>고유값과 고유벡터</span>"
    ]
  },
  {
    "objectID": "qmd/math_vec_cal.html",
    "href": "qmd/math_vec_cal.html",
    "title": "부록 D — 벡터 미분",
    "section": "",
    "text": "D.1 용어",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>벡터 미분</span>"
    ]
  },
  {
    "objectID": "qmd/math_vec_cal.html#용어",
    "href": "qmd/math_vec_cal.html#용어",
    "title": "부록 D — 벡터 미분",
    "section": "",
    "text": "vector differential: 벡터 미분\npartial derivative: 편미분\ngradient: 그레디언트\nJacobian: 야코비안, 자코비안",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>벡터 미분</span>"
    ]
  },
  {
    "objectID": "qmd/math_vec_cal.html#벡터-미분의-표기법",
    "href": "qmd/math_vec_cal.html#벡터-미분의-표기법",
    "title": "부록 D — 벡터 미분",
    "section": "D.2 벡터 미분의 표기법",
    "text": "D.2 벡터 미분의 표기법\n이제 다변량함수(multivariate function), \\(f: \\RR^n \\rightarrow \\RR^m\\)에 대한 미분을 생각해보자.\n먼저 간단한 예제를 고려해 보자. 두 열벡터\n\\[\n\\pmb x=\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}\n\\in \\RR_2, \\quad\n\\pmb y=\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3\n\\end{bmatrix} \\in \\RR^3\n\\]\n를 고려하고 다음과 같은 함수로 두 벡터의 관계가 정의된다고 하자.\n\\[\ny_1 = x_1^2 + x_2, \\quad y_2= \\exp (x_1) + 3 x_2, \\quad y_3 = \\sin(x_1) + x_2^3\n\\tag{D.1}\\]\n위의 관계를 함수 관계 \\(\\pmb f: \\RR^2 \\rightarrow \\RR^3\\) 로 나타내보면\n\\[\n\\pmb f(\\pmb x) =\n\\begin{bmatrix} f_1(\\pmb x) \\\\ f_2 (\\pmb x) \\\\ f_3(\\pmb x) \\end{bmatrix} =\n\\begin{bmatrix} x_1^2 + x_2 \\\\ \\exp (x_1) + 3 x_2 \\\\ \\sin(x_1) + x_2^3 \\end{bmatrix} =\n\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} = \\pmb y\n\\]\n이러한 경우 다변량 함수 \\(\\pmb f\\)를 벡터 \\(\\pmb x\\)로 미분하려면, 즉 미분 표기법을 이용하려면 편미분을 한 결과를 행렬의 형태를 정해야한다.\n\\[  \\pardifftwo{ \\pmb f}{\\pmb x} = (n \\times m)-\\text{matrix} \\quad \\text{ or }  \\quad (m \\times n)-\\text{matrix}? \\]\n일단 각각의 편미분 \\(\\pardifftwo{f_i}{x_j}\\)를 구해야 하며 이는 scalar 미분으로 쉽게 구해진다.\n\\[\n\\begin{aligned}\n\\pardifftwo{  f_1}{ x_1} & = 2x_1, & \\quad \\pardifftwo{  f_2}{ x_1} & = \\exp(x_1), & \\quad\n\\pardifftwo{  f_3}{ x_1} & = \\cos(x_1) \\\\\n\\pardifftwo{  f_1}{ x_2} & = 1,    & \\quad \\pardifftwo{  f_2}{ x_1} & = 3,         & \\quad\n\\pardifftwo{  f_3}{ x_1} & = 3 x_2^2 \\\\\n\\end{aligned}\n\\tag{D.2}\\]\n이제 이제 편미분값들을 행렬의 형태로 정리해보자. 편미분을 행렬에 배치할 떄 다음과 같은 규칙을 사용할 것이다.\n\n행렬의 행은 \\(\\pmb x\\)의 차원 \\(n\\) 과 같다.\n행렬의 열은 \\(\\pmb f\\)의 차원 \\(m\\) 과 같다.\n\n위와 같이 편미분을 배치하는 벡타 미분 표기법을 분모 표기법 (denominator layout)이라고 한다.\n\n\n\n\n\n\n분모 표기법\n\n\n\n\\[\n\\pmb J = \\nabla_x \\pmb x =\\pardifftwo{ \\pmb f}{\\pmb x}  \\equiv \\pardifftwo{ \\pmb f^t}{\\pmb x}\n\\underset{def}{\\equiv} \\begin{bmatrix}\n\\pardifftwo{  f_1}{ x_1} &  \\pardifftwo{  f_2}{ x_1} &  \\pardifftwo{  f_3}{ x_1}  \\\\\n\\pardifftwo{  f_1}{ x_2} &  \\pardifftwo{  f_2}{ x_2}  &  \\pardifftwo{  f_3}{ x_2}\n\\end{bmatrix}\n=  \\begin{bmatrix}\n2x_1 &  \\exp(x_1)  &  \\cos(x_1)   \\\\\n1 &  3  &  3x_2^2\n\\end{bmatrix}\n\\] \\(\\pmb J\\)는 야코비안 행렬(Jacobian matrix)이라고 부른다.\n\n\n이제 이러한 분자표기법의 특별한 결과를 알아보자\n\n\\(f: \\RR^n \\rightarrow \\RR^1\\) 인 경우\n\\(f: \\RR^n \\rightarrow \\RR^1\\) 인 경우 벡터미분 결과를 그레디언트(gradient)라고 부르며 다음과 같이 표기된다.\n\n\\[\n\\nabla_x f = \\pardifftwo{ f}{\\pmb x} =\n\\begin{bmatrix}\n\\pardifftwo{ f}{x_1} \\\\\n\\pardifftwo{ f}{x_2} \\\\\n\\vdots \\\\\n\\pardifftwo{ f}{x_n}\n\\end{bmatrix}\n\\]\n\n\\(f: \\RR^1 \\rightarrow \\RR^m\\) 인 경우\n\n\\[\n\\pardifftwo{\\pmb f}{x} =\n\\begin{bmatrix}\n\\pardifftwo{ f_1}{x} \\\\\n\\pardifftwo{ f_2}{x} \\\\\n\\vdots \\\\\n\\pardifftwo{ f_m}{x}\n\\end{bmatrix}\n\\]\n참고로 식 D.1 에서 정의한 함수 관계를 두 벡터 \\(\\pmb x\\) 와 \\(\\pmb y\\) 의 사상관계로 보면\n\\[ \\pmb f : \\pmb x \\mapsto \\pmb y \\] 다음과 같이 그레디언트 벡터를 표기할 수 있다.\n\\[  \\pardifftwo{ \\pmb f}{\\pmb x} = \\pardifftwo{ \\pmb y}{\\pmb x}\n=\n\\begin{bmatrix}\n\\pardifftwo{  y_1}{ x_1} &  \\pardifftwo{  y_2}{ x_1} &  \\pardifftwo{  y_3}{ x_1}  \\\\\n\\pardifftwo{  y_1}{ x_2} &  \\pardifftwo{  y_2}{ x_2}  &  \\pardifftwo{  y_3}{ x_2}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>벡터 미분</span>"
    ]
  },
  {
    "objectID": "qmd/math_vec_cal.html#함성함수의-미분법",
    "href": "qmd/math_vec_cal.html#함성함수의-미분법",
    "title": "부록 D — 벡터 미분",
    "section": "D.3 함성함수의 미분법",
    "text": "D.3 함성함수의 미분법\n이제 합성함수의 미분법(chain rule)에 대하여 알아보자.\n두 개의 함수\n\\[\n\\pmb g :\\RR^n \\mapsto \\RR^m, \\quad \\pmb f :\\RR^m \\mapsto \\RR^p\n\\] 가 있을 때, \\(\\pmb f\\)와 \\(\\pmb g\\)의 합성함수 \\(\\pmb h\\)는 다음과 같이 정의된다.\n\\[ \\pmb h( \\pmb x) = \\pmb f( \\pmb g( \\pmb x)) = \\pmb f \\circ \\pmb g\\] 즉,\n\\[\n\\pmb h : \\RR^n \\mapsto \\RR^m \\mapsto \\RR^p\n\\]\n이러한 합성함수의 미분은 다음과 같이 계산된다.\n\\[\n\\pardifftwo{ \\pmb h}{\\pmb x} = \\pardifftwo{ \\pmb f \\circ \\pmb g}{\\pmb x} = \\pardifftwo{ \\pmb g}{\\pmb x} \\pardifftwo{ \\pmb f}{\\pmb g}\n\\tag{D.3}\\]\n식 D.3 에서 \\(\\pardifftwo{ \\pmb f}{\\pmb g}\\)는 \\(m \\times p\\) Jacovian 벡터이고\n\\[\n\\pardifftwo{ \\pmb f}{\\pmb g}\n\\begin{bmatrix}\n\\pardifftwo{  f_1}{ g_1} &  \\pardifftwo{  f_2}{ g_1} & \\cdots &  \\pardifftwo{  f_p}{ g_1} \\\\\n\\pardifftwo{  f_1}{ g_2} &  \\pardifftwo{  f_2}{ g_2} & \\cdots &  \\pardifftwo{  f_p}{ g_2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\pardifftwo{  f_1}{ g_m} &  \\pardifftwo{  f_2}{ g_m} & \\cdots &  \\pardifftwo{  f_p}{ g_m} \\\\\n\\end{bmatrix}\n=(m \\times p)\n\\]\n\\(\\pardifftwo{ \\pmb g}{\\pmb x}\\)는 \\(n \\times m\\) Jacovian 벡터이다\n\\[\n\\pardifftwo{ \\pmb g}{\\pmb x} =\n\\begin{bmatrix}\n\\pardifftwo{  g_1}{ x_1} &  \\pardifftwo{  g_2}{ x_1} & \\cdots &  \\pardifftwo{  g_m}{ x_1} \\\\\n\\pardifftwo{  g_1}{ x_2} &  \\pardifftwo{  g_2}{ x_2} & \\cdots &  \\pardifftwo{  g_m}{ x_2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\pardifftwo{  g_1}{ x_n} &  \\pardifftwo{  g_2}{ x_n} & \\cdots &  \\pardifftwo{  g_m}{ x_n} \\\\\n\\end{bmatrix}\n= (n \\times m)\n\\]\n함성함수의 미분 공식을 차원으로 나타내면 다음과 같다.\n\\[\n\\underset{ n \\times p} {\\pardifftwo{ \\pmb h}{\\pmb x}} = \\underset{ n \\times m} {\\pardifftwo{ \\pmb g}{\\pmb x}} \\underset{ m \\times p} {\\pardifftwo{ \\pmb f}{\\pmb g}}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>벡터 미분</span>"
    ]
  },
  {
    "objectID": "qmd/math_vec_cal.html#두-벡터-내적의-미분",
    "href": "qmd/math_vec_cal.html#두-벡터-내적의-미분",
    "title": "부록 D — 벡터 미분",
    "section": "D.4 두 벡터 내적의 미분",
    "text": "D.4 두 벡터 내적의 미분\n\nD.4.1 상수벡터와 변수벡터의 내적\n먼저 상수 벡터 \\(\\pmb a\\)와 변수 벡터 \\(\\pmb x\\)의 내적의 미분을 생각해 보자.\n참고로 다음과 같이 두 벡터의 내적은 스칼라이다.\n\\[ \\pmb a^t \\pmb x = \\pmb x^t \\pmb a = a_1 x_1 + a_2x_2 + \\dots + a_n x_n \\]\n따라서 그레이디언트를 구하는 방법과 같이 결과는 열벡터로 표기된다.\n\\[\n\\pardifftwo{ \\pmb a^t \\pmb x}{\\pmb x} = \\pardifftwo{ \\pmb x^t \\pmb a}{\\pmb x} = \\pmb a =\n\\begin{bmatrix}\na_1 \\\\\na_2\\\\\n\\vdots \\\\\na_n\n\\end{bmatrix}\n\\]\n위의 식에서 상수벡터 \\(\\pmb a\\)는 가 전치로 앞에 나타나는 표현 \\(\\pmb x^t \\pmb a\\) 를 사용하면 결과 벡터 \\(\\pmb a\\)가 열벡터로 그대로 나타나지므로 내적의 미분 표기로 사용할 것이다.\n\\[\n\\pardifftwo{ \\pmb x^t \\pmb a}{\\pmb x} =  \\pardifftwo{  \\pmb x^t}{\\pmb x} \\pmb a =\\pmb I  \\pmb a = \\pmb a\n\\tag{D.4}\\]\n\n\nD.4.2 상수벡터와 함수벡터의 내적\n더 나아가서 상수 벡터 \\(\\pmb a\\)와 함수 벡터 \\(\\pmb f\\)의 내적의 미분도 식 D.4 을 표시하는 동일한 논리로 다음과 같이 표기할 수 있다.\n\\[\n\\pardifftwo{ \\pmb f^t \\pmb a}{\\pmb x} =  \\pardifftwo{  \\pmb f^t}{\\pmb x} \\pmb a\n\\tag{D.5}\\]\n참고로 식 D.5 에서 \\(\\pardifftwo{  \\pmb f}{\\pmb x}\\)는 행벡터가 아닌 행렬로 나타날 수 있다.\n\n\nD.4.3 함수벡터와 함수벡터의 내적\n이제 다음과 같이 같은 공간으로 사상되는 두 함수 \\(\\pmb f\\) 와 \\(\\pmb g\\) 의 내적을 생각해 보자.\n\\[ \\pmb f : \\RR^n \\mapsto \\RR^m, \\quad \\pmb g : \\RR^n \\mapsto \\RR^m\\]\n두 함수의 내적을 미분하는 경우 곱셉 법칙을 적용하여야 하는데 행렬의 곱셉에서는 교환법칙이 성립되지 않으므로 순서에 주의해야 한다.\n내적 \\(\\pmb f^t \\pmb g\\) 를 각각 따로 미분해야 하는데 각 벡터에 대해 따로 미분을 실행해 보자\n\n\\(\\pmb f\\) 를 미분하는 경우 \\(\\pmb g\\) 는 상수 벡터 \\(\\pmb a\\) 로 취급한다. 그리고 식 D.5 를 적용한다.\n\n\\[\n\\pardifftwo{ \\pmb f^t \\pmb g}{\\pmb x} = \\pardifftwo{ \\pmb f^t \\pmb a}{\\pmb x} =\n=  \\pardifftwo{ \\pmb f^t}{\\pmb x} \\pmb a=\n\\pardifftwo{ \\pmb f}{\\pmb x} \\pmb g\n\\tag{D.6}\\]\n\n\\(\\pmb g\\) 를 미분하는 경우 \\(\\pmb f\\) 는 상수 벡터 \\(\\pmb a\\) 로 취급한다. 그리고 식 D.5 를 적용한다.\n\n\\[\n\\pardifftwo{ \\pmb f^t \\pmb g}{\\pmb x} = \\pardifftwo{ \\pmb a^t \\pmb g}{\\pmb x} =\n\\pardifftwo{ \\pmb g^t \\pmb a}{\\pmb x} =\n\\pardifftwo{ \\pmb g^t}{\\pmb x}  \\pmb a =\n\\pardifftwo{ \\pmb g}{\\pmb x} \\pmb f\n\\tag{D.7}\\]\n이제 위의 두 결과 식 D.6 과 식 D.7 를 합치면 다음과 같은 최종적인 결과를 얻을 수 있다.\n\\[\n\\pardifftwo{ \\pmb f^t \\pmb g}{\\pmb x} =  \\pardifftwo{ \\pmb f}{\\pmb x} \\pmb g +  \\pardifftwo{ \\pmb g}{\\pmb x} \\pmb f\n\\tag{D.8}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>벡터 미분</span>"
    ]
  },
  {
    "objectID": "qmd/math_vec_cal.html#벡터-미분의-응용",
    "href": "qmd/math_vec_cal.html#벡터-미분의-응용",
    "title": "부록 D — 벡터 미분",
    "section": "D.5 벡터 미분의 응용",
    "text": "D.5 벡터 미분의 응용\n\nD.5.1 선형사상의 미분상\n이제 앞에서 배운 벡터의 미분을 이용하여 유용한 응용 공식을 유도해보자.\n먼저 선형변환 \\(\\pmb y = \\pmb A \\pmb x\\) 를 생각해 보자. 이때 (\\(M \\times N\\))-\\(\\pmb A\\)는 상수 행렬이다. 이때 \\(\\pmb y\\)를 \\(\\pmb x\\)로 미분하면 다음과 같다.\n먼저 행렬 \\(\\pmb A\\)의 \\(i\\) 번째 행을 \\(\\pmb a_i^t\\)라고 하면\n\\[\n\\pmb A = \\begin{bmatrix}\nA_{11} & A_{12} & \\dots & A_{1N} \\\\\nA_{21} & A_{22} & \\dots & A_{2N} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{M1} & A_{M2} & \\dots & A_{MN} \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\pmb a_1^t  \\\\\n\\pmb a_2^t  \\\\\n\\vdots \\\\\n\\pmb a_M^t  \\\\\n\\end{bmatrix}\n\\]\n선형변환 \\(\\pmb f(\\pmb x) = \\pmb A \\pmb x\\) 로 정의하면 다음과 같이 나타낼 수 있다.\n\\[\n\\pmb A \\pmb x=\n\\begin{bmatrix}\n\\pmb a_1^t \\pmb x \\\\\n\\pmb a_2^t \\pmb x \\\\\n\\vdots \\\\\n\\pmb a_M^t \\pmb x \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nf_1(\\pmb x) \\\\\nf_2(\\pmb x) \\\\\n\\vdots \\\\\nf_M(\\pmb x) \\\\\n\\end{bmatrix}\n= \\pmb f(\\pmb x)\n\\]\n따라서\n\\[\n\\pardifftwo{\\pmb A \\pmb x}{\\pmb x}   = \\pardifftwo{\\pmb f (\\pmb x)}{\\pmb x} =\n\\begin{bmatrix}\n\\pardifftwo{f_1}{x_1} & \\pardifftwo{f_2}{x_1} & \\dots & \\pardifftwo{f_M}{x_1} \\\\\n\\pardifftwo{f_1}{x_2} & \\pardifftwo{f_2}{x_2} & \\dots & \\pardifftwo{f_M}{x_2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\pardifftwo{f_1}{x_N} & \\pardifftwo{f_2}{x_N} & \\dots & \\pardifftwo{f_M}{x_N} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nA_{11} & A_{21} & \\dots & A_{M1} \\\\\nA_{12} & A_{22} & \\dots & A_{M2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{1N} & A_{2N} & \\dots & A_{MN} \\\\\n\\end{bmatrix} = \\pmb A^t\n\\] 따라서 선형사상의 미분은 선형변환 행렬의 전치이다.\n\\[ \\pardifftwo{\\pmb A \\pmb x}{\\pmb x}  = \\pmb A^t  \\tag{D.9}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>벡터 미분</span>"
    ]
  },
  {
    "objectID": "qmd/multivar.html",
    "href": "qmd/multivar.html",
    "title": "부록 E — 다변량 확률변수의 성질",
    "section": "",
    "text": "E.1 일변량분포\n일변량 확률변수 \\(X\\)가 확률밀도함수 \\(f(x)\\)를 가지는 분포를 따를때 기대값과 분산은 다음과 같이 정의된다.\n\\[ E(X) = \\int x f(x)  dx = \\mu, \\quad V(X) = E[ X-E(X)]^2=\\int (x-\\mu)^2 f(x) dx =\\sigma^2 \\]\n새로운 확률변수 \\(Y\\)가 확률변수 \\(X\\)의 선형변환으로 표시된다면 (\\(a\\)와 \\(b\\)는 실수)\n\\[ Y = aX+b\\]\n그 기대값(평균)과 분산은 다음과 같이 계산된다.\n\\[\n\\begin{aligned}\nE(Y) &= E(aX+b) \\\\\n&= \\int (ax+b) f(x) dx \\\\\n&= a \\int x f(x) dx + b \\\\\n&= a E(X) + b\\\\\n&= a \\mu + b \\\\\nV(Y) &= Var(aX+b) \\\\\n&= E[aX+b -E(aX+b)]^2 \\\\\n&= E[a(X-\\mu)]^2 \\\\\n&= a^2 E(X-\\mu)^2\\\\\n&= a^2 \\sigma^2\n\\end{aligned}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>다변량 확률변수의 성질</span>"
    ]
  },
  {
    "objectID": "qmd/multivar.html#확률벡터와-분포",
    "href": "qmd/multivar.html#확률벡터와-분포",
    "title": "부록 E — 다변량 확률변수의 성질",
    "section": "E.2 확률벡터와 분포",
    "text": "E.2 확률벡터와 분포\n확률벡터 \\(\\pmb X\\)가 \\(p\\) 차원의 다변량분포를 따른다고 하고 결합확률밀도함수 \\(f(\\pmb x) =f(x_1,x_2,\\dots,x_p)\\)를 를 가진다고 하자.\n\\[\n\\pmb X =\n  \\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n..  \\\\\nX_p\n\\end{bmatrix}\n\\]\n다변량 확률벡터의 기대값(평균벡터)과 공분산(행렬)은 다음과 같이 계산된다.\n\\[\n\\pmb E(\\pmb X) =\n  \\begin{bmatrix}\nE(X_1) \\\\\nE(X_2) \\\\\nE(X_3) \\\\\n..  \\\\\nE(X_p)\n\\end{bmatrix}\n=\n  \\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n..  \\\\\n\\mu_p\n\\end{bmatrix}\n=\\pmb \\mu\n\\]\n\\[\nV(\\pmb X) =Cov(\\pmb X) = E (\\pmb X-\\pmb \\mu) (\\pmb X-\\pmb \\mu)^t\n=\n  \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p} \\\\\n\\sigma_{12} & \\sigma_{22} & \\dots & \\sigma_{2p} \\\\\n& \\dots & \\dots & \\\\\n\\sigma_{1p} & \\sigma_{2p} & \\dots & \\sigma_{pp} \\\\\n\\end{bmatrix}\n= \\pmb \\Sigma\n\\]\n여기서 \\(\\sigma_{ii}=V(X_i)\\), \\(\\sigma_{ij} = Cov(X_i, X_j)=Cov(X_j, X_i)\\)이다. 따라서 공분산 행렬 \\(\\pmb \\Sigma\\)는 대칭행렬(symmetric matrix)이다. 다음 공식은 유용한 공식이다.\n\\[ \\pmb \\Sigma = E (\\pmb X-\\pmb \\mu) (\\pmb X-\\pmb \\mu)^t  = E(\\pmb X \\pmb X^t)-\\pmb \\mu \\pmb \\mu^t \\]\n두 확률변수의 상관계수 \\(\\rho_{ij}\\)는 다음과 같이 정의된다.\n\\[ \\rho_{ij} = \\frac{Cov(X_i, X_j)}{ \\sqrt{V(X_i) V(X_j)}} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\n  \\sigma_{jj}}} \\]\n새로운 확률벡터 \\(\\pmb Y\\)가 확률벡터 \\(\\pmb X\\) 의 선형변환라고 하자.\n\\[ \\pmb Y = \\pmb A  \\pmb X + \\pmb b \\]\n단 여기서 \\(\\pmb A = \\{ a_{ij} \\}\\)는 \\(p \\times p\\) 실수 행렬이고 \\(\\pmb b =(b_1 b_2 \\dots b_p)^t\\)는 \\(p \\times 1\\) 실수 벡터이다.\n확률벡터 \\(\\pmb Y\\)의 기대값(평균벡터)과 공분산은 다음과 같이 계산된다.\n\\[\n\\begin{aligned}\nE(\\pmb Y ) &= E(\\pmb A \\pmb X+ \\pmb b) \\\\\n&= \\pmb A E(\\pmb X)+ \\pmb b \\\\\n&= \\pmb A \\pmb \\mu+ \\pmb b \\\\\nV(\\pmb Y) &= Var(\\pmb A \\pmb X+ \\pmb b) \\\\\n&= E[\\pmb A \\pmb X+ \\pmb b -E(\\pmb A \\pmb X+ \\pmb b)] [\\pmb A \\pmb X+ \\pmb b -E(\\pmb A \\pmb X+ \\pmb b)]^t \\\\\n&= E[\\pmb A \\pmb X -  \\pmb A \\pmb \\mu] [\\pmb A \\pmb X -  \\pmb A \\pmb \\mu]^t \\\\\n&= E[\\pmb A (\\pmb X - \\pmb \\mu)] [\\pmb A (\\pmb X - \\pmb \\mu)]^t \\\\\n&= \\pmb A E [(\\pmb X - \\pmb \\mu) (\\pmb X - \\pmb \\mu)^t] \\pmb A^t \\\\\n&= \\pmb A \\pmb \\Sigma \\pmb A^t\n\\end{aligned}\n\\]\n만약 표본 \\(\\pmb X_i, \\pmb X_2, \\dots, \\pmb X_n\\) 이 독립적으로 평균이 \\(\\pmb \\mu\\) 이고 공분산이 \\(\\pmb \\Sigma\\) 인 분포에서 추출되었다면 표본의 평균벡터 \\(\\bar {\\pmb  X}\\) 는 평균이 \\(\\pmb \\mu\\) 이고 공분산이 \\(\\frac{1}{n}\\pmb \\Sigma\\) 인 분포를 따른다.\n\\[\n\\bar {\\pmb X} =\n  \\begin{bmatrix}\n\\sum_{i=1}^n X_{i1} / n  \\\\\n\\sum_{i=1}^n X_{i2} / n \\\\\n\\sum_{i=1}^n X_{i3} / n \\\\\n..  \\\\\n\\sum_{i=1}^n X_{ip} / n\n\\end{bmatrix}\n\\]\n여기서 \\(X_{ij}\\) 는 \\(i\\)번째 표본벡터 \\(\\pmb X_i =(X_{i1} X_{i2} \\dots X_{ip})^t\\)의 \\(j\\)번째 확률변수이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>다변량 확률변수의 성질</span>"
    ]
  },
  {
    "objectID": "qmd/multivar.html#다변량-정규분포",
    "href": "qmd/multivar.html#다변량-정규분포",
    "title": "부록 E — 다변량 확률변수의 성질",
    "section": "E.3 다변량 정규분포",
    "text": "E.3 다변량 정규분포\n일변량 확률변수 \\(X\\)가 평균이 \\(\\mu\\) 이고 분산이 \\(\\sigma^2\\)인 정규분포를 따른다면 다음과 같이 나타내고 \\[ X \\sim N(\\mu, \\sigma^2 ) \\] 확률밀도함수 \\(f(x)\\) 는 다음과 갇이 주어진다.\n\\[ f(x) = (2 \\pi \\sigma^2)^{-1/2} \\exp \\left ( - \\frac{(x-\\mu)^2}{2} \\right ) \\]\n\\(p\\)-차원 확률벡터 \\(\\pmb X\\)가 평균이 \\(\\pmb \\mu\\) 이고 공분산이 \\(\\pmb \\Sigma\\)인 다변량 정규분포를 따른다면 다음과 같이 나타내고 \\[ \\pmb X \\sim N_p(\\pmb \\mu, \\pmb \\Sigma ) \\] 확률밀도함수 \\(f(\\pmb x)\\) 는 다음과 갇이 주어진다.\n\\[ f(\\pmb x) = (2 \\pi)^{-p/2} | \\pmb \\Sigma|^{-1/2}\n   \\exp \\left ( - \\frac{(\\pmb x-\\pmb \\mu) \\pmb \\Sigma^{-1}(\\pmb x-\\pmb \\mu)^t}{2} \\right ) \\]\n다변량 정규분포 \\(N(\\pmb \\mu, \\pmb \\Sigma)\\)를 따르는 확률벡터 \\(\\pmb X\\)를 다음과 같이 두 부분으로 나누면\n\\[\n  \\pmb X =\n    \\begin{bmatrix}\n  \\pmb X_1 \\\\\n  \\pmb X_2\n  \\end{bmatrix}, \\quad\n  \\pmb X_1 =\n    \\begin{bmatrix}\n  \\pmb X_{11} \\\\\n  \\pmb X_{12} \\\\\n  \\pmb \\vdots \\\\\n  \\pmb X_{1p}\n  \\end{bmatrix}, \\quad\n  \\pmb X_2=\n    \\begin{bmatrix}\n  \\pmb X_{21} \\\\\n  \\pmb X_{22} \\\\\n  \\pmb \\vdots \\\\\n  \\pmb X_{2q}\n  \\end{bmatrix}\n  \\]\n각각 다변량 정규분포를 따르고 다음과 같이 나타낼 수 있다.\n\\[\n  \\begin{bmatrix}\n  E(\\pmb X_1) \\\\\n  E(\\pmb X_2)\n  \\end{bmatrix}\n  =\n    \\begin{bmatrix}\n  \\pmb \\mu_1 \\\\\n  \\pmb \\mu_2\n  \\end{bmatrix}\n  , \\quad\n  \\begin{bmatrix}\n  V(\\pmb X_1) & Cov(\\pmb X_1, X_2) \\\\\n  Cov(\\pmb X_2 X_1) & V(\\pmb X_2)\n  \\end{bmatrix}\n  =\n    \\begin{bmatrix}\n  \\pmb \\Sigma_{11} & \\pmb \\Sigma_{12} \\\\\n  \\pmb \\Sigma^t_{12} & \\pmb \\Sigma_{22}\n  \\end{bmatrix}\n  \\]\n\\[  \\pmb X =\n    \\begin{bmatrix}\n  \\pmb X_1 \\\\\n  \\pmb X_2\n  \\end{bmatrix}\n  \\sim\n  N_{p+q} \\left (\n    \\begin{bmatrix}\n    \\pmb \\mu_1 \\\\\n    \\pmb \\mu_2\n    \\end{bmatrix}\n    ,\\begin{bmatrix}\n    \\pmb \\Sigma_{11} & \\Sigma_{12} \\\\\n    \\pmb \\Sigma^t_{12} & \\Sigma_{22}\n    \\end{bmatrix}\n    \\right )\n  \\]\n확률벡터 \\(\\pmb X_2 = \\pmb x_2\\)가 주어진 경우 \\(\\pmb X_1\\)의 조건부 분포는 \\(p\\)-차원 다변량 정규분포를 따르고 평균과 공분산은 다음과 같다.\n\\[\n  E(\\pmb X_1 | \\pmb X_2 = \\pmb x_2 ) = \\pmb \\mu_1 + \\pmb \\Sigma_{12} \\pmb \\Sigma^{-1}_{22} (\\pmb \\mu_2 - \\pmb x_2), \\quad\n  V(\\pmb X_1 | \\pmb X_2 = \\pmb x_2 )  = \\pmb \\Sigma_{11} -\\pmb \\Sigma_{12} \\pmb \\Sigma^{-1}_{22} \\pmb \\Sigma^t_{12}\n  \\]\n예를 들어 \\(2\\)-차원 확률벡터 \\(\\pmb X=(X_1, X_2)^t\\)가 평균이 \\(\\pmb \\mu=(\\mu_1,\\mu_2)^t\\) 이고 공분산 \\(\\pmb \\Sigma\\)가 다음과 같이 주어진\n\\[\n\\pmb \\Sigma =\n  \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_{22}\n\\end{bmatrix}\n\\]\n이변량 정규분포를 따른다면 확률밀도함수 \\(f(\\pmb x)\\)에서 \\(\\exp\\)함수의 인자는 다음과 같이 주어진다. \\[\n\\begin{aligned}\n&(\\pmb x-\\pmb \\mu) \\pmb \\Sigma^{-1}(\\pmb x-\\pmb \\mu)^t\n= \\\\\n&-\\frac{1}{2 (1-\\rho^2)}\n\\left [\n  \\left ( \\frac{(x_1-\\mu_1)^2}{\\sigma_{11}} \\right )\n  +\\left ( \\frac{(x_2-\\mu_2)^2}{\\sigma_{22}} \\right )\n  -2 \\rho \\left ( \\frac{(x_1-\\mu_1)}{\\sqrt{\\sigma_{11}}} \\right )\n  \\left ( \\frac{(x_2-\\mu_2)}{\\sqrt{\\sigma_{22}}} \\right )\n  \\right ]\n\\end{aligned}\n\\]\n그리고 \\(p=2\\)인 경우 확률밀도함수의 상수부분은 다음과 같이 주어진다.\n\\[ (2 \\pi)^{-p/2} | \\pmb \\Sigma|^{-1/2} = \\frac{1}{ 2 \\pi \\sqrt{\\sigma_{11} \\sigma_{22} (1-\\rho^2)}} \\]\n여기서 \\(\\rho = \\sigma_{12} / \\sqrt{\\sigma_{11} \\sigma_{22}}\\)\n만약 \\(X_2 = x_2\\)가 주어졌을 때 \\(X_1\\)의 조건부 분포는 정규분포이고 평균과 분산은 다음과 같이 주어진다.\n\\[\n  E( X_1 |  X_2 =  x_2 ) =  \\mu_1 +  \\frac{\\sigma_{12}}{\\sigma_{22}} ( \\mu_2 -  x_2)  = \\mu_1 +  \\rho \\frac{\\sqrt{\\sigma_{11}}}{\\sqrt{\\sigma_{22}}} ( \\mu_2 -  x_2) \\]\n\\[\n  V( X_1 |  X_2 =  x_2 )  =  \\sigma_{11} - \\frac{\\sigma^2_{12}}{\\sigma_{22}}  = \\sigma_{11}(1-\\rho^2)\n\\]\n다변량 정규분포에서 공분산이 0인 두 확률 변수는 독립이다. \\[ \\sigma_{ij} = 0 \\leftrightarrow X_i \\text{ and } X_j \\text{ are independent} \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>다변량 확률변수의 성질</span>"
    ]
  },
  {
    "objectID": "qmd/multivar.html#표준정규분포로의-변환",
    "href": "qmd/multivar.html#표준정규분포로의-변환",
    "title": "부록 E — 다변량 확률변수의 성질",
    "section": "E.4 표준정규분포로의 변환",
    "text": "E.4 표준정규분포로의 변환\n일변량 확률변수 \\(X\\)가 평균이 \\(\\mu\\) 이고 분산이 \\(\\sigma^2\\)인 경우 다음과 같은 선형변환을 고려하면.\n\\[ Z = \\frac{X - \\mu}{\\sigma} = (\\sigma^2)^{-1/2} (X-\\mu) \\] 확률변수 \\(Z\\) 는 평균이 \\(0\\) 이고 분산이 \\(1\\)인 분포를 따른다.\n\\(p\\)차원 확률벡터 \\(\\pmb X\\) 가 평균이 \\(\\pmb \\mu\\) 이고 공분산이 \\(\\pmb \\Sigma\\)인 분포를 가진다고 가정하자. 공분산 행렬 \\(\\pmb \\Sigma\\)는 양정치 행렬(positive definite matrix)이며 다음과 같은 행렬의 분해가 가능하다.\n\\[ \\Sigma = \\pmb C \\pmb C^t \\]\n여기서 \\(\\pmb C\\) 는 정칙행렬이며 역행렬 \\(\\pmb C^{-1}\\)가 존재한다. 위와 같은 행렬의 분해는 스펙트럴 분해(spectral decomposition)을 이용하여 구할 수 있다. 공분산 행렬 \\(\\pmb \\Sigma\\)는 양정치 행렬이므로 고유치(eigen value) \\((\\lambda_1, \\lambda_2,\\dots, \\lambda_p)\\)가 모두 양수이고 정규직교 고유벡터(orthonormal eigen vector)의 행렬 \\(\\pmb P\\)을 이용하여 다음과 같은 분해가 가능하다.\n\\[ \\Sigma = \\pmb P \\pmb \\Lambda \\pmb P^t = \\pmb P \\pmb \\Lambda^{1/2} \\Lambda^{1/2} \\pmb P^t \\]\n여기서 \\(\\pmb \\Lambda\\)는 고유치 \\((\\lambda_1, \\lambda_2,\\dots, \\lambda_p)\\)를 대각원소로 가지는 대각행렬이며 \\(\\pmb \\Lambda^{1/2}\\)는 고유치의 제곱근을 대각원소로 가지는 대각행렬이다. 따라서 \\(\\pmb C = \\pmb P \\pmb \\Lambda^{1/2}\\)로 하면 위와 같은 행렬의 분해가 가능하다. 정규직교 고유벡터(orthonormal eigen vector)의 행렬 \\(\\pmb P\\)는 직교행렬이므로\n\\[ \\pmb C^{-1} =  (\\pmb P \\pmb \\Lambda^{1/2})^{-1} = \\pmb \\Lambda^{-1/2} \\pmb P^t \\]\n\\(p\\)차원 확률벡터 \\(\\pmb X\\)의 다음과 같은 선형변환을 고려하면. \\[ \\pmb Z = \\pmb C^{-1} ( \\pmb X- \\pmb \\mu) = \\pmb \\Lambda^{-1/2} \\pmb P^t ( \\pmb X- \\pmb \\mu)  \\] 확률벡터 \\(\\pmb Z\\) 는 평균이 \\(\\pmb 0\\) 이고 공분산이 \\(\\pmb I\\)인 분포를 따른다 (why?).\n확률벡터 \\(\\pmb X\\)가 정규분포를 따른다면 선형변환한 확률벡터 \\(\\pmb Z\\)도 정규분포를 따른다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>다변량 확률변수의 성질</span>"
    ]
  },
  {
    "objectID": "qmd/multivar.html#예제",
    "href": "qmd/multivar.html#예제",
    "title": "부록 E — 다변량 확률변수의 성질",
    "section": "E.5 예제",
    "text": "E.5 예제\n예를 들어 이변량확률벡터 \\(\\pmb X\\)가 다음과 같은 평균벡터와 공분산을 가진 정규분포를 따른다고 하자\n\\[\n\\pmb \\mu =\n  \\begin{bmatrix}\n1\\\\\n2\n\\end{bmatrix}\n\\quad\n\\pmb \\Sigma =\n  \\begin{bmatrix}\n2 & 1\\\\\n1 & 2\n\\end{bmatrix}\n\\]\n공분산행렬 \\(\\pmb \\sigma\\)의 고유치는 \\(|\\pmb \\sigma -\\lambda \\pmb I|=0\\)의 방정식을 풀어 구할 수 있다.\n\\[\n|\\pmb \\sigma -\\lambda \\pmb I|  =\n  \\begin{bmatrix}\n2-\\lambda & 1\\\\\n1 & 2-\\lambda\n\\end{bmatrix}\n= \\lambda^2 -4 \\lambda +3=0\n\\]\n방정식을 풀면 고유치는 \\((\\lambda_1, \\lambda_2) = (3,1)\\)이다. 각 고유치에 대한 고유벡터 \\(\\pmb p=(p_1, p_2)^t\\)는 \\(\\pmb \\Sigma \\pmb p = \\lambda \\pmb p\\) 으로 구할 수 있다. 각 고유치에 대하여 방정식을 구하면 다음 두 개의 방정식을 얻을 수 있다.\n\\[\np_1 - p_2 = 1 \\text{ and } p_1 + p_2 = 0\n\\]\n정규직교 벡터의 조건을 만족 시키기 위해서 \\(p^2_1 + p^2_2=1\\)의 조건을 적용하면 다음과 같은 정규직교 고유행렬을 얻을 수 있다.\n\\[\n\\pmb P =\n  \\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\\\\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\]\n또한\n\\[\n\\pmb \\Lambda =\n  \\begin{bmatrix}\n3 & 0\\\\\n0 & 1\n\\end{bmatrix}\n\\quad\n\\pmb \\Lambda^{1/2} =\n  \\begin{bmatrix}\n\\sqrt{3} & 0\\\\\n0 & 1\n\\end{bmatrix}\n\\]\n따라서 \\(C^{-1} =  \\Lambda^{-1/2} \\pmb P^t\\) 이며\n\\[\n\\pmb C^{-1} =\n  \\pmb \\Lambda^{-1/2} \\pmb P^t =\n  \\begin{bmatrix}\n\\frac{1}{\\sqrt{3}} & 0\\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n=\n  \\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{6}}\\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>다변량 확률변수의 성질</span>"
    ]
  },
  {
    "objectID": "qmd/quadratic.html",
    "href": "qmd/quadratic.html",
    "title": "부록 F — 이차형식과 제곱합의 분포",
    "section": "",
    "text": "F.1 이차형식\n\\(n\\)-차원 벡터 \\({\\pmb  x}^t=[x_1,x_2,\\dots,x_n]\\)과 대칭행렬 \\(\\pmb  A\\)에 대하여 이차형식(quadratic form)은 다음과 같이 정의된다.\n\\[\nQ_A(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x =\\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j\n\\tag{F.1}\\]\n이차형식의 정의에서 반드시 행렬 \\(\\pmb  A\\)를 대칭행렬로 정의하지 않아도 되지만 임의의 행렬에 대하여 이차형식의 값이 동일한 대칭행렬이 존재하기 때문에 정의에서 이차형식으로 국한하는 것이 일반적이다.\n정칙행렬 \\(\\pmb  B\\)에 대하여 다음과 같은 선형변환을 고려하자.\n\\[   \\pmb  x = \\pmb  B \\pmb  y \\quad \\text{ or } \\quad \\pmb  y = \\pmb  {B}^{-1} \\pmb  x \\]\n벡터 \\(\\pmb  x\\)로 정의된 이차형식은 벡터 \\(\\pmb  y\\)의 형태로 다음과 같이 변환할 수 있다.\n\\[\nQ(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x = \\pmb  y^t \\pmb  B^t \\pmb  A \\pmb  B \\pmb  y =Q^*(\\pmb  y)\n\\]\n이차형식의 성질은 정칙 선형변환에서 유지된다. 즉 행렬 \\(\\pmb  A\\)가 양(반)정치 행렬이고 행렬 \\(\\pmb  B\\)가 정칙행렬이면 행렬 \\(\\pmb  B^t \\pmb  A \\pmb  B\\)도 양(반)정치 행렬이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>이차형식과 제곱합의 분포</span>"
    ]
  },
  {
    "objectID": "qmd/quadratic.html#이차형식",
    "href": "qmd/quadratic.html#이차형식",
    "title": "부록 F — 이차형식과 제곱합의 분포",
    "section": "",
    "text": "정의 F.1 (양정치 행렬) 이차형식 \\(Q_A(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x\\)가 영벡터가 아닌 모든 벡터 \\(\\pmb  x\\)에 대하여 0 보다 크면, 즉\n\\[ \\pmb  x^t \\pmb  A \\pmb  x  &gt;0  \\quad \\text{ for all } \\quad \\pmb x \\in \\RR^n\\]\n\\(\\pmb  A\\)를 양정치(positive definite)라고 부른다.\n만약 이차형식 \\(Q_A(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x\\)가 영벡터가 아닌 모든 벡터 \\(\\pmb  x\\)에 대하여 0 보다 크거나 같다면\n\\[ \\pmb  x^t \\pmb  A \\pmb  x  \\ge 0 \\quad \\text{ for all } \\quad \\pmb x \\in \\RR^n\\]\n\\(\\pmb  A\\)를 양반정치(positive semi-definite)라고 부른다.\n\\(\\blacksquare\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>이차형식과 제곱합의 분포</span>"
    ]
  },
  {
    "objectID": "qmd/quadratic.html#대칭행렬의-대각화",
    "href": "qmd/quadratic.html#대칭행렬의-대각화",
    "title": "부록 F — 이차형식과 제곱합의 분포",
    "section": "F.2 대칭행렬의 대각화",
    "text": "F.2 대칭행렬의 대각화\n\\(n\\)차원 대칭행렬 \\(\\pmb  A\\) 에 대하여 직교행렬 \\(\\pmb  P\\)가 존재하여 다음과 같은 분해가 가능하다.\n\\[\n\\pmb  P^t \\pmb  A \\pmb  P = \\pmb  \\Lambda = diag(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)\n\\tag{F.2}\\]\n식 F.2 의 분해에서 \\(\\lambda_i\\)는 행렬 \\(\\pmb  A\\)의 고유치이며 행렬 \\(\\pmb  P\\)의 \\(i\\) 번째 열은 대응하는 고유벡터 \\(\\pmb  p_i\\) 로 구성되어 있다.\n\\[\n\\pmb  P = [ \\pmb  p_1~~ \\pmb  p_2 ~~ \\dots ~~ \\pmb  p_n ]\n\\] 이제 위의 분해를 증명해 보자. 고유치 \\(\\lambda_i\\) 와 대응하는 고유벡터 \\(\\pmb  p_i\\)의 정의에 따라서 다음과 같은 \\(n\\)개의 식을 얻을 수 있고\n\\[ \\pmb  A \\pmb  p_i = \\lambda_i \\pmb  p_i , \\quad i=1,2,3\\dots, n \\]\n위의 식을을 합쳐서 표기하면 다음과 같은 식을 얻으며 이는 식 F.2 를 의미한다.\n\\[ \\pmb  A \\pmb  P = \\pmb  P \\pmb  \\Lambda \\]\n식 F.2 를 다시 쓰면 다음과 같은 스펙트럴 분해(spectral decomposition)를 얻는다.\n\\[\n\\pmb  A  = \\pmb  P \\pmb  \\Lambda \\pmb  P^t  = \\sum_{i=1}^n \\lambda_i \\pmb  p_i \\pmb  {p}_i^t\n\\tag{F.3}\\]\n참고로 대각합과 행렬식에 대한 고유치위 관계를 나타내는 다음의 유용한 두 식을 반드시 기억하자.\n\\[ tr(\\pmb  A) = \\sum_i \\lambda_i ,\\quad |\\pmb  A| = \\prod_i \\lambda_i \\]\n대칭행렬의 분해 식 F.2 를 이용하면 다음과 같은 이차형식의 분해를 얻을 수 있다.\n\\[\nQ(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x = \\pmb  x^t \\pmb  P \\pmb  \\Lambda \\pmb  P^t \\pmb  x = \\pmb  y^t \\pmb  \\Lambda \\pmb  y= \\sum_{i=1}^n \\lambda_i y_i^2\n\\tag{F.4}\\]\n위의 식에서\n\\[ \\pmb  y = \\pmb  P^t \\pmb  x \\]\n이차형식의 분해식 식 F.4 를 보면 행렬 \\(\\pmb  A\\)의 모든 고유치가 0보다 크면 양정치 임을 알 수 있다. 또한 모든 고유치가 0보다 크거나 같으면 양반정치 임을 알 수 있다.\n또한 \\(rank(\\pmb  A) = rank(\\pmb  \\Lambda)\\)이며 이는 0이 아닌 고유치의 개수가 행렬 \\(\\pmb  A\\)의 계수(rank)임을 알 수 있다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>이차형식과 제곱합의 분포</span>"
    ]
  },
  {
    "objectID": "qmd/quadratic.html#멱등행렬",
    "href": "qmd/quadratic.html#멱등행렬",
    "title": "부록 F — 이차형식과 제곱합의 분포",
    "section": "F.3 멱등행렬",
    "text": "F.3 멱등행렬\n\\(n\\)-차원 행렬 \\(\\pmb  A\\) 가 다음과 같은 성질을 가지면 멱등행렬(idenpotent matrix)라고 부른다.\n\\[ \\pmb  A^2 = \\pmb  A \\pmb  A = \\pmb  A \\]\n멱등행렬은 다음과 같은 성질을 가지고 있다.\n\n멱등행렬의 고유치는 0 또는 1이다.\n멱등행렬은 대각합이 계수와 같다.\n\n\\[ tr(\\pmb  A) =rank(\\pmb  A) \\]\n\n멱등행렬은 양반정치 행렬이다.\n\\(\\pmb  A\\) 멱등행렬이면 \\(\\pmb  I - \\pmb  A\\)도 멱등행렬이다.\n\n특별히 대칭인 멱등행렬을 사영행렬(또는 투영행렬, projection matrix)라고 부른다.\n최소제곱법에서 식 B.11 에서 나타난 행렬 \\(\\pmb  H = \\pmb  X (\\pmb  X^t \\pmb  X)^{-1} \\pmb  X^t\\)는 멱등행렬이며 따라서 사영행렬이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>이차형식과 제곱합의 분포</span>"
    ]
  },
  {
    "objectID": "qmd/quadratic.html#이차형식의-분포",
    "href": "qmd/quadratic.html#이차형식의-분포",
    "title": "부록 F — 이차형식과 제곱합의 분포",
    "section": "F.4 이차형식의 분포",
    "text": "F.4 이차형식의 분포\n\nF.4.1 카이제곱 분포\n만약 확률변수 \\(x\\) 이 표준 정규분포 \\(N(0,1)\\) 을 따른다면 \\(y=x^2\\) 은 자유도가 1인 카이제곱 분포 \\(\\chi^2_1\\)를 따른다. 더 나아가 \\(n\\) 개의 확률변수 \\(x_1,x_2,\\dots,x_n\\) 이 서로 독립이고 표준 정규분포 \\(N(0,1)\\) 을 따른다면 제곱합 \\(v=x_1^2 +x_2^2 +\\cdots + x_n^2\\)은 자유도가 n인 카이제곱 분포 \\(\\chi^2_n\\)를 따른다.\n이렇게 카이제곱 분포는 표준 정규분포를 따르는 서로 독립인 확률변수들의 제곱값에 대한 분포이다.\n\n\nF.4.2 비중심 카이제곱 분포\n만약 확률변수 \\(x\\)가 \\(N(\\mu, 1)\\)을 따른다면 \\(v=x^2\\)은 자유도가 1인 비중심 카이제곱 분포, \\(\\chi^2_1(\\lambda^2)\\) 를 따른다. 여기서 비중심 카이제곱 분포의 자유도는 1이고 비중심모수 \\(\\lambda^2 = \\mu^2\\)으로 주어진다.\n이제 \\(n\\)개의 서로 독립인 확률 변수 \\(x_1,x_2,\\cdots,x_n\\)이 각각 \\(N(\\mu_i, 1)\\)을 따른다면 \\(v=x_1^2+\\dots+x_n^2\\)은 자유도가 \\(n\\)이고 비중심 모수가 \\(\\lambda^2 = \\sum_{i=1}^n \\mu_i^2\\)인 비중심 카이제곱 분포, \\(\\chi^2_n(\\lambda^2)\\) 를 따른다.\n참고로 확률변수 \\(x\\)가 \\(N(0, 1)\\)을 따른다면 \\(v=x^2\\)은 중심 카이제곱 분포, \\(\\chi^2_1\\) 를 따르며 이때는 비중심모수가 \\(\\lambda^2=0\\)이다. 즉, 비중심모수가 0인 비중심 카이제곱 분포(non-central chi square distribution)를 중심 카이제곱 분포(central chi square distribution)라고 한다. 또한 중심 카이제곱 분포는 중심을 빼고 카이제곱 분포라고 부른다.\n\n\nF.4.3 이차형식의 분포\n\\(n\\)개의 서로 독립인 확률 변수 \\(x_1,x_2,\\cdots,x_n\\)이 각각 \\(N(\\mu_i, \\sigma^2)\\)를 따른다면 \\(n\\)-차원의 확률벡터 \\(\\pmb x\\)는 다음과 같은 다변량 정규분포를 따른다고 할 수 있다.\n\\[ \\pmb x \\sim N(\\pmb \\mu, \\sigma^2 \\pmb I) \\]\n위에서 \\(\\pmb \\mu^t =(\\mu_1, \\mu_2, \\dots, \\mu_n)\\)\n이제 이차형식의 분포에 대하여 논의하자.\n\n정리 F.1 (이차형식의 분포) \\(n\\)-차원의 확률벡터 \\(\\pmb x\\)가 \\(N(\\pmb \\mu, \\sigma^2 \\pmb I)\\)를 따른다면 이차형식 \\(Q=\\pmb x^t \\pmb A \\pmb x\\)의 분포는 다음과 같다.\n\\[\nV = \\frac{Q}{\\sigma^2} = \\frac {\\pmb x^t \\pmb A \\pmb x}{\\sigma^2} ~~ \\equiv_d ~~ \\sum_{i=1}^n \\lambda_i \\frac{u^2_i}{\\sigma^2}\n\\tag{F.5}\\]\n위의 식에서 \\(x \\equiv_d y\\) 는 확률변수 \\(x\\)와 \\(y\\)가 동일한 분포를 가진다는 것을 의미한다.\n식 F.5 에서 행렬 \\(\\pmb A\\)의 스펙트럴 분해는 \\(\\pmb A = \\pmb P \\pmb \\Lambda \\pmb P^t\\)이며 \\(\\lambda_i\\)는 행렬 \\(\\pmb A\\)의 고유치, 즉 행렬 \\(\\pmb \\Lambda\\)의 대각원소이다. 또한 확률변수 \\(u_i\\)들은 서로 독립이며 정규분포 \\(N(\\eta_i, \\sigma^2)\\)를 따른다. 여기서 \\(\\eta_1, \\eta_2,\\dots, \\eta_n\\)는 다음과 같이 정의된다.\n\\[\n\\pmb \\eta =\n\\begin{bmatrix}\n\\eta_1 \\\\\n\\eta_2 \\\\\n\\vdots \\\\\n\\eta_n\n\\end{bmatrix} = \\pmb P^t \\pmb \\mu\n\\]\n즉, 식 F.5 에서 \\(u_1^2/\\sigma^2, u_2^2/\\sigma^2, \\dots, u_n^2/\\sigma^2\\)는 서로 독립이며 각각 자유도가 1 이고 비중심 모수가 \\(\\eta_1^2/\\sigma^2, \\eta_2^2/\\sigma^2, \\dots,  \\eta_n^2/\\sigma^2\\)인 비중심 카이제곱-분포를 따른다.\n\\(\\blacksquare\\)\n\n정리 F.1 의 식 F.5 에서 나타난 이차형식의 분포는 비중심 카이제곱 분포를 따르는 서로 독립인 확률 변수들의 가중 평균과 같다는 것이다.\n이는 이차형식의 분포가 비중심 카이제곱 분포를 따른다는 것이 아님을 주의해야 한다. 그러면 어느 경우에 이차형식의 분포가 비중심 카이제곱 분포를 따르는가 생각해 보자.\n가장 쉽게 생각할 수 있는 경우가 식 F.5 에서 \\(\\lambda_i\\) 들의 값들이 0 또는 1인 경우이다. 이러한 경우는 행렬\n\\(\\pmb A\\) 가 멱등행렬인 경우이다. 실제로 다음 정리는 이차형식의 분포가 비중심 카이제곱 분포를 따르는 필요충분 조건이 행렬\n\\(\\pmb A\\) 가 멱등행렬이라는 것을 말해준다.\n\n따름정리 F.1 \\(n\\)-차원의 확률벡터 \\(\\pmb x\\)가 \\(N(\\pmb \\mu, \\sigma^2 \\pmb I)\\)를 따른다면 이차형식 \\(Q=\\pmb x^t \\pmb A \\pmb x\\)의 분포가 자유도가 \\(r\\) 이며 다음과 같은 비중심 모수 \\(\\lambda^2\\)을 가지는 비중심 카이제곱 분포를 따르는 필요충분 조건은 \\(\\pmb A\\)가 멱등행렬이고 \\(rank(\\pmb A)=r\\) 인 경우이다.\n\\[\n\\lambda^2 = \\frac{\\pmb \\mu^t \\pmb  A \\pmb  \\mu}{\\sigma^2}\n\\]\n더 나아가 \\(\\pmb A \\pmb \\mu = \\pmb 0\\) 이면 이차형식의 분포는 자유도가 \\(r\\)인 (중심)카이제곱 분포를 따른다.\n\\(\\blacksquare\\)\n\n\n\nF.4.4 이차형식의 독립\n두 개의 이차형식이 독립일 조건은 다음 정리와 같다.\n\n정리 F.2 (이차형식의 독립) \\(n\\)-차원의 확률벡터 \\(\\pmb {x}\\) 가 \\(N\\left(\\pmb {\\mu}, \\sigma^2 \\pmb {I}\\right)\\) 를 따른다고 하자. 두 이차형식 \\(Q_1=\\pmb {x}^t \\pmb {A} \\pmb {x}\\) 과 \\(Q_2=\\pmb {x}^t \\pmb {B} \\pmb {x}\\) 가 서로 독립일 필요충분 조건은 \\(\\pmb {A B}=\\pmb {0}\\) 이다.\n\\(\\blacksquare\\)\n\n\n\nF.4.5 이차형식의 차이\n만약 3 개의 이차형식 \\(Q, Q_1, Q_2\\) 가 있어서 다음과 같은 관계가 있다고 하자.\n\\[\nQ=\\pmb {x}^t \\pmb {A} \\pmb {x}=Q_1+Q_2=\\pmb {x}^t \\pmb {A}_1 \\pmb {x}+\\pmb {x}^t \\pmb {A}_2 \\pmb {x}\n\\]\n이러한 경우 두 이차형식 \\(Q\\) 과 \\(Q_1\\) 이 각각 카이제곱 분포를 따를 때 \\(Q_2=Q-Q_1\\) 이 카이제곱 분포를 따르는 조건이 중요하다. 다음 정리는 그 조건을 행렬 \\(\\pmb {A}_2\\) 가 양반정치인 경우라는 것을 말해준다.\n\n정리 F.3 (이차형식의 차이) \\(n\\)-차원의 확률벡터 \\(\\pmb {x}\\) 가 \\(N\\left(\\pmb {\\mu}, \\sigma^2 \\pmb {I}\\right)\\) 를 따른다고 하자. 세 개의 이차형식 \\(Q=\\pmb {x}^t \\pmb {A} \\pmb {x}, Q_1=\\pmb {x}^t \\pmb {A}_1 \\pmb {x}\\), \\(Q_2=\\pmb {x}^t \\pmb {A}_2 \\pmb {x}\\) 가 있다고 하고 \\(Q=Q_1+Q_2\\) 인 관계를 가진다고 가정하자.\n만약 \\(Q / \\sigma^2\\) 이 \\(\\chi_r^2\\left(\\lambda^2\\right)\\) 을 따르고 \\(Q_1 / \\sigma^2\\) 이 \\(\\chi_{r_1}^2\\left(\\lambda_1^2\\right)\\) 을 따르며 행렬 \\(\\pmb {A}_2\\) 가 양반정치 행렬이면 다음을 만족한다. 두 이차형식 \\(Q_1\\) 과 \\(Q_2\\) 는 서로 독립이다. 또한 이차형식 \\(Q_2\\) 는 자유도가 \\(r_2=r-r_1\\) 이고 비중심 모수가 \\(\\lambda_2^2=\\lambda^2-\\lambda_1^2\\) 인 비중심 카이제곱분포를 따른다.\n\\(\\blacksquare\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>이차형식과 제곱합의 분포</span>"
    ]
  },
  {
    "objectID": "qmd/quadratic.html#코크란의-정리",
    "href": "qmd/quadratic.html#코크란의-정리",
    "title": "부록 F — 이차형식과 제곱합의 분포",
    "section": "F.5 코크란의 정리",
    "text": "F.5 코크란의 정리\n선형모형에서 자주 등장하는 제곱합들의 분해, 즉 이차형식의 분해를 생각할 때 각 제곱합들의 분포를 아는 것이 매우 중요하다. 다음에 제시된 코크란의 정리(Cochran’s Theorem)는 총 제곱합을 분해했을 때 각 제곱합의 분포가 카이제곱 분포를 따를 조건을 말해준다.\n\n정리 F.4 (COCHRAN’S THEOREM) \\(n\\)-차원의 확률벡터 \\(\\pmb {x}\\) 가 \\(N\\left(\\pmb {\\mu}, \\sigma^2 \\pmb {I}\\right)\\) 를 따른다고 하자. \\(k\\) 개의 이차형식 \\(Q_j=\\pmb {x}^t \\pmb {A}_j \\pmb {x}, j=1,2, \\ldots, k\\) 를 생각하고 다음과 같은 관계를 가진다고 하자.\n\\[\n\\pmb {x}^t \\pmb {x}=\\sum_{i=1}^n x_i^2=\\sum_{j=1}^k Q_j\n\\]\n즉, \\(\\sum_{j=1}^k \\pmb {A}_j=\\pmb {I}\\) 이다. 또한 \\(r_j=\\operatorname{rank}\\left(\\pmb {A}_j\\right)\\) 이고 \\(\\lambda_j^2=\\pmb {\\mu}^t \\pmb {A}_j \\pmb {\\mu}\\) 라고 하자.\n\\(k\\) 개의 이차형식 \\(Q_1, Q_2, \\ldots, Q_k\\) 들이 모두 독립이고 각 이차형식 \\(Q_j / \\sigma^2\\) 가 비중심 카이제곱 분포 \\(\\chi_{r_j}^2\\left(\\lambda_j^2\\right)\\) 를 따를 필요충분 조건은 다음과 같다.\n\\[\nr_1+r_2+\\cdots+r_k=n\n\\] \\(\\blacksquare\\)\n\n이제 제곱합의 분포들에 대하여 지금까지 학습한 내용을 정리해보자. 만약 \\(n\\)-차원의 확률벡터 \\(\\pmb {x}\\) 가 \\(N\\left(\\pmb {\\mu}, \\sigma^2 \\pmb {I}\\right)\\) 를 따른다고 하고 위의 코크란의 정리와 같이 제곱합의 분해를 고려하자. 다음에 제시된 모든 문장은 서로 동치(equivalent)이다.\n\n이차형식 \\(Q_1, Q_2, \\ldots, Q_k\\) 들이 모두 독립이다.\n모든 \\(j=1,2, \\ldots, k\\) 에 대하여 이차형식 \\(Q_j / \\sigma^2\\) 가 비중심 카이제곱 분포 \\(\\chi_{r_j}^2\\left(\\lambda_j^2\\right)\\) 를 따른다.\n\\(\\pmb {A}_1, \\pmb {A}_2, \\ldots, \\pmb {A}_k\\) 가 모두 멱등행렬이다.\n모든 \\(j \\neq k\\) 에 대하여 \\(\\pmb {A}_j \\pmb {A}_k=\\pmb {0}\\) 이다.\n\\(r_1+r_2+\\cdots+r_k=n\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>이차형식과 제곱합의 분포</span>"
    ]
  },
  {
    "objectID": "qmd/aic.html",
    "href": "qmd/aic.html",
    "title": "부록 G — 모형선택의 정보 기준",
    "section": "",
    "text": "G.1 Kullback-Leibler 정보\n앞에서 소개한 AIC 는 두 개의 분포에 대한 거리를 반양하는 정보 기준에 의하여 유도된 모형선택의 측도이다. 이제 AIC 가 어떻게 두 개의 분포에 대한 거리에서 유도되는지 알아보자.\n두 분포의 거리를 나타내는 KL-정보는 다음과 같은 성질을 가진다.\n먼저 \\({\\pmb  y} = \\{ y_1, y_2,\\dots, y_n \\}\\) 가 참분포(true distribution) \\(G(y)\\) or \\(g(y)\\) 에서 독립적으로 얻은 확률변수라고 하자.\n이제 \\(F(y)\\) or \\(f(y)\\) 는 얻어진 자료에 대한 모형으로 사용하고자 하는 분포이며 이를 후보 모형(candidate distribution)이라고 하다. 일반적으로 고려하는 분포들을 모아놓은 집합을 후보 모형군(family of candidate distributions) 이라고 하며 \\(\\{ f(y | \\theta) |  \\theta \\in  \\Theta \\}\\) 라고 표기한다. 이런 후보 모형군은 분포를 결정하는 모수들을 모아 놓은 모수 집합 \\(\\Theta\\)으로 표시하기도 한다.\n이제 후보 모형군에 속하는 임의의 분포 \\(f(y) = f(y|\\theta)\\) 와 참모형 \\(g(y)\\) 의 KL-정보는 다음과 같다.\n\\[\nI(g;f) = E_G[ \\log g(y) ]-E_G[ \\log f(y)  ]\n\\]\n위에서 정의한 \\(I(g;f)\\)의 값이 작을수록 좋은 것이며 이는 고려한 후보 분포 \\(f(y)\\) 가 참모형 \\(g(y)\\) 에 더 가깝다는 의미이기 떄문이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>모형선택의 정보 기준</span>"
    ]
  },
  {
    "objectID": "qmd/aic.html#kullback-leibler-정보",
    "href": "qmd/aic.html#kullback-leibler-정보",
    "title": "부록 G — 모형선택의 정보 기준",
    "section": "",
    "text": "정의 G.1 (Kullback-Leibler 정보) 두 개의 분포 \\(F\\) 와 \\(G\\) 가 있다고 가정하고 각 분포에 대한 확률밀도함수가 \\(f\\) 와 \\(g\\) 로 주어졌다. KL-정보(Kullback-Leibler information) 는 두 개의 분포 \\(F\\) 와 \\(G\\) 의 거리를 다음과 같이 정의하는 정보기준이다.\n\\[\n\\begin{aligned}\nI(g;f) & = E_G \\left [ \\log \\left \\{ \\frac{g(y)}{f(y)} \\right \\} \\right ] \\\\\n& = \\int \\log \\left \\{ \\frac{g(y)}{f(y)} \\right \\} g(y) dy \\\\\n& = \\int \\left [ \\log g(y)-\\log f(y)  \\right ] g(y) dy\n\\end{aligned}\n\\]\n\\(\\blacksquare\\)\n\n\n\n\\(I(g;f) \\ge 0\\)\n만약 \\(I(g;f) = 0\\) 이면 \\(g(y)=f(y)\\) a.e.\n\n\n\n보기 G.1 (정규분포의 거리) 두 개의 정규분포 \\(F \\equiv N(\\mu,\\sigma^2)\\) 와 \\(G\\equiv N(\\xi, \\tau^2)\\) 을 고려하자.\n분포 \\(G\\) 에서 \\((y-\\mu)^2\\) 의 기대값은 다음과 같이 주어지므로\n\\[\nE_G (y-\\mu)^2 = E_G(y-\\xi+\\xi -\\mu)^2 =\\tau^2 +(\\xi-\\mu)^2\n\\]\n분포 \\(G\\) 를 가정하고 확률밀도함수 \\(f\\) 의 로그에 대한 기대값은 다음과 같다.\n\\[\n\\begin{aligned}\nE_G[ \\log f(y) ] &= E_G \\left [ -\\frac{1}{2} \\log(2\\pi \\sigma^2) -(y-\\mu)^2/(2\\sigma^2)  \\right ]  \\\\\n  &= -\\frac{1}{2} \\log(2\\pi \\sigma^2)-[\\tau^2 +(\\xi-\\mu)^2]/(2\\sigma^2)\n\\end{aligned}\n\\]\n유사한 방법으로 분포 \\(G\\) 에서 확률밀도함수 \\(g\\) 의 로그에 대한 기대값도 다음과 같이 구할 수 있다.\n\\[ E_G[ \\log g(y) ] = -\\frac{1}{2} \\log(2\\pi \\tau^2)-\\frac{1}{2} \\]\n위에서 구한 제곱합의 기대값을 이용하면 두 개의 정규분포 \\(F\\) 와 \\(G\\) 의 KL-정보는 다음과 같이 주어진다.\n\\[\nI(g;f) = E_G[ \\log g(y) ]-E_G[ \\log f(y) ] = \\frac{1}{2} \\left \\{ \\log \\frac{\\sigma^2}{\\tau^2} + \\frac{\\tau^2 +(\\xi-\\mu)^2}{\\sigma^2} -1 \\right \\}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>모형선택의 정보 기준</span>"
    ]
  },
  {
    "objectID": "qmd/aic.html#가능도-함수",
    "href": "qmd/aic.html#가능도-함수",
    "title": "부록 G — 모형선택의 정보 기준",
    "section": "G.2 가능도 함수",
    "text": "G.2 가능도 함수\n이제 자료에 대한 후보 분포를 \\(F(y)\\) 또는 \\(f(y)\\) 라고 하고 로그 가능도 함수의 기대값을 고려하자. 이 때 기대값은 참분포에서 계산된 기대값이다.\n\\[\nE_G[ \\log f(y)  ] = \\int \\log f(y) g(y) dy\n\\tag{G.1}\\]\n실제 자료를 분석하는 경우 참분포를 알 수 없기 때문에 로그 가능도 함수의 기대값 식 G.1 을 구하는 것은 불가능하다. 따라서 참분포에 대한 추정을 하여 구해야 하는데 참분포 \\(G\\)에 대한 추정량은 자료를 이용하여 구할 수 있는 경험 분포함수(emprical distribution)를 이용할 수 있다.\n표본 자료 \\({\\pmb  y} = \\{ y_1, y_2,\\dots, y_n \\}\\) 를 이용하여 얻은 참분포 \\(G\\)에 대한 경험적 추정 분포는 다음과 같다.\n\\[\n\\hat G(y) = \\frac{1}{n} \\sum_{i=1}^n I(y \\le y_i)\n\\tag{G.2}\\]\n이제 자료에서 얻은 경험분포를 사용하여 로그 가능도 함수의 기대값 식 G.1 에 대한 추정량을 구하면 다음과 같다.\n\\[\nE_{\\hat G} [ \\log f(y)  ] = \\int \\log f(y) d \\hat G(y) = \\frac{1}{n} \\sum_{i=1}^n  \\log f(y_i)\n\\tag{G.3}\\]\n대수의 법칙(the law of large numbers)에 의하여 표본의 개수 \\(n\\) 이 커지먄 다음이 성립한다.\n\\[\n\\frac{1}{n} \\sum_{i=1}^n  \\log f(y_i) \\rightarrow_{a.e.}  E_{ G} [ \\log f(y)  ]\n\\]\n표본에 대한 모수적 확률 모형들을 모아놓은 집합, 모형공간 \\(\\{f(y| \\pmb  \\theta) | \\pmb  \\theta \\in \\pmb  \\Theta \\subset R^p \\}\\). 을 고려하자. 표본자료 \\({\\pmb  y} = \\{ y_1, y_2,\\dots, y_n \\}\\)로 부터 얻은 로그가능도함수는 다음과 같이 주어진다.\n일단 여기서는 참분포 \\(g(y)\\) 가 모수적 확률 모형 집합에 속하는 분포라고 가정하자.\n\\[\ng(x) =f(y| \\pmb  \\theta_0) \\text{ for some } \\pmb  \\theta_0 \\in \\pmb  \\Theta\n\\]\n\\[\n\\ell(\\pmb  \\theta) = \\sum_{i=1}^n \\log f(x_i | \\pmb  \\theta )\n\\]\n최대가능도 추정량 \\(\\hat {\\pmb  \\theta} = \\hat {\\pmb  \\theta}({\\pmb  y})\\)은 다음과 같이 로그가능도함수를 최대로 하는 추정량이다.\n\\[\n\\hat {\\pmb  \\theta} = arg \\max_{\\pmb  \\theta \\in \\pmb  \\Theta} \\ell(\\pmb  \\theta)\n\\]\n이제 \\(\\pmb  \\theta_0\\) 을 다음에 주어진 방정식의 근이라고 하자.\n\\[\n\\int  \\frac{\\partial \\log f(y | \\pmb  \\theta ) } {\\partial  \\pmb  \\theta}f(y | \\pmb  \\theta )  dy =0\n\\]\n위의 식에서 다음과 같이 로그 확률함수를 모수벡터로 미분한 양을 스코어함수(score function) \\(u(\\pmb  \\theta; \\pmb  y)\\)이라고 부른다.\n\\[\nu(\\pmb  \\theta; \\pmb  y) = \\frac{\\partial \\log f(y | \\pmb  \\theta ) } {\\partial  \\pmb  \\theta}\n\\]\n따라서 \\(\\pmb  \\theta_0\\) 을 다음에 주어진 방정식의 근이다.\n\\[\nE_{\\theta} [u(\\pmb  \\theta; \\pmb  y) ] =0\n\\]\n만약 정상적인 조건들(regularity conditions)이 만족하면 다음과 같은 결과를 얻을 수 있다.\n\n가능도 방정식 \\(\\partial \\ell(\\pmb  \\theta)/\\partial  \\pmb  \\theta =0\\) 은 점근적으로(with probability 1) 방정식의 해 \\(\\hat {\\pmb  \\theta}\\) 를 가진다.\n최대가능도추정량(MLE) \\(\\hat {\\pmb  \\theta}\\) 는 점근적으로 \\(\\pmb  \\theta_0\\) 에 수렴한다.\n최대가능도추정량은 점근적으로 다음과 같은 정규분포를 따른다.\n\n\\[\n\\sqrt{n} ( \\hat {\\pmb  \\theta} - \\pmb  \\theta_0) \\rightarrow_d N(0, I(\\pmb  \\theta_0))\n\\tag{G.4}\\]\n위의 식에서 \\(I(\\pmb  \\theta)\\) 는 피셔정보(Fisher information matrix) 이라고 부르며 다음과 같이 정의된다.\n\\[\nI(\\pmb  \\theta) = \\int f(y | \\pmb  \\theta ) \\frac{\\partial \\log f(y | \\pmb  \\theta ) } {\\partial  \\pmb  \\theta} \\frac{\\partial \\log f(y | \\pmb  \\theta ) } {\\partial  \\pmb  \\theta^t} dy\n\\]\n위에서 최대가능도 추정량에 대한 모든 성질은 참분포 \\(g(y)\\) 가 모수적 확률 모형들의 집합 \\(\\{f(y| \\pmb  \\theta) | \\pmb  \\theta \\in \\pmb  \\Theta \\subset R^p \\}\\) 에 속한다고 가정하였다\n\\[ g(x) =f(y| \\pmb  \\theta_0)) \\text{ for some } \\pmb  \\theta_0 \\]\n만약 \\(g(y)\\) 가 우리가 고려하고 있는 모수적 모형들의 집합에 속해있지 않다면 앞에서 구한 최대가능도 추정량의 점근적 성질들은 어떻게 될까?\n\\[ g(x) \\ne f(y| \\pmb  \\theta) \\text{ for all } \\pmb  \\theta \\]\n이제 참분포 \\(g(y)\\) 가 모수적 확률 모형들의 집합에 속하지 않을 수도 있다고 가정하자.\n또한 \\(\\pmb  \\theta_0\\) 를 다음 방정식의 근이라고 하자.\n\\[\n\\int g(y) \\frac{\\partial \\log f(y | \\pmb  \\theta ) } {\\partial  \\pmb  \\theta} dy =0\n\\]\n이러한 가정하에서는 다음이 성립한다.\n\n최대가능도추정량(MLE) \\(\\hat {\\pmb  \\theta}\\) 는 점근적으로 \\(\\pmb  \\theta_0\\) 에 수렴한다.\n최대가능도추정량은 점근적으로 다음과 같은 정규분포를 따른다.\n\n\\[\n\\sqrt{n} ( \\hat {\\pmb  \\theta} - \\pmb  \\theta_0) \\rightarrow_d\nN(0, J^{-1}(\\pmb  \\theta_0) I(\\pmb  \\theta_0) J^{-1}(\\pmb  \\theta_0) )\n\\]\n위의 식에서 \\(I(\\pmb  \\theta)\\) 와 \\(J(\\pmb  \\theta)\\) 는 다음과 같이 정의되는 양이다.\n\\[\n\\begin{aligned}\nI(\\pmb  \\theta) &= \\int g(y) \\frac{\\partial \\log f(y | \\pmb  \\theta ) } {\\partial  \\pmb  \\theta} \\frac{\\partial \\log f(y | \\pmb  \\theta ) } {\\partial  \\pmb  \\theta^t} dy \\\\\nJ(\\pmb  \\theta) &= - \\int g(y) \\frac{\\partial^2 \\log f(y | \\pmb  \\theta ) } {\\partial  \\pmb  \\theta \\pmb  \\theta^t}  dy\n\\end{aligned}\n\\]\n주목할 점은 만약 참분포가 모수적 분포의 집합에 속하면, 즉 \\(g(y) =f(y| \\pmb  \\theta_0)\\) 이면 \\(I(\\pmb  \\theta_0) =J(\\pmb  \\theta_0)\\) 이 성립하고 식 G.4 이 성립힌다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>모형선택의 정보 기준</span>"
    ]
  },
  {
    "objectID": "qmd/aic.html#aic",
    "href": "qmd/aic.html#aic",
    "title": "부록 G — 모형선택의 정보 기준",
    "section": "G.3 AIC",
    "text": "G.3 AIC\n이제 \\({\\pmb  y} = \\{ y_1, y_2,\\dots, y_n \\}\\) 는 참모형 \\(g(y)\\)에서 얻어진 독립표본이라고 하자. 모수적 확률 분포의 집합 \\(\\{f(y| \\pmb  \\theta) | \\pmb  \\theta \\in \\pmb  \\theta \\subset R^p \\}\\)을 고려한다. 또한 모르는 모수 \\(\\pmb  \\theta\\) 는 최대가능도 추정량 \\(\\hat {\\pmb  \\theta}\\) 에 의하여 추정된다고 하자.\n이제 추정된 모수로 구한 확률분포 \\(f(y|\\hat {\\pmb  \\theta})\\) 와 참분포 \\(g(y)\\) 가 얼마나 차이가 있는지 관심이 있으며 이 거리를 K-L 정보 를 이용하여 구하면 다음과 같다.\n\\[\nI(g(z);~f(z|\\hat {\\pmb  \\theta})) = E_G [ \\log g(z)] - E_G[\\log f(z|\\hat {\\pmb  \\theta}) ]\n\\tag{G.5}\\]\n위의 식 G.5 에서 기대값 \\(E_G()\\) 는 참분포 \\(g(z)\\)에 추출한 새로운 확률변수 \\(z\\)에 대한 기대값이며 표본으로 부터 구한 \\(\\hat {\\pmb  \\theta}=\\hat {\\pmb  \\theta}({\\pmb  y}_n)\\) 는 표본 \\(\\pmb  y\\)의 함수로서 기대값 \\(E_G()\\)과 관계없이 고정된 양이다.\n위의 식 G.5 에 주어진 K-L 정보에서 앞의 기대값 \\(E_G [ \\log g(z)]\\) 은 언제나 주어진 상수이므로 참분포와 모수적 분포의 거리를 나타내는 양으로 K-L 정보에서 뒤의 기대값이 모수적 분포의 적함도를 반영하는 중요한 측도이다.\n\\[\nE_G[\\log f(z|\\hat {\\pmb  \\theta}) ]  = \\int \\log f(z|\\hat {\\pmb  \\theta}) g(z) dz\n\\tag{G.6}\\]\n\\(I(g(z);~f(z|\\hat {\\pmb  \\theta})) \\ge 0\\) 이므로 위의 식 G.6 에 주어진 양이 크면 클수록 참분포와 거리가 작아지므로 더 좋은 분포의 추정량이라고 말할 수 있다.\n여기서 중요한 점은 실제 문제에서는 참분포 \\(g(y)\\) 를 알 수 없으며 식 G.6 의 값을 추정하려면 참분포 \\(g(y)\\) 에 대한 추정량이 필요하다. 가장 간단한 추정량은 참분포의 분포함수 \\(G\\) 를 경험적 표본 분포함수로 추정하는 것이다. 아래는 참분포의 분포함수에 대한 단순 추정량 \\(\\hat G\\) 이다.\n\\[\nE_{\\hat G} [\\log f(z|\\hat {\\pmb  \\theta}) ]  = \\int \\log f(z|\\hat {\\pmb  \\theta}) d \\hat G(z) = \\frac{1}{n} \\sum_{i=1}^n \\log f(y_i|\\hat {\\pmb  \\theta})  \n\\tag{G.7}\\]\n사실 식 G.7 는 최대값을 가지는 로그 가능도함수를 \\(n\\) 으로 나눈 양이다\n이제 식 G.7 으로 주어진 추정량으로 식 G.6 를 추정해야 하는데 사실 두 양이 모두 표본 \\(y_1, y_2, \\dots, y_n\\)에 의해 얻어진 \\(\\hat {\\pmb  \\theta}\\)의 함수이다. 따라서 두 통계량 모두 참분포 \\(g(y)\\) 에서 얻어진 표본 \\(y_1, y_2, \\dots, y_n\\) 도 고려해야 한다.\n\\[\nE_{G(y)}  \\left [ \\frac{1}{n} \\sum_{i=1}^n \\log f(y_i|\\hat {\\pmb  \\theta}) \\right ]\n=_?\nE_{G(y)}  \\left [  E_{G(z)} [\\log f(z|\\hat {\\pmb  \\theta}) ] \\right ]\n\\tag{G.8}\\]\n불행하게도 위의 두 기대값의 값이 다르기 때문에 식 G.7 에 나타난 추정량은 식 G.6 의 불편추정량이 아니다. 따라서 식 G.7 에 나타난 추정량은 다음과 같이 주어진 편이(bias)를 구해서 보종할 수 있다.\n\\[\nb(G) = E_{G({\\pmb  y})} \\left [ \\log f({\\pmb  y}|\\hat {\\pmb  \\theta}) -  E_G(\\log f(z|\\hat {\\pmb  \\theta}(\\pmb  y))) \\right ]\n\\tag{G.9}\\]\n식 G.9 에 나타난 편이에 대한 해석은 다음과 같이 말할 수 있다.\n\n실제로 추정해야 하는 측도는 \\(E_{G(y)}  [  E_{G(z)} [\\log f(z|\\hat {\\pmb  \\theta}) ]  ]\\)이며 이는 표본 \\(\\pmb  y\\) 에서 추정량을 이용하여 새로운 반응변수 \\(z\\) 를 예측할 때의 측도이다.\n하지만 표본 추정량에 근거한 \\(E_{G(y)}  [ \\sum_{i=1}^n \\log f(y_i|\\hat {\\pmb  \\theta}) /n ]\\)은 표본 \\(\\pmb  y\\) 에서 추정량을 이용하여 다시 표본에서 얻은 반응값을 예측하는 측도이다.\n따라서 두 측도 사이에는 차이가 존재하며 표본 추정량에 근거한 \\(E_{G(y)}  [ \\sum_{i=1}^n \\log f(y_i|\\hat {\\pmb  \\theta}) ]\\)는 실제로 과대 추정된다(과적합 발생).\n이러한 이유로 가능도함수로 나타난 측도 \\(-2 \\sum_{i=1}^n \\log f(y_i|\\hat {\\pmb  \\theta})\\) 를 추정된 편이로 보정해주어야 올바른 추론이다.\n\n만약 우리가 식 G.9 에 나타난 편이 \\(b(G)\\)을 추정할 수 있다면 우리가 찾은 모수적 최적 모형과 참모형의 K-L거리에 근거한 모형의 적합도 \\(IC({\\pmb  y};\\hat {\\pmb  \\theta})\\) 를 다음과 같이 정의할 수 있다.\n\\[\n\\begin{aligned}\nIC({\\pmb  X};\\hat G) &= -2(\\text{log-likelihood of the model} - \\text{bias estimator}) \\\\\n   &= -2 \\sum_{i=1}^n \\log f(y_i|\\hat {\\pmb  \\theta}) +2  (\\text{bias estimator } b(G))\n\\end{aligned}\n\\]\n본 강의에서는 구하지 않겠지만 식 G.9 에 나타난 편이 \\(b(G)\\)을 는 다음과 같이 두 행렬의 곱에 대각원소의 합으로 나타난다.\n\\[\nb(G) = tr [ I(\\pmb  \\theta_0) J(\\pmb  \\theta_0)^{-1} ]\n\\tag{G.10}\\]\n위의 식에서 \\(I(\\pmb  \\theta)\\) 와 \\(J(\\pmb  \\theta)\\)는 다음과 같이 정의된 양이다.\n\\[\n\\begin{aligned}\nI(\\pmb  \\theta) &= \\int g(y) \\frac{\\partial \\log f(y | \\pmb  \\theta ) } {\\partial  \\pmb  \\theta} \\frac{\\partial \\log f(y | \\pmb  \\theta ) } {\\partial  \\pmb  \\theta^t} dy \\\\\nJ(\\pmb  \\theta) &= - \\int g(y) \\frac{\\partial^2 \\log f(y | \\pmb  \\theta ) } {\\partial  \\pmb  \\theta \\pmb  \\theta^t}  dy\n\\end{aligned}\n\\]\n만약 참모형 \\(g(z)\\) 이 다음과 같이 고려한 모수적 모형의 집합 \\(\\{f(y| \\pmb  \\theta) | \\pmb  \\theta \\in \\pmb  \\theta \\subset R^p \\}\\) 에 속한다면\n\\[  \n\\text{ If } g(y) =f(y|\\pmb  \\theta_0) \\quad \\text{ for some } \\theta_0 , \\quad \\text{then} \\quad  I(\\pmb  \\theta) = J(\\pmb  \\theta)\n\\]\n이런 조건에서는 식 G.10 에 주어진 편이가 다음과 같이 모수의 개수로 나타난다.\n\\[\nb(G) = tr [ I(\\pmb  \\theta_0) J(\\pmb  \\theta_0)^{-1} ] = tr({\\pmb  I}_p) = p\n\\]\n따라서 K-L 정보기준으로 유도된 참모형과 최대가능도 추정법으로 선택된 분포의 거리로 표현되는 AIC(Akaike Information Criteria)는 다음과 같이 정의된다.\n\\[ AIC = -2 \\sum_{i=1}^n \\log f(y_i|\\hat {\\pmb  \\theta}) +2p \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>모형선택의 정보 기준</span>"
    ]
  },
  {
    "objectID": "qmd/aic.html#bic",
    "href": "qmd/aic.html#bic",
    "title": "부록 G — 모형선택의 정보 기준",
    "section": "G.4 BIC",
    "text": "G.4 BIC\n베이지안 정보 기준(BIC) 또는 슈바르츠 정보 기준(SIC, SBC, SBIC)은 모형공간에서 최적의 모형을 선택하는 기준으로, 일반적으로 BIC가 낮은 모델이 선호된다. 이는 부분적으로 (AIC)과 밀접한 관련이 있다.\n먼저 BIC 를 유도하는 과정을 살펴보려면 베이지안 통계에서 나타나는 모수에 대한 사전분포(prior distribution) \\(g(\\theta)=p(\\theta)\\) 을 고려해야 한다.\n또한 모형 공간을 \\(\\mathcal{M}=\\left\\{m_1, \\ldots, m_M\\right\\}\\) 이라고 하자. 또한 \\(m \\in \\mathcal{M}\\) 을 모형공간에 속하는 하나의 모형을 나타낸다.\n\\(\\ell(\\theta)\\) 을 로그 가능도 함수라고 하자. 아레 식에서 \\(f(y \\mid \\theta, m)\\) 은 모형 \\(m\\) 에서 주어진 모수 \\(\\theta\\) 에 대한 반응변수 \\(y\\) 의 조건부 확률밀도함수이다.\n\\[\n\\ell(\\theta)=\\log f(y \\mid \\theta, m)\n\\]\n다음으로 로그 가능도 함수와 사전분포를 이용하여 함수 \\(g\\) 와 \\(h\\) 를 다음과 같이 정의하자.\n\\[\n\\begin{aligned}\n& g(\\theta)=p(\\theta \\mid m ) \\\\\n& h(\\theta)=\\frac{1}{n} \\ell (\\theta) .\n\\end{aligned}\n\\tag{G.11}\\]\n베이지안 통계에 의하면 반응변수 \\(y\\)에 대한 주변분포(marginal distribution) \\(p(y \\mid m)\\) 는 다음과 같이 주어진다.\n\\[\n\\begin{aligned}\np(y \\mid m) & =\\int_{\\Theta} f(y \\mid \\theta, m) p(\\theta \\mid m) \\mathrm{d} \\theta \\\\\n& =\\int_{\\Theta} \\exp [n h(\\theta)] g(\\theta) \\mathrm{d} \\theta\n\\end{aligned}\n\\tag{G.12}\\]\nThis is an integral suitable for Laplace approximation which states that\n이제 위의 적분에 대한 라플라스 근사를 적용하면 다음과 같이 주어진다.\n\\[\n\\int_{\\Theta} \\exp [n h(\\theta)] g(\\theta) \\mathrm{d} \\theta=\\left(\\sqrt{\\frac{2 \\pi}{n}}\\right)^p \\exp \\left[n h\\left(\\theta_0\\right)\\right]\\left(g\\left(\\theta_0\\right)\\left| H \\left(\\theta_0\\right)\\right|^{-1 / 2}+O(1 / n)\\right)\n\\tag{G.13}\\]\n위의 식에서\n\\(\\theta_0\\) 는 \\(h(\\theta)\\) 를 최대화하는 값이고 \\(H \\left(\\theta_0\\right)\\) 는 \\(\\theta_0\\) 에서 계산된 \\(h(\\theta)\\)수의 헤시안 행렬(Hessian matrix) 이다.\n우리는 지금 최대가능도 추정법을 다루고 있으므로 위의 식에서 나타난 \\(\\theta_0\\) 는 최대가능도 추정량 \\(\\hat{\\theta}\\) 이다.\n\\[\n\\hat{\\theta}=\\underset{\\theta}{\\arg \\max } ~\\ell(\\theta) .\n\\]\n위의 결과에서 식 G.13 를 식 G.11 와 식 G.12 에 적용하면 다음 결과를 얻을 수 있다.\n\\[\np(y \\mid m ) \\approx\\left(\\sqrt{\\frac{2 \\pi}{n}}\\right)^p f(y \\mid \\hat{\\theta}, m) p(\\hat{\\theta} \\mid m)| H (\\hat{\\theta})|^{-1 / 2} .\n\\tag{G.14}\\]\n식 G.14 에 로그를 취하고 \\(-2\\) 다음과 같은 결과를 얻는다.\n\\[\n-2 \\log p(y \\mid m ) \\approx-2 \\ell(\\hat{\\theta})+p \\log n-p \\log (2 \\pi)-2 \\log p(\\hat{\\theta} \\mid m)+\\log |J(\\hat{\\theta})| .\n\\tag{G.15}\\]\n표본의 크기가 커지면(\\(n \\rightarrow \\infty\\)) , 식 G.15 의 마지막 3개의 항은 \\(O_p(1)\\) 으로 나머지 항에 비교하여 무시할 수 있다.\n이제 모형 \\(\\mathcal{M}=\\left\\{m_1, \\ldots, m_M\\right\\}\\) 에서 최적의 모형을 선택하는 기준은 모형에 대한 사후분포 \\(p\\left(m_j \\mid y\\right)\\) 를 최대로 하는 기준을 사용하는데 이는 우리가 근사한 주변분포 \\(p\\left(y \\mid m_j\\right)\\) 에 비례하는 것을 이용하여 다음과 같이 모형의 선택 기준으로 BIC 를 정의할 수 있다. \\[\n\\operatorname{BIC}(m)=-2 \\log f(y \\mid \\hat{\\theta}, m)+p \\log n .\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>모형선택의 정보 기준</span>"
    ]
  },
  {
    "objectID": "qmd/practice-01.html",
    "href": "qmd/practice-01.html",
    "title": "부록 H — R-실습: 중회귀 모형 적합",
    "section": "",
    "text": "H.1 예제 3.3 자료\n예제 3.3에 나온 중고차 가격자료를 이용한 R 실습입니다.\nhead(usedcars)\n\n  price year mileage   cc automatic\n1   790   78  133462 1998         1\n2  1380   39   33000 2000         1\n3   270  109  120000 1800         0\n4  1190   20   69727 1999         1\n5   590   70  112000 2000         0\n6  1120   58   39106 1998         1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>R-실습: 중회귀 모형 적합</span>"
    ]
  },
  {
    "objectID": "qmd/practice-01.html#예제-3.3-자료",
    "href": "qmd/practice-01.html#예제-3.3-자료",
    "title": "부록 H — R-실습: 중회귀 모형 적합",
    "section": "",
    "text": "H.1.1 산점도 행렬\n\npairs(usedcars)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>R-실습: 중회귀 모형 적합</span>"
    ]
  },
  {
    "objectID": "qmd/practice-01.html#중회귀-모형의-적합",
    "href": "qmd/practice-01.html#중회귀-모형의-적합",
    "title": "부록 H — R-실습: 중회귀 모형 적합",
    "section": "H.2 중회귀 모형의 적합",
    "text": "H.2 중회귀 모형의 적합\n\nfit0 &lt;- lm(price ~ year + mileage + cc + automatic, usedcars)\n\n계획행렬은 다음과 같이 구할 수 있다.\n\nmodel.matrix(fit0)\n\n   (Intercept) year mileage   cc automatic\n1            1   78  133462 1998         1\n2            1   39   33000 2000         1\n3            1  109  120000 1800         0\n4            1   20   69727 1999         1\n5            1   70  112000 2000         0\n6            1   58   39106 1998         1\n7            1   53   95935 1800         1\n8            1   68  120000 1800         0\n9            1   15   20215 1798         1\n10           1   96  140000 1800         0\n11           1   63   68924 1998         1\n12           1   82   90000 2000         0\n13           1   76   81279 1998         0\n14           1   17   24070 1798         1\n15           1   38   40000 2000         0\n16           1   46   56887 1832         1\n17           1   95   91216 1997         1\n18           1   37   48680 1998         1\n19           1   68    8000 2000         0\n20           1   41   60634 1835         1\n21           1   69  114131 1998         1\n22           1   71   75000 1800         0\n23           1   99  124417 1998         1\n24           1  129  130000 1800         0\n25           1   57   77559 1997         1\n26           1  107   75216 1838         1\n27           1   45   52000 2000         0\n28           1   80   58000 2000         1\n29           1  113  134500 1800         0\n30           1   41   80000 2000         0\nattr(,\"assign\")\n[1] 0 1 2 3 4\n\n\nfit0 에 저장된 결과를 다음과 같이 함수 str을 이용하여 볼 수 있다.\n\nstr(fit0)\n\nList of 12\n $ coefficients : Named num [1:5] 525.28696 -5.79964 -0.00226 0.38879 165.31263\n  ..- attr(*, \"names\")= chr [1:5] \"(Intercept)\" \"year\" \"mileage\" \"cc\" ...\n $ residuals    : Named num [1:30] 76.98 212.69 -51.4 -4.01 -53.45 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:30] -4407 -1434 -369 -229 419 ...\n  ..- attr(*, \"names\")= chr [1:30] \"(Intercept)\" \"year\" \"mileage\" \"cc\" ...\n $ rank         : int 5\n $ fitted.values: Named num [1:30] 713 1167 321 1194 643 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:5] 0 1 2 3 4\n $ qr           :List of 5\n  ..$ qr   : num [1:30, 1:5] -5.477 0.183 0.183 0.183 0.183 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:5] \"(Intercept)\" \"year\" \"mileage\" \"cc\" ...\n  .. ..- attr(*, \"assign\")= int [1:5] 0 1 2 3 4\n  ..$ qraux: num [1:5] 1.18 1.18 1.08 1.03 1.26\n  ..$ pivot: int [1:5] 1 2 3 4 5\n  ..$ tol  : num 1e-07\n  ..$ rank : int 5\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 25\n $ xlevels      : Named list()\n $ call         : language lm(formula = price ~ year + mileage + cc + automatic, data = usedcars)\n $ terms        :Classes 'terms', 'formula'  language price ~ year + mileage + cc + automatic\n  .. ..- attr(*, \"variables\")= language list(price, year, mileage, cc, automatic)\n  .. ..- attr(*, \"factors\")= int [1:5, 1:4] 0 1 0 0 0 0 0 1 0 0 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:5] \"price\" \"year\" \"mileage\" \"cc\" ...\n  .. .. .. ..$ : chr [1:4] \"year\" \"mileage\" \"cc\" \"automatic\"\n  .. ..- attr(*, \"term.labels\")= chr [1:4] \"year\" \"mileage\" \"cc\" \"automatic\"\n  .. ..- attr(*, \"order\")= int [1:4] 1 1 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(price, year, mileage, cc, automatic)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:5] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. ..- attr(*, \"names\")= chr [1:5] \"price\" \"year\" \"mileage\" \"cc\" ...\n $ model        :'data.frame':  30 obs. of  5 variables:\n  ..$ price    : int [1:30] 790 1380 270 1190 590 1120 815 450 1290 420 ...\n  ..$ year     : int [1:30] 78 39 109 20 70 58 53 68 15 96 ...\n  ..$ mileage  : int [1:30] 133462 33000 120000 69727 112000 39106 95935 120000 20215 140000 ...\n  ..$ cc       : int [1:30] 1998 2000 1800 1999 2000 1998 1800 1800 1798 1800 ...\n  ..$ automatic: int [1:30] 1 1 0 1 0 1 1 0 1 0 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language price ~ year + mileage + cc + automatic\n  .. .. ..- attr(*, \"variables\")= language list(price, year, mileage, cc, automatic)\n  .. .. ..- attr(*, \"factors\")= int [1:5, 1:4] 0 1 0 0 0 0 0 1 0 0 ...\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:5] \"price\" \"year\" \"mileage\" \"cc\" ...\n  .. .. .. .. ..$ : chr [1:4] \"year\" \"mileage\" \"cc\" \"automatic\"\n  .. .. ..- attr(*, \"term.labels\")= chr [1:4] \"year\" \"mileage\" \"cc\" \"automatic\"\n  .. .. ..- attr(*, \"order\")= int [1:4] 1 1 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(price, year, mileage, cc, automatic)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:5] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. .. ..- attr(*, \"names\")= chr [1:5] \"price\" \"year\" \"mileage\" \"cc\" ...\n - attr(*, \"class\")= chr \"lm\"\n\n\n\nH.2.1 회귀계수의 추정과 결정계수\n함수 summary 는 각 계수의 추정값과 가설 \\(H_0: \\beta_i=0\\)에 대한 t-검정 결과를 보여준다. 또한 결정계수 \\(R^2\\)도 구해준다.\n\nsummary(fit0)\n\n\nCall:\nlm(formula = price ~ year + mileage + cc + automatic, data = usedcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-177.35  -63.91   -0.99   70.34  212.69 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.253e+02  3.998e+02   1.314 0.200823    \nyear        -5.800e+00  9.283e-01  -6.247 1.55e-06 ***\nmileage     -2.263e-03  7.211e-04  -3.138 0.004324 ** \ncc           3.888e-01  2.022e-01   1.923 0.065958 .  \nautomatic    1.653e+02  3.986e+01   4.147 0.000339 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 101.1 on 25 degrees of freedom\nMultiple R-squared:  0.9045,    Adjusted R-squared:  0.8892 \nF-statistic: 59.21 on 4 and 25 DF,  p-value: 2.184e-12\n\n\n각 회귀 계수에 대한 신뢰구간은 함수 confint로 구할 수 있다.\n\nconfint(fit0)\n\n                    2.5 %        97.5 %\n(Intercept) -2.981256e+02  1.348699e+03\nyear        -7.711605e+00 -3.887669e+00\nmileage     -3.748021e-03 -7.776672e-04\ncc          -2.763072e-02  8.052054e-01\nautomatic    8.322275e+01  2.474025e+02\n\n\n\n\nH.2.2 분산분석\n\nanova(fit0)\n\nAnalysis of Variance Table\n\nResponse: price\n          Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nyear       1 2056608 2056608 201.2036 1.841e-13 ***\nmileage    1  135864  135864  13.2919 0.0012228 ** \ncc         1   52409   52409   5.1273 0.0324794 *  \nautomatic  1  175828  175828  17.2018 0.0003389 ***\nResiduals 25  255538   10222                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nH.2.3 예측값\n반응변수에 대한 예측값 \\(\\hat {\\pmb y} = \\pmb X \\hat {\\pmb \\beta}\\)는 함수 predict를 이용한다.\n\npredict(fit0)\n\n        1         2         3         4         5         6         7         8 \n 713.0214 1167.3146  321.4025 1194.0114  643.4485 1042.5270  865.9501  559.1876 \n        9        10        11        12        13        14        15        16 \n1256.9013  351.5409  946.0553  623.6355  677.3900 1236.5788  991.9617 1007.3483 \n       17        18        19        20        21        22        23        24 \n 709.6348 1142.6549  890.3836 1029.0340  808.9611  643.6167  611.6964  182.7813 \n       25        26        27        28        29        30 \n 960.9247  614.4275  924.2101  872.9584  265.3927  884.0490 \n\n\n새로운 자료에 대한 예측값 \\(\\widehat { E(y|x)}\\)은 다음과 같이 데이터프레임을 만들고 예측한다.\n\nnw &lt;- data.frame(year=60, mileage=10000, cc=200, automatic=1)\nnw\n\n  year mileage  cc automatic\n1   60   10000 200         1\n\npredict(fit0, newdata=nw, interval=\"confidence\")\n\n       fit       lwr      upr\n1 397.7504 -342.6272 1138.128\n\n\n새로운 관측값에 대항 예측은 다음과 같이 한다.\n\npredict(fit0, newdata=nw, interval=\"prediction\")\n\n       fit       lwr      upr\n1 397.7504 -371.3501 1166.851",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>R-실습: 중회귀 모형 적합</span>"
    ]
  },
  {
    "objectID": "qmd/practice-01.html#잔차-분석",
    "href": "qmd/practice-01.html#잔차-분석",
    "title": "부록 H — R-실습: 중회귀 모형 적합",
    "section": "H.3 잔차 분석",
    "text": "H.3 잔차 분석\n\nplot(fit0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nH.3.1 제곱합의 종류\n\nH.3.1.1 순차제곱합\n순차제곱합은 모형에 들어가는 변수의 순서에 따라서 제곱합이 틀려진다.\n다음의 예를 보면 두 모형이 같은 변수들로 적합되지만 순서가 달라지면 순차제곱합이 다르다.\n\nmodel1 &lt;- price ~ year + mileage + cc + automatic\nmodel2 &lt;- price ~ mileage + automatic + cc + year\nfit1 &lt;- lm(model1, usedcars)\nfit2 &lt;- lm(model2, usedcars)\nanova(fit1)\n\nAnalysis of Variance Table\n\nResponse: price\n          Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nyear       1 2056608 2056608 201.2036 1.841e-13 ***\nmileage    1  135864  135864  13.2919 0.0012228 ** \ncc         1   52409   52409   5.1273 0.0324794 *  \nautomatic  1  175828  175828  17.2018 0.0003389 ***\nResiduals 25  255538   10222                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(fit2)\n\nAnalysis of Variance Table\n\nResponse: price\n          Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nmileage    1 1637355 1637355 160.1870 2.274e-12 ***\nautomatic  1  341741  341741  33.4335 5.006e-06 ***\ncc         1   42683   42683   4.1758   0.05168 .  \nyear       1  398929  398929  39.0283 1.552e-06 ***\nResiduals 25  255538   10222                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n하지만 회귀계수의 추정량은 동일하다.\n\nsummary(fit1)\n\n\nCall:\nlm(formula = model1, data = usedcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-177.35  -63.91   -0.99   70.34  212.69 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.253e+02  3.998e+02   1.314 0.200823    \nyear        -5.800e+00  9.283e-01  -6.247 1.55e-06 ***\nmileage     -2.263e-03  7.211e-04  -3.138 0.004324 ** \ncc           3.888e-01  2.022e-01   1.923 0.065958 .  \nautomatic    1.653e+02  3.986e+01   4.147 0.000339 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 101.1 on 25 degrees of freedom\nMultiple R-squared:  0.9045,    Adjusted R-squared:  0.8892 \nF-statistic: 59.21 on 4 and 25 DF,  p-value: 2.184e-12\n\nsummary(fit2)\n\n\nCall:\nlm(formula = model2, data = usedcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-177.35  -63.91   -0.99   70.34  212.69 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.253e+02  3.998e+02   1.314 0.200823    \nmileage     -2.263e-03  7.211e-04  -3.138 0.004324 ** \nautomatic    1.653e+02  3.986e+01   4.147 0.000339 ***\ncc           3.888e-01  2.022e-01   1.923 0.065958 .  \nyear        -5.800e+00  9.283e-01  -6.247 1.55e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 101.1 on 25 degrees of freedom\nMultiple R-squared:  0.9045,    Adjusted R-squared:  0.8892 \nF-statistic: 59.21 on 4 and 25 DF,  p-value: 2.184e-12\n\n\n\n\nH.3.1.2 편제곱합\n편제곱합은 다른 변수들로 보정된 제곱합으로 순서에 관계없이 일정하다.패키지 car 에 있는 함수 Anova 를 사용하면 편제곱합을 구할 수 있다.\n\nAnova(fit1, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: price\n            Sum Sq Df F value    Pr(&gt;F)    \n(Intercept)  17645  1  1.7262 0.2008228    \nyear        398929  1 39.0283 1.552e-06 ***\nmileage     100649  1  9.8467 0.0043244 ** \ncc           37794  1  3.6975 0.0659577 .  \nautomatic   175828  1 17.2018 0.0003389 ***\nResiduals   255538 25                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(fit2, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: price\n            Sum Sq Df F value    Pr(&gt;F)    \n(Intercept)  17645  1  1.7262 0.2008228    \nmileage     100649  1  9.8467 0.0043244 ** \nautomatic   175828  1 17.2018 0.0003389 ***\ncc           37794  1  3.6975 0.0659577 .  \nyear        398929  1 39.0283 1.552e-06 ***\nResiduals   255538 25                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>R-실습: 중회귀 모형 적합</span>"
    ]
  },
  {
    "objectID": "qmd/practice-01.html#부분-f-검정",
    "href": "qmd/practice-01.html#부분-f-검정",
    "title": "부록 H — R-실습: 중회귀 모형 적합",
    "section": "H.4 부분 F 검정",
    "text": "H.4 부분 F 검정\n배기량(cc)에 대항 계수가 0인지 검정해보자.\n\\[ H_0: ~ \\beta_k =0 \\]\n하나의 계수에 대한 검정은 분산분석 표의 t-검정으로도 가능하며 결과는 동일하다.\n\nfullmodel &lt;- price ~ year + mileage + cc + automatic\nreducemodel1 &lt;- price ~ year + mileage + automatic\nfitfull &lt;- lm(fullmodel, data=usedcars)\nfitreduce1 &lt;- lm(reducemodel1, data=usedcars)\nanova(fitreduce1, fitfull)\n\nAnalysis of Variance Table\n\nModel 1: price ~ year + mileage + automatic\nModel 2: price ~ year + mileage + cc + automatic\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     26 293332                              \n2     25 255538  1     37794 3.6975 0.06596 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n이제 두 개 이상 의 변수에 대하여 부분 F 검정을 해보자. 설명변수 cc 와 automatic에 대한 계수가 0인지 검정해보자.\n\\[ H_0: ~ \\beta_k = \\beta_l = 0 \\]\n\nreducemodel2 &lt;- price ~ year + mileage\nfitreduce2 &lt;- lm(reducemodel2, data=usedcars)\nanova(fitreduce2, fitfull)\n\nAnalysis of Variance Table\n\nModel 1: price ~ year + mileage\nModel 2: price ~ year + mileage + cc + automatic\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     27 483775                                  \n2     25 255538  2    228237 11.165 0.0003429 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>R-실습: 중회귀 모형 적합</span>"
    ]
  },
  {
    "objectID": "qmd/practice-01.html#선형-가설에-대한-검정",
    "href": "qmd/practice-01.html#선형-가설에-대한-검정",
    "title": "부록 H — R-실습: 중회귀 모형 적합",
    "section": "H.5 선형 가설에 대한 검정",
    "text": "H.5 선형 가설에 대한 검정\n다음과 같은 선형 가설을 생각자.\n\\[ H_0: \\pmb L \\pmb \\beta= \\pmb 0\\]\n교과서 예제 4.4 에서 다음과 같은 가설을 고려한다.\n\\[ H_0: \\beta_2=0, \\beta_3= 2.5 \\beta_4 \\]\n\nmodreduce &lt;- lm(suneung ~ kor + I(2.5*math + sci), data=suneung)\nmodfull &lt;- lm(suneung ~ kor + eng + math + sci, data=suneung)\nanova(modreduce, modfull)\n\nAnalysis of Variance Table\n\nModel 1: suneung ~ kor + I(2.5 * math + sci)\nModel 2: suneung ~ kor + eng + math + sci\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     22 3136.4                           \n2     20 3023.5  2    112.95 0.3736  0.693\n\n\n위의 검정은 다음과 과 같이 선형행렬 \\(L\\)을 정의하고 함수 car::linearHypothesis를 이용한 결과와 같다.\n\\[\nH_0:\n\\begin{bmatrix}\n0 & 0  & 1 & 0 & 0  \\\\\n0 & 0  & 0 & 1 &-2.5\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\beta_3 \\\\\n\\beta_4\n\\end{bmatrix}\n=\\pmb 0\n\\]\n\nL &lt;- matrix(c(0,0,1,0,0,0,0,0,1,-2.5),2,5, byrow=TRUE)\nL\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    1    0  0.0\n[2,]    0    0    0    1 -2.5\n\nlinearHypothesis(modfull, hypothesis.matrix=L)\n\nLinear hypothesis test\n\nHypothesis:\neng = 0\nmath - 2.5 sci = 0\n\nModel 1: restricted model\nModel 2: suneung ~ kor + eng + math + sci\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     22 3136.4                           \n2     20 3023.5  2    112.95 0.3736  0.693",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>R-실습: 중회귀 모형 적합</span>"
    ]
  },
  {
    "objectID": "qmd/practice-02.html",
    "href": "qmd/practice-02.html",
    "title": "부록 I — R-실습: 중회귀 모형 진단",
    "section": "",
    "text": "I.1 변수변환",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>R-실습: 중회귀 모형 진단</span>"
    ]
  },
  {
    "objectID": "qmd/practice-02.html#변수변환",
    "href": "qmd/practice-02.html#변수변환",
    "title": "부록 I — R-실습: 중회귀 모형 진단",
    "section": "",
    "text": "I.1.1 예제 4.8\n여기서 이용한 자료 bug 는 살충제의 독성실험에서 살충제에 노출된 벌레들의 생존개체수를 시간대별로 관측한 것이다.\n\ny :생존벌레의 수\ntime :시간(분)\n\n\nplot(y~time, regbook::bug)\n\n\n\n\n\n\n\n\n이제 로그변화을 고려해 보자.\n\nbug2 &lt;-regbook::bug\nbug2$logy &lt;- log(bug2$y)\nplot(logy~time, bug2)\n\n\n\n\n\n\n\n\n변환된 자료에 대한 회귀분석을 수행해 보자.\n\nfitlog &lt;- lm(logy~time, bug2)\nplot(logy~time, bug2)\nabline(fitlog)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>R-실습: 중회귀 모형 진단</span>"
    ]
  },
  {
    "objectID": "qmd/practice-02.html#box-cox-변환",
    "href": "qmd/practice-02.html#box-cox-변환",
    "title": "부록 I — R-실습: 중회귀 모형 진단",
    "section": "I.2 Box-Cox 변환",
    "text": "I.2 Box-Cox 변환\nBox-Cox 변환은 다음과 같이 수행한다. 패키지 MASS 의 함수 boxcox 를 이용한다.\n\nI.2.1 예제 4.10\n\nfoot:발길이(mm), 양말을 벗은 상태로 측정하였고 오른쪽 발만 측정하였다.\nforearm: 팔안쪽길이(mm), 손목부터 팔꿈치가 접히는 부분까지의 길이이다. 오른쪽 팔만 측정하였다.\n\n변환이 필요없는 경우에 대한 예제이다.\n\n# plot histogram of foot by ggplot2\naflength %&gt;% ggplot(aes(x=foot)) + geom_histogram(binwidth=15, fill=\"skyblue\", color=\"black\") + theme_minimal()\n\n\n\n\n\n\n\n\n\nplot(foot ~ forearm, data=aflength)\n\n\n\n\n\n\n\nex411 &lt;- boxcox(lm(foot ~ forearm, data=aflength))\n\n\n\n\n\n\n\n\n\n\nI.2.2 예제 4.11\n예제 4.11 자료 wool 는 Box & Cox의 1964년 논문에서 사용한 예제로, 양모의 강력을 알아보기 위해 \\(3^3\\) 요인실험을 수행한 결과이다.\n\ncycle :반응변수. 시편이 끊어질 때까지의 측정 횟수.\nlength :시편의 길이\nload : 시편에 가한 하중\namplitude :하중을 가한 폭\n\n반응변수 cycle 의 히스토그램능 보면 오른쪽으로 매우 치우친 분포로서 정규분포와 매우 다른 모양을 보인다.\n\n# plot histogram of foot by ggplot2\nwool %&gt;% ggplot(aes(x=cycle)) + geom_histogram(binwidth=200, fill=\"skyblue\", color=\"black\") + theme_minimal()\n\n\n\n\n\n\n\n\n잔차 분석의 결과를 보면 잔차에\n\nwoolfm1 &lt;- lm(cycle~length + amplitude + load, data=wool)\nsummary(woolfm1)\n\n\nCall:\nlm(formula = cycle ~ length + amplitude + load, data = wool)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-644.5 -279.1 -150.2  199.5 1268.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4521.370   1621.721   2.788 0.010454 *  \nlength        13.200      2.301   5.736 7.66e-06 ***\namplitude   -535.833    115.057  -4.657 0.000109 ***\nload         -62.167     23.011  -2.702 0.012734 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 488.1 on 23 degrees of freedom\nMultiple R-squared:  0.7291,    Adjusted R-squared:  0.6937 \nF-statistic: 20.63 on 3 and 23 DF,  p-value: 1.028e-06\n\n\n이제 Box-Cox 변환을 적용해 보자.\n\nboxcox(woolfm1)\n\n\n\n\n\n\n\n\n위의 결과에서 \\(\\lambda = 0\\)이 가장 좋은 변환으로 나타났다. 이는 로그 변환이 가장 적절하다는 의미이다. 이제 이 변환을 적용해 보자.\n\nwool$logcycle &lt;- log(wool$cycle)\nwoolfm2 &lt;- lm(logcycle~length + amplitude + load, data=wool)\nsummary(woolfm2)\n\n\nCall:\nlm(formula = logcycle ~ length + amplitude + load, data = wool)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43592 -0.11250  0.00802  0.11635  0.26790 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.551813   0.616683  17.111 1.41e-14 ***\nlength       0.016648   0.000875  19.025 1.43e-15 ***\namplitude   -0.630866   0.043752 -14.419 5.22e-13 ***\nload        -0.078524   0.008750  -8.974 5.66e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1856 on 23 degrees of freedom\nMultiple R-squared:  0.9658,    Adjusted R-squared:  0.9614 \nF-statistic: 216.8 on 3 and 23 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(woolfm2)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>R-실습: 중회귀 모형 진단</span>"
    ]
  },
  {
    "objectID": "qmd/practice-02.html#다중공선성",
    "href": "qmd/practice-02.html#다중공선성",
    "title": "부록 I — R-실습: 중회귀 모형 진단",
    "section": "I.3 다중공선성",
    "text": "I.3 다중공선성\n\nI.3.1 고유값과 고유벡터에 대한 이론\n선형모형 \\(E(\\pmb y | \\pmb X) = \\pmb X \\pmb \\beta\\) 에서 계획행렬 \\(\\pmb X\\)의 열들이 선형독립이 아닌 경우 다중공선성이 발생한다. 다중공선성은 계획행렬 \\(\\pmb X\\)의 열들이 선형종속인 경우에 발생한다.\n대칭행렬 \\(\\pmb X^t \\pmb  X\\)의 고유값 \\(\\lambda_i\\)와 그에 대응하는 고유벡터 \\(\\pmb  p_i\\)는 다음을 만족하는 실수와 벡터이다.\n\\[ (\\pmb X^t \\pmb  X ) \\pmb  p_i = \\lambda_i \\pmb  p_i \\]\n고유값 \\(\\lambda_i\\)을 구하는 방법은 다음의 방정식을 만족하는 해를 구하는 것이다.\n\\[ det \\left ( \\pmb X^t \\pmb  X - \\lambda_i \\pmb I \\right ) = 0\\]\n여기서 \\(det(\\pmb A)\\)는 행렬 \\(\\pmb A\\)의 행렬식을 의미한다.\n\\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{p}\\)를 \\(\\pmb X^t \\pmb X\\)의 고유값이라고 하자. \\(\\pmb X^t \\pmb X\\)의 각 고유값에 대한 정규직교 고유벡터(orthonormal eigenvector)를 \\(\\pmb  p_1, \\pmb  p_2,\\dots,\\pmb  p_{p}\\)라고 하자, 즉\n\\[ \\pmb  p_i^t \\pmb  p_i = 1 , \\quad \\pmb  p_i^t \\pmb  p_j = 0 \\quad (i \\ne j) \\]\n더 나아가 행렬 \\(\\pmb P\\)를 고유벡터를 모아놓은 행렬로 정의하자.\n\\[ \\pmb P=[\\pmb p_1 ~ \\pmb p_2 ~\\dots ~ \\pmb p_{p} ] \\]\n이때 \\(p \\times p\\) - 차원의 행렬 \\(\\pmb P\\)는 직교행렬이다.\n\\[ \\pmb P^t \\pmb P =\\pmb P  \\pmb P^t =\\pmb I \\]\n이제 다음과 같이 \\(\\pmb X^t \\pmb X\\)를 나타낼 수 있다.\n\\[ \\pmb P^t (\\pmb X^t \\pmb X) \\pmb P = \\text{diag}(\\lambda_1 , \\lambda_2 , \\dots , \\lambda_{p}) = \\pmb \\Lambda \\]\n또한\n\\[ \\pmb P^t (\\pmb X^t \\pmb X)^{-1} \\pmb P = \\text{diag} \\left (\\frac{1}{\\lambda_1} , \\frac{1}{\\lambda_2} , \\dots , \\frac{1}{\\lambda_{p}} \\right ) = \\pmb \\Lambda^{-1} \\]\n위의 식에서 알 수 있듯이 \\(1/\\lambda_i\\)는 \\((\\pmb X^t \\pmb X)^{-1}\\)의 고유값이다.\n행렬 \\(\\pmb P\\)가 직교행렬이기 때문에 다음과 같은 표현도 가능하다.\n\\[ (\\pmb X^t \\pmb X) =  \\pmb P \\pmb \\Lambda \\pmb P^t,\n\\quad (\\pmb X^t \\pmb X)^{-1} =  \\pmb P \\pmb \\Lambda^{-1} \\pmb P^t  \\]\n고유벡터와 고유값의 정의에 의하여 고유값 \\(\\lambda_k\\)이 매우 0에 가까우면 다음이 성립하고\n\\[ \\pmb p_k^t (\\pmb X^t \\pmb X) \\pmb p_k = (\\pmb X \\pmb p_k)^t ( \\pmb X \\pmb p_k) \\approx 0   \\] 위의 식은 다음과 같이 행렬 \\(\\pmb X\\)의 열들간에 선형관계 $ X p_k = $ 이 있다는 것을 의미한다.\n\\[  p_{1k} \\pmb x_1 +  p_{2k} \\pmb x_2 + \\dots  p_{p,k} \\pmb x_p \\approx 0 \\]\n위에서 \\(\\pmb p_k\\)와 \\(\\pmb X\\)는 다음과 같이 표시한다.\n\\[ \\pmb X=[\\pmb x_1~ \\pmb x_2~ \\dots~\\pmb x_{p}], \\quad\n\\pmb p_k =\n\\begin{bmatrix}\np_{1k} \\\\  \np_{2k} \\\\\n\\vdots \\\\\np_{p,k}\n\\end{bmatrix}\\]\n또한 회귀계수 벡터 \\(\\hat \\beta\\)의 공분산 행렬이 다음과 같이 주어지므로\n\\[\nCov(\\hat {\\pmb \\beta}) = \\sigma^2 (\\pmb X^t \\pmb X)^{-1} = \\sigma^2  \\pmb P \\pmb \\Lambda^{-1} \\pmb P^t\n\\tag{I.1}\\]\n다음과 같은 식이 성립한다.\n\\[\nvar(\\hat \\beta_k) / \\sigma^2 = \\frac{p^2_{k1}}{\\lambda_1} + \\frac{p^2_{k2}}{\\lambda_2} + \\dots \\frac{p^2_{k, p}}{\\lambda_{p}}\n\\tag{I.2}\\]\n\n\nI.3.2 고유값과 고유벡터에 대한 예제: 두 개의 독립변수\n이제 다음과 두 개의 독립변수가 있는 회귀 모형을 고려해 보자.\n\\[ y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + e_i, i=1,2,\\cdots,n \\]\n절편을 제외한 두 개의 표준화된 독립변수들로 이루어진 행렬을 \\(\\pmb X\\)로 표시하자.\n\\[  \\pmb X = [ \\pmb x_1 ~ \\pmb x_2 ]   \\]\n위에서 디자인 행렬 \\(\\pmb X\\)는 원래 독립변수의 디자인 행렬 \\(X\\)의 열들을 표준화한 변수로 구성된 것이다..\n\\[ \\sum_{i=1}^n x_{i1} =0, \\quad \\sum_{i=1}^n x_{i2} =0, \\quad \\sum_{i=1}^n x_{i1}^2 =1, \\quad \\sum_{i=1}^n x_{i2}^2 =1, \\quad \\sum_{i=1}^n x_{i1} x_{i2} =\\rho \\]\n이제 \\(\\pmb X^t \\pmb X\\)는 두 독립변수의 상관계수 행렬임을 알 수 있다.\n\\[  \\pmb X^t \\pmb X =\n\\begin{bmatrix}\n1  &  \\rho \\\\\n\\rho & 1\n\\end{bmatrix}\n=\\pmb R, \\quad\n0 &lt; \\rho &lt; 1\n\\]\n여기서 두 독립변수 \\(x_1\\)과 \\(x_2\\)의 상관계수 \\(\\rho\\)는 0보다 크다고 가정하자.\n이제 \\(\\pmb X^t \\pmb X\\)의 고유값(\\(\\lambda_i\\))과 고유벡터(\\(\\pmb p_i\\))는 다음과 같은 방정식을 만족하는 수 \\(\\lambda_i\\)와 벡터 \\(\\pmb p_i\\) 이다.\n\\[ (\\pmb X^t \\pmb X) \\pmb p_i = \\lambda_i \\pmb p_i, \\quad \\pmb p_i^t \\pmb p_i=1  \\]\n일단 먼저 고유값을 구하는 방법은 \\(det(\\pmb X^t \\pmb X - \\lambda_i \\pmb I ) =0\\)을 만족하는 값을 찾는 것이다. 여기서 \\(det(\\pmb A)\\)는 \\(\\pmb A\\)의 행렬식을 의미한다.\n\\[\ndet(\\pmb X^t \\pmb X - \\lambda_i \\pmb I ) = det \\left (\n\\begin{bmatrix}\n1-\\lambda_i  &  \\rho \\\\\n\\rho & 1-\\lambda_i\n\\end{bmatrix}\n\\right ) =0\n\\] 위의 방정식은 다음과 같이 요약할 수 있고\n\\[ \\lambda_i^2 -2 \\lambda_i + (1-\\rho^2) =0 \\]\n해는 다음과 같이 주어진다.\n\\[ \\lambda_1 = 1+ \\rho, \\quad \\lambda_2 = 1 -\\rho \\quad (\\lambda_1 \\ge \\lambda_2) \\]\n이제 각 고유값에 대한 고유벡터를 구해보자. 각 고유값 \\(\\lambda_i\\)에 대한 고유벡터를 \\(\\pmb p_i\\) 라고 하면\n\\[\n\\pmb p_1 =\n\\begin{bmatrix}\np_{11} \\\\\np_{21}\n\\end{bmatrix},\n~ p^2_{11}+p^2_{21}=1\n\\quad \\quad\n\\pmb p_2 =\n\\begin{bmatrix}\np_{12} \\\\\np_{22}\n\\end{bmatrix},~\np^2_{12}+p^2_{11}=1\n\\]\n다음과 같은 방정식을 만족해야 한다.\n\\[ (\\pmb X^t \\pmb X) \\pmb p_1 = \\lambda_1 \\pmb p_1 , \\quad  (\\pmb X^t \\pmb X) \\pmb p_2 = \\lambda_2 \\pmb p_2 \\]\n즉,\n\\[\n\\begin{bmatrix}\n1  &  \\rho \\\\\n\\rho & 1\n\\end{bmatrix}\n\\begin{bmatrix}\np_{11} \\\\\np_{21}\n\\end{bmatrix}\n=\n(1+ \\rho)\n\\begin{bmatrix}\np_{11} \\\\\np_{21}\n\\end{bmatrix}\n, \\quad\n\\begin{bmatrix}\n1  &  \\rho \\\\\n\\rho & 1\n\\end{bmatrix}\n\\begin{bmatrix}\np_{12} \\\\\np_{22}\n\\end{bmatrix}\n=\n(1- \\rho)\n\\begin{bmatrix}\np_{12} \\\\\np_{22}\n\\end{bmatrix}\n\\]\n위의 두 방정식은 정리하면 다음과 더 단순한 방정식을 얻는다.\n\\[  p_{11} -  p_{21} = 0, \\quad  p_{12}+ p_{22}=0 \\]\n이제 위의 식을 만족하고 길이가 1인 두 벡터를 찾으면 다음과 같은 두 개의 직교하고 길이가 1인 고유벡터 \\(\\pmb p_1\\)과 \\(\\pmb p_2\\)를 찾을 수 있다.\n\\[\n\\pmb p_1 =\n\\begin{bmatrix}\np_{11} \\\\\np_{21}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1/\\sqrt{2} \\\\\n1/\\sqrt{2}\n\\end{bmatrix},\n\\quad \\quad\n\\pmb p_2 =\n\\begin{bmatrix}\np_{12} \\\\\np_{22}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1/\\sqrt{2} \\\\\n-1/\\sqrt{2}\n\\end{bmatrix}\n\\]\n따라서 앞 절의 이론에서 나온 고유벡터로 구성된 행렬 \\(\\pmb P\\)와 고유값을 대각원소로 하는 행렬 \\(\\pmb \\Lambda\\)는 다음과 같다.\n\\[\n\\pmb P = [\\pmb p_1~ \\pmb p_2]\n= \\begin{bmatrix}\np_{11} &  p_{12}\\\\\np_{21} &  p_{22}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1/\\sqrt{2} &  1/\\sqrt{2}\\\\\n1/\\sqrt{2} &  -1/\\sqrt{2}\n\\end{bmatrix},\n\\quad \\quad\n\\pmb \\Lambda =\n\\begin{bmatrix}\n\\lambda_1 & 0 \\\\\n0 & \\lambda_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1+\\rho & 0 \\\\\n0 & 1-\\rho\n\\end{bmatrix}\n\\]\n이제 다음이 성립함을 확인할 수 있다.\n\\[ \\pmb P^t (\\pmb X^t \\pmb X) \\pmb P =  \\pmb \\Lambda, \\quad (\\pmb X^t \\pmb X)^{-1} =  \\pmb P \\pmb \\Lambda^{-1} \\pmb P^t   \\]\n즉,\n\\[\\begin{align*}\n\\pmb P^t (\\pmb X^t \\pmb X) \\pmb P\n& =  \n\\begin{bmatrix}\n1/\\sqrt{2} &  1/\\sqrt{2}\\\\\n1/\\sqrt{2} &  -1/\\sqrt{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1/\\sqrt{2} &  1/\\sqrt{2}\\\\\n1/\\sqrt{2} &  -1/\\sqrt{2}\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n1+\\rho & 0 \\\\\n0 & 1-\\rho\n\\end{bmatrix} \\\\\n&=\n\\pmb \\Lambda\n\\end{align*}\\]\n또한 다음도 성립함을 확인할 수 있다.\n\\[  (\\pmb X^t \\pmb X)^{-1} =  \\pmb P \\pmb \\Lambda^{-1} \\pmb P^t   \\]\n즉,\n\\[\n\\begin{aligned}\n(\\pmb X^t \\pmb X)^{-1} & =  \\pmb P \\pmb \\Lambda^{-1} \\pmb P^t \\\\   \n&  =\n\\begin{bmatrix}\np_{11} &  p_{12}\\\\\np_{21} &  p_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{1}{\\lambda_1} & 0 \\\\\n0 & \\frac{1}{\\lambda_2}\n\\end{bmatrix}\n\\begin{bmatrix}\np_{11} &  p_{21}\\\\\np_{12} &  p_{22}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n1/\\sqrt{2} &  1/\\sqrt{2}\\\\\n1/\\sqrt{2} &  -1/\\sqrt{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{1}{1+\\rho} & 0 \\\\\n0 & \\frac{1}{1-\\rho}\n\\end{bmatrix}\n\\begin{bmatrix}\n1/\\sqrt{2} &  1/\\sqrt{2}\\\\\n1/\\sqrt{2} &  -1/\\sqrt{2}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\np_{11}^2 \\frac{1}{\\lambda_1} +   p_{12}^2 \\frac{1}{\\lambda_2} &\np_{11}  p_{21} \\frac{1}{\\lambda_1} +  p_{12}  p_{22} \\frac{1}{\\lambda_2} \\\\\np_{11}  p_{21} \\frac{1}{\\lambda_1} +  p_{12}  p_{22} \\frac{1}{\\lambda_2}   &\np_{21}^2 \\frac{1}{\\lambda_1} +   p_{22}^2 \\frac{1}{\\lambda_2}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n(\\frac{1}{\\sqrt{2}})^2 \\frac{1}{1+\\rho} +  (\\frac{1}{\\sqrt{2}})^2 \\frac{1}{1-\\rho} &\n(\\frac{1}{\\sqrt{2}})^2 \\frac{1}{1+\\rho} + (\\frac{1}{\\sqrt{2}}) (-\\frac{1}{\\sqrt{2}}) \\frac{1}{1-\\rho} \\\\\n(\\frac{1}{\\sqrt{2}})^2 \\frac{1}{1+\\rho} + (\\frac{1}{\\sqrt{2}}) (-\\frac{1}{\\sqrt{2}}) \\frac{1}{1-\\rho}  &\n(\\frac{1}{\\sqrt{2}})^2 \\frac{1}{1+\\rho} +  (-\\frac{1}{\\sqrt{2}})^2 \\frac{1}{1-\\rho}\n\\end{bmatrix} \\\\\n& =\n\\frac{1}{1-\\rho^2}\n\\begin{bmatrix}\n1  & -\\rho \\\\\n-\\rho & 1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n앞 절에서 나온 회귀계수 추정량의 분산 공식 식 I.1 과 식 I.2 를 적용하면 다음과 같은 식을 얻을 수 있다.\n\\[\n\\begin{aligned}\nVar(\\hat \\beta_k)/\\sigma^2\n& = \\frac{p^2_{k1}}{\\lambda_1} + \\frac{p^2_{k2}}{\\lambda_2} \\\\\n& = \\frac{1}{2} \\left ( \\frac{1}{1+\\rho} + \\frac{1}{1-\\rho} \\right ) \\\\\n& = \\frac{1}{1-\\rho^2}\n\\end{aligned}\n\\]\n위의 분산 공식에서 제일 작은 두 번째 고유값 \\(\\lambda_2 = 1- \\rho\\)가 0에 가까우면 분산이 매우 커지는 것을 알 수 있다. 이 고유값은 상관계수 \\(\\rho\\)가 1에 가까울 수록 0에 가까워 진다.\n\n\nI.3.3 예제 4.13\n중고차 예제에서 가상의 변수를 만들어 적합할 때 완벽한 선형관계가 존재하면 적합 시 변수를 제거하는 것을 알 수 있다.\n\nusedcars2 &lt;- usedcars %&gt;%  mutate(ccmile = cc + mileage)\nfitcoll1 &lt;- lm(price ~ year + mileage + cc + automatic + ccmile, usedcars2)\nsummary(fitcoll1)\n\n\nCall:\nlm(formula = price ~ year + mileage + cc + automatic + ccmile, \n    data = usedcars2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-177.35  -63.91   -0.99   70.34  212.69 \n\nCoefficients: (1 not defined because of singularities)\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.253e+02  3.998e+02   1.314 0.200823    \nyear        -5.800e+00  9.283e-01  -6.247 1.55e-06 ***\nmileage     -2.263e-03  7.211e-04  -3.138 0.004324 ** \ncc           3.888e-01  2.022e-01   1.923 0.065958 .  \nautomatic    1.653e+02  3.986e+01   4.147 0.000339 ***\nccmile              NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 101.1 on 25 degrees of freedom\nMultiple R-squared:  0.9045,    Adjusted R-squared:  0.8892 \nF-statistic: 59.21 on 4 and 25 DF,  p-value: 2.184e-12\n\n\n\n\nI.3.4 예제 4.14\n모형을 적합해 보자.\n\nhald.lm &lt;- lm(y~ ., data=hald)\nsummary(hald.lm)\n\n\nCall:\nlm(formula = y ~ ., data = hald)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1750 -1.6709  0.2508  1.3783  3.9254 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  62.4054    70.0710   0.891   0.3991  \nx1            1.5511     0.7448   2.083   0.0708 .\nx2            0.5102     0.7238   0.705   0.5009  \nx3            0.1019     0.7547   0.135   0.8959  \nx4           -0.1441     0.7091  -0.203   0.8441  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.446 on 8 degrees of freedom\nMultiple R-squared:  0.9824,    Adjusted R-squared:  0.9736 \nF-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07\n\n\n상관계수 행렬의 고유값을 계산해 보자.\n\nR &lt;- cor(hald[2:5])\nR\n\n           x1         x2         x3         x4\nx1  1.0000000  0.2285795 -0.8241338 -0.2454451\nx2  0.2285795  1.0000000 -0.1392424 -0.9729550\nx3 -0.8241338 -0.1392424  1.0000000  0.0295370\nx4 -0.2454451 -0.9729550  0.0295370  1.0000000\n\nsolve(R)\n\n         x1        x2        x3       x4\nx1 38.49621  94.11969  41.88410  99.7858\nx2 94.11969 254.42317 105.09139 267.5394\nx3 41.88410 105.09139  46.86839 111.1451\nx4 99.78580 267.53942 111.14509 282.5129\n\ndiag(solve(R))\n\n       x1        x2        x3        x4 \n 38.49621 254.42317  46.86839 282.51286 \n\neigenval &lt;- eigen(R)$values\neigenval\n\n[1] 2.235704035 1.576066070 0.186606149 0.001623746\n\nsqrt(max(eigenval)/eigenval)\n\n[1]  1.000000  1.191022  3.461339 37.106342\n\n\nVIF를 구해보자.\n\ncar::vif(hald.lm)\n\n       x1        x2        x3        x4 \n 38.49621 254.42317  46.86839 282.51286 \n\nsummary(regbook::vif(hald.lm))\n\n\nVIF:\n    x1     x2     x3     x4 \n 38.50 254.42  46.87 282.51 \n\nVariance Proportion:\n  Eigenvalues Cond.Index          x1           x2          x3           x4\n1 2.235704035   1.000000 0.002632084 0.0005589686 0.001481988 0.0004753347\n2 1.576066070   1.191022 0.004269804 0.0004272931 0.004954638 0.0004572915\n3 0.186606149   3.461339 0.063519491 0.0020822791 0.046495910 0.0007243995\n4 0.001623746  37.106342 0.929578621 0.9969314592 0.947067464 0.9983429744\n\n\n\\(x_2\\)를 제외하고 분석해 보자.\n\nhald.lm2 &lt;- lm(y~ x1 + x3 + x4, data=hald)\nsummary(hald.lm2)\n\n\nCall:\nlm(formula = y ~ x1 + x3 + x4, data = hald)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9323 -1.8090  0.4806  1.1398  3.7771 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 111.68441    4.56248  24.479 1.52e-09 ***\nx1            1.05185    0.22368   4.702  0.00112 ** \nx3           -0.41004    0.19923  -2.058  0.06969 .  \nx4           -0.64280    0.04454 -14.431 1.58e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.377 on 9 degrees of freedom\nMultiple R-squared:  0.9813,    Adjusted R-squared:  0.975 \nF-statistic: 157.3 on 3 and 9 DF,  p-value: 4.312e-08\n\nsummary(regbook::vif(hald.lm2))\n\n\nVIF:\n   x1    x3    x4 \n3.678 3.460 1.181 \n\nVariance Proportion:\n  Eigenvalues Cond.Index           x1         x3         x4\n1   1.8683737   1.000000 0.0720157120 0.07053018 0.02229687\n2   0.9838532   1.378056 0.0002285765 0.02382939 0.79011946\n3   0.1477731   3.555775 0.9277557115 0.90564042 0.18758367",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>R-실습: 중회귀 모형 진단</span>"
    ]
  },
  {
    "objectID": "qmd/practice-03.html",
    "href": "qmd/practice-03.html",
    "title": "부록 J — R-실습: 관측값에 대한 진단",
    "section": "",
    "text": "J.1 교과서 예제 3.8\n자료 usedcars 에 대한 잔차 분석 (예제 3.8, 예제 5.3) 입니다.\n먼저 자료 usedcars 에서 주어진 모든 설명변수를 사용하여 중회귀모형을 적합해 봅니다.\nusedcars.lm &lt;- lm(price ~ year + mileage + cc + automatic, usedcars)\nsummary(usedcars.lm)\n\n\nCall:\nlm(formula = price ~ year + mileage + cc + automatic, data = usedcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-177.35  -63.91   -0.99   70.34  212.69 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.253e+02  3.998e+02   1.314 0.200823    \nyear        -5.800e+00  9.283e-01  -6.247 1.55e-06 ***\nmileage     -2.263e-03  7.211e-04  -3.138 0.004324 ** \ncc           3.888e-01  2.022e-01   1.923 0.065958 .  \nautomatic    1.653e+02  3.986e+01   4.147 0.000339 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 101.1 on 25 degrees of freedom\nMultiple R-squared:  0.9045,    Adjusted R-squared:  0.8892 \nF-statistic: 59.21 on 4 and 25 DF,  p-value: 2.184e-12",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>R-실습: 관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/practice-03.html#교과서-예제-3.8",
    "href": "qmd/practice-03.html#교과서-예제-3.8",
    "title": "부록 J — R-실습: 관측값에 대한 진단",
    "section": "",
    "text": "J.1.1 잔차그림\nR 에서는 plot 함수를 이용하여 기본적인 잔차그림을 그릴 수 있습니다.\n\n그림 Reidual vs Fitted 는\n그림 Normal Q-Q 는 잔차가 정규분포를 따르는지를 확인하는 그림이다.\n그림 Scale-Location 은 예측값 \\(\\hat y_i\\) 에 대한 표준화된 잔차 \\(r_i/\\sqrt{1-h_{ii}}\\) 를 그린 것이다.\n\n\n잔차 대 예측값(Residuals vs Fitted)\n\n\n이 그래프는 등분산성(homoscedasticity)과 선형성 가정을 확인하는 데 도움이 된다.\n각 예측값 \\(\\hat y_i\\) 에 대한 잔차 \\(r_i\\) 를 그린 것이다.\n이상적으로는, 잔차들이 수평축(0-라인) 주변에 무작위로 흩어져 있어야 하며, 이는 관계가 선형이고 오류 항의 분산이 일정함을 나타낸다.\n\n\n정규 Q-Q(Quantile-Quantile)\n\n\n정규 Q-Q 그래프는 잔차의 정규성을 확인하는 데 사용된다.\n점들이 제공된 직선을 따라 배치되어야 이상적이며, 이는 잔차가 정규 분포에 가깝다는 것을 나타낸다.\n주어진 선으로부터의 이탈은 정규성으로부터의 벗어남을 나타낸다.\n\n\n스케일-위치(Scale-Location) 또는 스프레드-위치(Spread-Location)\n\n\n잔차 대 예측값 그래프와 유사하게, 스케일-위치 그래프는 잔차의 퍼진 정도를 보여주므로 등분산성을 확인하는 데 사용된다.\n스케일-위치 그래프(또는 스프레드-위치 그래프)의 y축은 일반적으로 표준화된 잔차 절대값 의 제곱근을 나타낸다.\n잔차 절대값 의 제곱근을 보여주는 것은 잔차의 분산을 안정화하는 데 도움을 주어, 이질적 분산(등분산성이 아닌)의 패턴을 시각적으로 더 쉽게 식별할 수 있게 한다.\n점들이 대략적으로 수평선을 이루고 균등하게 퍼져 있어야 등분산성의 가정이 적합하다고 판단된다.\n\n\n잔차 대 지렛값(Residuals vs Leverage)\n\n\n이 그래프는 회귀선에 영향을 줄 수 있는 영향력 있는 관찰값을 식별하는 데 도움이 된다.\n높은 지렛값와 큰 잔차를 가진 관찰값(이상치)을 잘 파악할 수 있도록 만들어진 그림이다.\n이 그래프에서는 다음과 같은 통계량들이 제시되는 것에 유의하자.\n\nx축: 지렛값(leverage Values, \\(h_{ii}\\))\ny축: 표준화된 잔차(Standardized Residuals)\n등고선: Cook’s Distance\n\n\n\nplot(usedcars.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ.1.2 잔차\n다음과 같은 함수를 통하여 다양한 잔차들와 지렛값을 구할 수 있다.\n\nresid_inter &lt;- rstandard(usedcars.lm)  # internally studentized residual - 내표준화 잔차\nresid_exter &lt;- rstudent(usedcars.lm)   # externally studentized residual - 외표준화 잔차\nhatval &lt;- hatvalues(usedcars.lm)       # leverage value - 지렛값\ndata.frame(resid_inter , resid_exter, hatval)\n\n    resid_inter  resid_exter     hatval\n1   0.859118727  0.854468915 0.21455013\n2   2.223678962  2.432560486 0.10501551\n3  -0.553417436 -0.545588388 0.15599165\n4  -0.044084943 -0.043195925 0.18996253\n5  -0.576180936 -0.568325835 0.15814304\n6   0.816421059  0.810807796 0.11903880\n7  -0.551399974 -0.543574935 0.16470220\n8  -1.198080391 -1.209098031 0.18743332\n9   0.378399054  0.371820157 0.25147525\n10  0.745189938  0.738380670 0.17431785\n11 -0.010873657 -0.010653990 0.07847932\n12  1.537452127  1.583088042 0.11334881\n13 -0.706665346 -0.699408397 0.11029244\n14  1.286252868  1.304157036 0.23928783\n15  0.305838319  0.300221293 0.17774944\n16 -1.862061057 -1.965848316 0.11253640\n17 -0.425962246 -0.418878889 0.15297508\n18 -1.582023043 -1.634007936 0.08908012\n19 -1.128902196 -1.135412108 0.37287989\n20  0.746526192  0.739734875 0.11591279\n21 -0.739861832 -0.732982630 0.15005336\n22 -0.783820912 -0.777598695 0.13701582\n23  0.529387292  0.521623446 0.18549015\n24  1.320232140  1.341155689 0.22878139\n25 -0.009531759 -0.009339195 0.07924745\n26  0.414031097  0.407063963 0.27781733\n27  0.813419294  0.807745473 0.15066704\n28 -1.855054061 -1.957267375 0.14953912\n29  0.158642701  0.155515766 0.17056129\n30 -0.055408868 -0.054292716 0.18765466\n\n\n\n\nJ.1.3 영향점 측도\ninfluence.measures 함수를 통하여 영향점을 파악하는 진단값을 구할 수 있다.\n가장 중요한 통계량은 다음과 같다.\n\ndifft : DFFITS\ncook.d : Cook’s distance\nhat : leverage value\n\n\n# DFBETAS for each model variable, DFFITS, covariance ratios, \n# Cook's distances and the diagonal elements of the hat matrix\n# Cases which are influential with respect to any of these measures \n# are marked with an asterisk.\ninfluence.measures(usedcars.lm)\n\nInfluence measures of\n     lm(formula = price ~ year + mileage + cc + automatic, data = usedcars) :\n\n     dfb.1_  dfb.year  dfb.milg   dfb.cc dfb.atmt    dffit cov.r   cook.d\n1  -0.20716 -0.108544  0.327125  0.17932  0.19256  0.44658 1.344 4.03e-02\n2  -0.18773  0.033416 -0.347354  0.24746  0.23435  0.83326 0.455 1.16e-01\n3  -0.10302 -0.086990  0.008925  0.10950  0.06372 -0.23455 1.366 1.13e-02\n4   0.00469  0.016237 -0.011729 -0.00589 -0.00320 -0.02092 1.513 9.12e-05\n5   0.11228  0.102465 -0.135083 -0.12498  0.13462 -0.24632 1.363 1.25e-02\n6  -0.07885  0.136480 -0.181209  0.08714  0.11239  0.29805 1.216 1.80e-02\n7  -0.14228  0.100579 -0.106201  0.14682 -0.10174 -0.24137 1.381 1.20e-02\n8  -0.27687  0.308498 -0.320654  0.25704  0.24993 -0.58070 1.123 6.62e-02\n9   0.15471 -0.066016 -0.054370 -0.13883  0.03146  0.21551 1.592 9.62e-03\n10  0.13093 -0.056206  0.169173 -0.13765 -0.10220  0.33927 1.328 2.34e-02\n11  0.00143 -0.000670  0.000312 -0.00142 -0.00166 -0.00311 1.331 2.01e-06\n12 -0.27881  0.085189 -0.022194  0.31082 -0.33867  0.56603 0.842 6.04e-02\n13  0.10909 -0.027792  0.028700 -0.12774  0.15983 -0.24625 1.246 1.24e-02\n14  0.52930 -0.229975 -0.166587 -0.47750  0.11713  0.73144 1.145 1.04e-01\n15 -0.02434 -0.037299 -0.032166  0.04432 -0.10023  0.13959 1.464 4.04e-03\n16 -0.47183  0.092199  0.101866  0.45460 -0.29124 -0.70004 0.655 8.79e-02\n17  0.08120 -0.112348  0.030779 -0.06848 -0.09956 -0.17801 1.396 6.55e-03\n18  0.14607  0.138551  0.019412 -0.18052 -0.15882 -0.51098 0.795 4.90e-02\n19  0.08767 -0.454297  0.733107 -0.15988  0.36621 -0.87551 1.505 1.52e-01\n20  0.17302 -0.084980  0.007572 -0.16482  0.10281  0.26785 1.239 1.46e-02\n21  0.14757  0.081228 -0.204480 -0.13332 -0.13993 -0.30798 1.292 1.93e-02\n22 -0.21369 -0.011866  0.084126  0.19253  0.16353 -0.30984 1.255 1.95e-02\n23 -0.12798  0.071546  0.083597  0.10512  0.13711  0.24893 1.423 1.28e-02\n24  0.22299  0.430646 -0.103308 -0.26300 -0.09994  0.73047 1.108 1.03e-01\n25  0.00129  0.000354 -0.000686 -0.00128 -0.00137 -0.00274 1.332 1.56e-06\n26  0.06915  0.204983 -0.141142 -0.08585  0.12561  0.25248 1.641 1.32e-02\n27 -0.08134 -0.093035 -0.048156  0.12751 -0.25004  0.34021 1.263 2.35e-02\n28  0.28532 -0.571184  0.447521 -0.26551 -0.37866 -0.82073 0.688 1.21e-01\n29  0.02625  0.019365  0.010861 -0.02922 -0.01619  0.07052 1.471 1.04e-03\n30  0.00732  0.016726 -0.010214 -0.01018  0.01709 -0.02609 1.509 1.42e-04\n      hat inf\n1  0.2146    \n2  0.1050    \n3  0.1560    \n4  0.1900    \n5  0.1581    \n6  0.1190    \n7  0.1647    \n8  0.1874    \n9  0.2515    \n10 0.1743    \n11 0.0785    \n12 0.1133    \n13 0.1103    \n14 0.2393    \n15 0.1777    \n16 0.1125    \n17 0.1530    \n18 0.0891    \n19 0.3729    \n20 0.1159    \n21 0.1501    \n22 0.1370    \n23 0.1855    \n24 0.2288    \n25 0.0792    \n26 0.2778   *\n27 0.1507    \n28 0.1495    \n29 0.1706    \n30 0.1877    \n\n\n패키지 olsrr 의 함수 ols_plot_cooksd_bar 과 ols_plot_dffits 를 이용하여 각각 Cook’s distance 와 DFFIT 를 시각화할 수 있다.\n\nols_plot_cooksd_bar(usedcars.lm)\n\n\n\n\n\n\n\n\n\nols_plot_dffits(usedcars.lm)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>R-실습: 관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/practice-03.html#교과서-연습문제-5.14",
    "href": "qmd/practice-03.html#교과서-연습문제-5.14",
    "title": "부록 J — R-실습: 관측값에 대한 진단",
    "section": "J.2 교과서 연습문제 5.14",
    "text": "J.2 교과서 연습문제 5.14\n\nhead(MLB1)\n\n  hit85 hit86\n1 0.265 0.264\n2 0.309 0.296\n3 0.268 0.240\n4 0.243 0.229\n5 0.289 0.289\n6 0.266 0.286\n\n\n\nJ.2.1 회귀모형 적합\n\nmlb1.lm &lt;- lm(hit86 ~ hit85, data=MLB1)\nsummary(mlb1.lm)\n\n\nCall:\nlm(formula = hit86 ~ hit85, data = MLB1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11265 -0.01708 -0.00075  0.01887  0.05700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.09470    0.02310   4.100 7.49e-05 ***\nhit85        0.63383    0.08622   7.351 2.47e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02679 on 122 degrees of freedom\nMultiple R-squared:  0.307, Adjusted R-squared:  0.3013 \nF-statistic: 54.04 on 1 and 122 DF,  p-value: 2.471e-11\n\n\n\nggplot(MLB1, aes(x=hit85, y=hit86)) + geom_point() + labs(x = \"1985년 타율\", y = \"1986년 타율\") +\n  labs(title=\"1985년과 1986년 타율의 관계\") + \n  geom_line(aes(y=mlb1.lm$fitted.values), color=\"blue\")\n\n\n\n\n\n\n\n\n\n\nJ.2.2 잔차그림\n다음잔차 그림을 보면 다음과 같은 관측값이 이상점 또는 영향점일 가능성이 크다.\n\n잔차가 큰 관측값 번호: 71, 92, 106\n지렛값이 큰 관측값 번호 : 12\n\n\nmlb1.lm &lt;- lm(hit86 ~ hit85, data=MLB1)\nplot(mlb1.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ.2.3 잔차와 지렛값 분석\n아래 잔차와 지렛값을 보면 위에서 그림을 그려서 파악한 것과 일치하는 것을 확인할 수 있다.\n\nresid_inter &lt;- rstandard(mlb1.lm)  # internal studentized residual\nresid_exter &lt;- rstudent(mlb1.lm)   # external studentized residual\nhatval &lt;- hatvalues(mlb1.lm)       # leverage\nresid_df &lt;- data.frame(resid_inter , resid_exter, hatval)\nresid_df\n\n    resid_inter resid_exter      hatval\n1    0.05000699  0.04980213 0.008086100\n2    0.20604678  0.20523630 0.026823219\n3   -0.92071669 -0.92013786 0.008089609\n4   -0.74122875 -0.73985250 0.013757221\n5    0.41794462  0.41652651 0.013334552\n6    0.85074812  0.84977870 0.008066554\n7   -0.11752895 -0.11705291 0.021076586\n8    1.49950517  1.50730174 0.008822848\n9    0.47628862  0.47477421 0.022674209\n10   1.22323441  1.22575081 0.008655952\n11  -0.96655846 -0.96629590 0.016511088\n12   1.15252304  1.15408980 0.114892991\n13  -1.12220362 -1.12340818 0.011587908\n14   0.94814587  0.94775033 0.019727886\n15  -0.65464742 -0.65310705 0.056746543\n16   0.94426865  0.94384613 0.017044019\n17   0.29746497  0.29635083 0.014253231\n18  -0.64880397 -0.64725708 0.019043095\n19   0.35993460  0.35864690 0.031379027\n20   0.85838160  0.85744961 0.015865556\n21  -0.52346010 -0.52189678 0.028627834\n22   1.54555587  1.55450230 0.008269034\n23   0.60818462  0.60660721 0.010534910\n24  -0.84308211 -0.84207635 0.015369379\n25  -0.17556386 -0.17486494 0.058014551\n26  -0.21071097 -0.20988382 0.012441433\n27  -0.32339097 -0.32220100 0.012441433\n28   0.24968479  0.24872294 0.013281927\n29  -1.78885907 -1.80534673 0.009668354\n30   0.20995888  0.20913441 0.008187340\n31   1.54555587  1.55450230 0.008269034\n32  -0.40887179 -0.40747191 0.011587908\n33  -0.08987603 -0.08950989 0.008187340\n34   1.20706156  1.20934748 0.013334552\n35  -0.92405584 -0.92349840 0.016444430\n36   1.61388664  1.62469547 0.009194229\n37   1.08032096  1.08106766 0.012877634\n38   1.36629192  1.37121192 0.012393486\n39   0.47981611  0.47829711 0.025854922\n40   0.68662786  0.68513312 0.009668354\n41   0.28365424  0.28258253 0.010571163\n42  -0.89128320 -0.89052689 0.016511088\n43  -0.32339097 -0.32220100 0.012441433\n44   0.74859901  0.74724285 0.014253231\n45   0.71830103  0.71686860 0.008195527\n46  -0.66485460 -0.66332696 0.008509772\n47  -1.79282669 -1.80945914 0.008195527\n48   1.53177579  1.54036942 0.017113016\n49   0.17108078  0.17039863 0.010571163\n50   0.65981706  0.65828292 0.014769957\n51  -1.16526569 -1.16699260 0.031490124\n52  -0.08849917 -0.08813855 0.013281927\n53   0.28750459  0.28642092 0.011631177\n54  -0.14646855 -0.14587986 0.009447831\n55  -1.25801434 -1.26105391 0.008089609\n56  -1.25437758 -1.25736066 0.030515312\n57  -0.50371052 -0.50216434 0.010225342\n58   0.73588754  0.73449735 0.008655952\n59  -0.35597858 -0.35470091 0.008494569\n60  -0.45705663 -0.45556979 0.021076586\n61  -0.32779322 -0.32659089 0.013757221\n62  -1.33805577 -1.34244750 0.016444430\n63  -0.84844614 -0.84746568 0.021821186\n64   0.24777624  0.24682078 0.015369379\n65   0.51479711  0.51324069 0.009218787\n66   0.08012340  0.07979645 0.008638411\n67  -0.43719987 -0.43574587 0.008086100\n68   0.95954615  0.95923199 0.008187340\n69  -1.31293093 -1.31687542 0.008126362\n70   0.80156331  0.80038181 0.032372217\n71   1.98105733  2.00544148 0.042377621\n72   0.74550574  0.74414103 0.015865556\n73  -0.25726760 -0.25628058 0.008638411\n74  -0.07337522 -0.07307550 0.017735660\n75   0.68826166  0.68676971 0.020352702\n76  -0.21977971 -0.21892046 0.008638411\n77  -1.22738844 -1.22996533 0.028523754\n78  -0.85364557 -0.85269021 0.016511088\n79   0.36253805  0.36124383 0.019649534\n80   0.83049085  0.82942806 0.017735660\n81   0.97690284  0.97671857 0.008067723\n82   0.16870598  0.16803274 0.013812185\n83   0.48970050  0.48816941 0.008067723\n84  -1.04451327 -1.04490630 0.008638411\n85  -0.30800228 -0.30685670 0.015865556\n86  -0.40245640 -0.40106992 0.008509772\n87   0.88119841  0.88038574 0.011216193\n88   0.01518447  0.01512212 0.008988241\n89  -0.15011541 -0.14951273 0.008822848\n90   1.90682355  1.92793919 0.012877634\n91   0.23854907  0.23762483 0.009010460\n92  -4.25681260 -4.59422430 0.024271666\n93   1.68633876  1.69933527 0.014310534\n94  -0.90035488 -0.89965120 0.012025947\n95   0.32117873  0.31999502 0.010571163\n96  -1.64324327 -1.65491152 0.010534910\n97  -1.95470304 -1.97789425 0.019727886\n98   0.58748696  0.58590363 0.031379027\n99   0.42112866  0.41970434 0.008269034\n100 -1.72921841 -1.74361729 0.010903785\n101 -0.08361786 -0.08327684 0.023372533\n102 -0.30660015 -0.30545871 0.009968065\n103 -0.45847196 -0.45698295 0.008802968\n104  1.10252512  1.10350851 0.008988241\n105 -1.42714716 -1.43330075 0.012827348\n106  2.13685857  2.16906138 0.008822848\n107 -1.86838606 -1.88791915 0.010534910\n108 -1.67127243 -1.68379544 0.035476081\n109 -0.57561180 -0.57402789 0.017664324\n110 -1.06483510 -1.06542465 0.030515312\n111  0.70374907  0.70228584 0.008802968\n112 -0.67499469 -0.67348138 0.008822848\n113  0.70374907  0.70228584 0.008802968\n114  1.47948910  1.48681148 0.012441433\n115 -0.11903487 -0.11855291 0.009968065\n116  0.73105151  0.72964915 0.011216193\n117  0.67662401  0.67511317 0.012827348\n118 -0.91392269 -0.91330116 0.011631177\n119  0.17440445  0.17370986 0.070186094\n120 -0.46425912 -0.46276146 0.013281927\n121 -0.07146215 -0.07117016 0.009447831\n122 -0.26258954 -0.26158508 0.008822848\n123 -0.58988369 -0.58830072 0.008509772\n124  1.56476539  1.57421623 0.009218787\n\n\n\nresid_df %&gt;% dplyr::arrange(desc(abs(resid_inter))) %&gt;% head(10)\n\n    resid_inter resid_exter      hatval\n92    -4.256813   -4.594224 0.024271666\n106    2.136859    2.169061 0.008822848\n71     1.981057    2.005441 0.042377621\n97    -1.954703   -1.977894 0.019727886\n90     1.906824    1.927939 0.012877634\n107   -1.868386   -1.887919 0.010534910\n47    -1.792827   -1.809459 0.008195527\n29    -1.788859   -1.805347 0.009668354\n100   -1.729218   -1.743617 0.010903785\n93     1.686339    1.699335 0.014310534\n\n\n\nresid_df %&gt;% dplyr::arrange(desc(hatval)) %&gt;% head(10)\n\n    resid_inter resid_exter     hatval\n12    1.1525230   1.1540898 0.11489299\n119   0.1744045   0.1737099 0.07018609\n25   -0.1755639  -0.1748649 0.05801455\n15   -0.6546474  -0.6531071 0.05674654\n71    1.9810573   2.0054415 0.04237762\n108  -1.6712724  -1.6837954 0.03547608\n70    0.8015633   0.8003818 0.03237222\n51   -1.1652657  -1.1669926 0.03149012\n19    0.3599346   0.3586469 0.03137903\n98    0.5874870   0.5859036 0.03137903\n\n\n\n\nJ.2.4 영향점 측도\n아래 COOK 거리와 DFFIT 값을 보면 다음 관측값등이 회귀 적합에 큰 영향을 미치는 것으로 나타난다.\n\n92, 71, 12, 108, 97\n\n위에서 잔차와 지렛값만으로 파악한 것과 거의 일치하는 것을 확인할 수 있다.\n다만 차이가 나는 점은 다음과 같다.\n\n106 번 관측값이 잔차는 매우 크지만 지렛값이 상대적으로 작아서 COOK 거리와 DFFIT 값이 크게 나타나지 않는다는 것이다.\n97, 108 번 관측값은 잔차는 작지만 지렛값이 크기 때문에 COOK 거리와 DFFIT 값이 크게 나타난다는 것이다.\n\n92, 71, 12 관측값은 잔차와 지렛값이 모두 크기 때문에 COOK 거리와 DFFIT 값이 크게 나타난다.\n\ninfluence.measures(mlb1.lm)\n\nInfluence measures of\n     lm(formula = hit86 ~ hit85, data = MLB1) :\n\n       dfb.1_  dfb.ht85    dffit cov.r   cook.d     hat inf\n1    0.000699 -0.000232  0.00450 1.025 1.02e-05 0.00809    \n2   -0.026394  0.028494  0.03407 1.044 5.85e-04 0.02682    \n3   -0.004039 -0.004628 -0.08310 1.011 3.46e-03 0.00809    \n4   -0.062872  0.056210 -0.08738 1.022 3.83e-03 0.01376    \n5   -0.026354  0.030441  0.04842 1.027 1.18e-03 0.01333    \n6    0.009192 -0.001218  0.07663 1.013 2.94e-03 0.00807    \n7   -0.014528  0.013495 -0.01718 1.038 1.49e-04 0.02108    \n8   -0.027304  0.041692  0.14221 0.988 1.00e-02 0.00882    \n9   -0.053240  0.058048  0.07232 1.036 2.63e-03 0.02267    \n10  -0.018262  0.029939  0.11454 1.000 6.53e-03 0.00866    \n11   0.079949 -0.089550 -0.12520 1.018 7.84e-03 0.01651    \n12  -0.387291  0.400945  0.41580 1.124 8.62e-02 0.11489   *\n13  -0.077277  0.067073 -0.12164 1.007 7.38e-03 0.01159    \n14  -0.093863  0.103379  0.13445 1.022 9.05e-03 0.01973    \n15   0.141276 -0.148373 -0.16019 1.070 1.29e-02 0.05675   *\n16   0.098625 -0.090211  0.12429 1.019 7.73e-03 0.01704    \n17   0.026146 -0.023481  0.03564 1.030 6.40e-04 0.01425    \n18   0.061989 -0.068474 -0.09018 1.029 4.09e-03 0.01904    \n19   0.058748 -0.055642  0.06455 1.047 2.10e-03 0.03138    \n20   0.084010 -0.076341  0.10887 1.021 5.94e-03 0.01587    \n21   0.070568 -0.075934 -0.08960 1.042 4.04e-03 0.02863    \n22   0.036802 -0.022323  0.14195 0.985 9.96e-03 0.00827    \n23   0.035849 -0.030310  0.06259 1.021 1.97e-03 0.01053    \n24   0.064199 -0.072531 -0.10521 1.020 5.55e-03 0.01537    \n25  -0.041733  0.040267 -0.04340 1.079 9.49e-04 0.05801   *\n26   0.011921 -0.013973 -0.02356 1.029 2.80e-04 0.01244    \n27   0.018301 -0.021450 -0.03616 1.028 6.59e-04 0.01244    \n28   0.020330 -0.018086  0.02886 1.029 4.20e-04 0.01328    \n29  -0.089226  0.072652 -0.17838 0.973 1.56e-02 0.00967    \n30   0.004279 -0.002327  0.01900 1.024 1.82e-04 0.00819    \n31   0.036802 -0.022323  0.14195 0.985 9.96e-03 0.00827    \n32  -0.028029  0.024328 -0.04412 1.026 9.80e-04 0.01159    \n33  -0.001831  0.000996 -0.00813 1.025 3.33e-05 0.00819    \n34  -0.076515  0.088384  0.14059 1.006 9.85e-03 0.01333    \n35  -0.093489  0.085243 -0.11941 1.019 7.14e-03 0.01644    \n36   0.069829 -0.054861  0.15651 0.983 1.21e-02 0.00919    \n37  -0.064901  0.075488  0.12348 1.010 7.61e-03 0.01288    \n38   0.103195 -0.090783  0.15361 0.998 1.17e-02 0.01239    \n39   0.068818 -0.064637  0.07792 1.040 3.06e-03 0.02585    \n40   0.033861 -0.027572  0.06770 1.019 2.30e-03 0.00967    \n41  -0.011489  0.014223  0.02921 1.026 4.30e-04 0.01057    \n42   0.073680 -0.082528 -0.11539 1.020 6.67e-03 0.01651    \n43   0.018301 -0.021450 -0.03616 1.028 6.59e-04 0.01244    \n44   0.065925 -0.059208  0.08985 1.022 4.05e-03 0.01425    \n45  -0.001461  0.008239  0.06517 1.016 2.13e-03 0.00820    \n46   0.007749 -0.014057 -0.06145 1.018 1.90e-03 0.00851    \n47   0.003689 -0.020796 -0.16448 0.972 1.33e-02 0.00820    \n48  -0.132459  0.147796  0.20325 0.995 2.04e-02 0.01711    \n49  -0.006928  0.008577  0.01761 1.027 1.56e-04 0.01057    \n50   0.060215 -0.054307  0.08060 1.024 3.26e-03 0.01477    \n51   0.169415 -0.181494 -0.21043 1.026 2.21e-02 0.03149    \n52  -0.007204  0.006409 -0.01023 1.030 5.27e-05 0.01328    \n53  -0.014418  0.017206  0.03107 1.027 4.86e-04 0.01163    \n54   0.004051 -0.005452 -0.01425 1.026 1.02e-04 0.00945    \n55  -0.005535 -0.006343 -0.11388 0.998 6.45e-03 0.00809    \n56   0.178355 -0.191340 -0.22307 1.022 2.48e-02 0.03052    \n57  -0.028057  0.023463 -0.05104 1.023 1.31e-03 0.01023    \n58  -0.010943  0.017940  0.06863 1.016 2.36e-03 0.00866    \n59  -0.010679  0.007387 -0.03283 1.023 5.43e-04 0.00849    \n60  -0.056545  0.052524 -0.06685 1.035 2.25e-03 0.02108    \n61  -0.027754  0.024813 -0.03857 1.029 7.49e-04 0.01376    \n62  -0.135900  0.123913 -0.17358 1.003 1.50e-02 0.01644    \n63  -0.107969  0.100501 -0.12658 1.027 8.03e-03 0.02182    \n64  -0.018817  0.021259  0.03084 1.031 4.79e-04 0.01537    \n65  -0.012600  0.017518  0.04951 1.022 1.23e-03 0.00922    \n66   0.002659 -0.001920  0.00745 1.025 2.80e-05 0.00864    \n67  -0.006114  0.002033 -0.03934 1.022 7.79e-04 0.00809    \n68   0.019626 -0.010675  0.08715 1.010 3.80e-03 0.00819    \n69  -0.022710  0.010399 -0.11920 0.996 7.06e-03 0.00813    \n70   0.133778 -0.126857  0.14640 1.040 1.07e-02 0.03237    \n71  -0.358382  0.379615  0.42187 0.994 8.68e-02 0.04238   *\n72   0.072909 -0.066253  0.09448 1.024 4.48e-03 0.01587    \n73  -0.008540  0.006166 -0.02392 1.024 2.88e-04 0.00864    \n74   0.006522 -0.007251 -0.00982 1.035 4.86e-05 0.01774    \n75   0.082988 -0.076917  0.09899 1.030 4.92e-03 0.02035    \n76  -0.007295  0.005267 -0.02044 1.025 2.10e-04 0.00864    \n77  -0.189194  0.178493 -0.21076 1.021 2.21e-02 0.02852    \n78   0.070550 -0.079022 -0.11048 1.021 6.12e-03 0.01651    \n79   0.042469 -0.039270  0.05114 1.035 1.32e-03 0.01965    \n80  -0.074025  0.082301  0.11145 1.023 6.23e-03 0.01774    \n81   0.007426  0.001756  0.08809 1.009 3.88e-03 0.00807    \n82  -0.011176  0.012828  0.01989 1.030 1.99e-04 0.01381    \n83   0.003712  0.000878  0.04403 1.021 9.75e-04 0.00807    \n84  -0.034820  0.025141 -0.09754 1.007 4.75e-03 0.00864    \n85  -0.030065  0.027320 -0.03896 1.031 7.65e-04 0.01587    \n86   0.004686 -0.008499 -0.03716 1.023 6.95e-04 0.00851    \n87   0.057715 -0.049704  0.09377 1.015 4.40e-03 0.01122    \n88   0.000601 -0.000462  0.00144 1.026 1.05e-06 0.00899    \n89   0.002708 -0.004136 -0.01411 1.025 1.00e-04 0.00882    \n90  -0.115741  0.134624  0.22020 0.969 2.37e-02 0.01288    \n91  -0.005069  0.007342  0.02266 1.025 2.59e-04 0.00901    \n92   0.545385 -0.592108 -0.72460 0.755 2.25e-01 0.02427   *\n93  -0.118528  0.135273  0.20476 0.984 2.06e-02 0.01431    \n94   0.048192 -0.056968 -0.09926 1.015 4.93e-03 0.01203    \n95  -0.013010  0.016106  0.03308 1.026 5.51e-04 0.01057    \n96  -0.097802  0.082691 -0.17076 0.982 1.44e-02 0.01053    \n97   0.195887 -0.215746 -0.28059 0.973 3.84e-02 0.01973    \n98   0.095974 -0.090900  0.10546 1.044 5.59e-03 0.03138    \n99   0.009936 -0.006027  0.03832 1.022 7.39e-04 0.00827    \n100  0.076513 -0.093419 -0.18307 0.978 1.65e-02 0.01090    \n101 -0.011158  0.010426 -0.01288 1.041 8.37e-05 0.02337    \n102  0.010450 -0.013394 -0.03065 1.025 4.73e-04 0.00997    \n103 -0.016699  0.012473 -0.04307 1.022 9.33e-04 0.00880    \n104  0.043875 -0.033691  0.10509 1.005 5.51e-03 0.00899    \n105 -0.112509  0.099557 -0.16338 0.996 1.32e-02 0.01283    \n106 -0.039292  0.059996  0.20464 0.950 2.03e-02 0.00882   *\n107 -0.111573  0.094334 -0.19480 0.969 1.86e-02 0.01053    \n108 -0.298349  0.283857 -0.32292 1.006 5.14e-02 0.03548    \n109 -0.061854  0.056746 -0.07698 1.029 2.98e-03 0.01766    \n110  0.151129 -0.162132 -0.18902 1.029 1.78e-02 0.03052    \n111  0.025662 -0.019169  0.06618 1.017 2.20e-03 0.00880    \n112  0.012200 -0.018629 -0.06354 1.018 2.03e-03 0.00882    \n113  0.025662 -0.019169  0.06618 1.017 2.20e-03 0.00880    \n114 -0.084450  0.098983  0.16688 0.993 1.38e-02 0.01244    \n115  0.004056 -0.005198 -0.01190 1.027 7.13e-05 0.00997    \n116  0.047833 -0.041194  0.07771 1.019 3.03e-03 0.01122    \n117  0.052994 -0.046894  0.07696 1.022 2.97e-03 0.01283    \n118  0.045973 -0.054864 -0.09908 1.015 4.91e-03 0.01163    \n119  0.046341 -0.044900  0.04773 1.093 1.15e-03 0.07019   *\n120 -0.037825  0.033650 -0.05369 1.027 1.45e-03 0.01328    \n121  0.001976 -0.002660 -0.00695 1.026 2.44e-05 0.00945    \n122  0.004739 -0.007235 -0.02468 1.024 3.07e-04 0.00882    \n123  0.006873 -0.012467 -0.05450 1.019 1.49e-03 0.00851    \n124 -0.038647  0.053732  0.15185 0.985 1.14e-02 0.00922    \n\n\n\ndata.frame(influence.measures(mlb1.lm)$infmat) %&gt;% arrange(desc(cook.d)) %&gt;% head(10)\n\n        dfb.1_   dfb.ht85      dffit     cov.r     cook.d        hat\n92   0.5453850 -0.5921082 -0.7245987 0.7553712 0.22537707 0.02427167\n71  -0.3583817  0.3796146  0.4218724 0.9943835 0.08683731 0.04237762\n12  -0.3872908  0.4009454  0.4158038 1.1236842 0.08621185 0.11489299\n108 -0.2983494  0.2838571 -0.3229242 1.0062796 0.05136735 0.03547608\n97   0.1958869 -0.2157455 -0.2805886 0.9731151 0.03844727 0.01972789\n56   0.1783549 -0.1913398 -0.2230737 1.0217219 0.02476301 0.03051531\n90  -0.1157414  0.1346236  0.2202043 0.9693882 0.02371680 0.01287763\n77  -0.1891943  0.1784931 -0.2107561 1.0207619 0.02211610 0.02852375\n51   0.1694152 -0.1814937 -0.2104279 1.0264159 0.02207447 0.03149012\n93  -0.1185276  0.1352728  0.2047561 0.9838364 0.02064312 0.01431053\n\n\n\nols_plot_cooksd_bar(mlb1.lm)\n\n\n\n\n\n\n\n\n\nols_plot_dffits(mlb1.lm)\n\n\n\n\n\n\n\n\n\n\nJ.2.5 결론\n위에서 나타난 관측값들을 나타내는 산점도를 다시 그려보자.\n\ninflunce_obs &lt;- c(92, 71, 12, 108, 97)\nMLB1$row_number &lt;- seq_len(nrow(MLB1))\n\nggplot(MLB1, aes(x=hit85, y=hit86)) + geom_point() + labs(x = \"1985년 타율\", y = \"1986년 타율\") +\n  labs(title=\"1985년과 1986년 타율의 관계\") + \n  geom_line(aes(y=mlb1.lm$fitted.values), color=\"blue\") + \n  geom_point(data = MLB1[influnce_obs, ], aes(x=hit85, y=hit86),\n                    color = \"red\", size = 3) +\n  geom_text(data = MLB1[influnce_obs, ], aes(x=hit85, y=hit86, label = row_number),\n                   color = \"black\", vjust = 1.5, hjust = 0.5)\n\n\n\n\n\n\n\n\n이제 영향점과 이상점을 제거한 후 회귀모형을 다시 적합해보자.\n\nMLB1_clean &lt;- MLB1 %&gt;% filter(!row_number %in% influnce_obs)\n\nmlb1_clean.lm &lt;- lm(hit86 ~ hit85, data=MLB1_clean)\nsummary(mlb1_clean.lm)\n\n\nCall:\nlm(formula = hit86 ~ hit85, data = MLB1_clean)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.051190 -0.016541 -0.000852  0.017538  0.056161 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.10198    0.02283   4.468 1.84e-05 ***\nhit85        0.61039    0.08577   7.117 9.55e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02385 on 117 degrees of freedom\nMultiple R-squared:  0.3021,    Adjusted R-squared:  0.2961 \nF-statistic: 50.65 on 1 and 117 DF,  p-value: 9.554e-11\n\n\n영향점과 이상점을 제거하기 전과 후의 회귀선을 그려서 비교해 보자.\n\n영향점과 이상점을 제거하기 전의 기울기의 추정치는 0.633832 이다.\n영향점과 이상점을 제거한 후의 기울기의 추정치는 0.6103926 이다.\n\n\np &lt;- ggplot(MLB1, aes(x=hit85, y=hit86)) + geom_point() + labs(x = \"1985년 타율\", y = \"1986년 타율\") +\n  labs(title=\"1985년과 1986년 타율의 관계\") + \n  geom_line(aes(y=mlb1.lm$fitted.values), color=\"blue\") + \n  geom_line(data = MLB1_clean, aes(y=mlb1_clean.lm$fitted.values), color=\"red\") +\n  #add label for two regression lines\n  annotate(\"text\", x = 0.25, y = 0.35, label = paste(\"Original: \", round(mlb1.lm$coef[2], 2)), color = \"blue\") +\n  annotate(\"text\", x = 0.25, y = 0.34, label = paste(\"Cleaned: \", round(mlb1_clean.lm$coef[2], 2)), color = \"red\")\n\np",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>R-실습: 관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/practice-04.html",
    "href": "qmd/practice-04.html",
    "title": "부록 K — R-실습: 모형의 선택",
    "section": "",
    "text": "K.1 AIC 와 BIC 계산\n통계모형을 선택하는 척도로서 가능도함수이론에 근거한 AIC(Akaike information criteria, 식 6.9 )와 베이지안 검정이론에 기초한 BIC(bayesian or schwartz information criteria, 식 6.8 )가 있으며 정의는 모형의 선택에 대한 절에 나타니 있다.\nAIC 와 BIC 는 함수 AIC 와 BIC 를 이용하면 구할 수 있다.\nfit1 &lt;- lm(price ~ ., data=houseprice)\nfit1\n\n\nCall:\nlm(formula = price ~ ., data = houseprice)\n\nCoefficients:\n(Intercept)          tax       ground        floor         year  \n    1.21874      0.05195      0.01159      0.34941     -0.21894  \n\nAIC(fit1)\n\n[1] 121.5611\n\nBIC(fit1)\n\n[1] 129.3361\n위의 AIC 와 BIC 에서 로그 가능도 함수 \\(\\ell(\\hat {\\pmb \\theta})\\) 는 함수 logLik 로 구할 수 있다.\n참고로 선형모형에 대한 AIC 와 BIC 는 식 6.10 에 의하여 다음과 같이 계산할 수 있다.\n\\[\n\\begin{aligned}\nAIC &= n\\log(2\\pi) + n + n \\log  \\frac{SSE_p}{n} + 2(p+1) \\\\\nBIC &= n\\log(2\\pi) + n + n \\log  \\frac{SSE_p}{n} + (\\log n) (p+1)\n\\end{aligned}\n\\]\n이제 회귀모형을 적합한 결과과 식 6.10 를 이용하여 직접 AIC 와 BIC 를 계산해보자.\nn &lt;- dim(houseprice)[1]\np &lt;- length(coef(fit1))\nsse &lt;- deviance(fit1)\nc(n,p,sse)\n\n[1] 27.00000  5.00000 91.44876\n\naic1 &lt;- -2*logLik(fit1) + 2*(p+1)\nbic1 &lt;- -2*logLik(fit1) + log(n)*(p+1) \nc(aic1, bic1)\n\n[1] 121.5611 129.3361\n\naic2 &lt;- n*log(2*pi) + n + n*log(deviance(fit1)/n)  + 2*(p+1) \nbic2 &lt;- n*log(2*pi) + n + n*log(deviance(fit1)/n)  + log(n)*(p+1)\nc(aic2, bic2)\n\n[1] 121.5611 129.3361",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>R-실습: 모형의 선택</span>"
    ]
  },
  {
    "objectID": "qmd/practice-04.html#예제-6.1",
    "href": "qmd/practice-04.html#예제-6.1",
    "title": "부록 K — R-실습: 모형의 선택",
    "section": "K.2 예제 6.1",
    "text": "K.2 예제 6.1\n\nK.2.1 변수선택의 통계량\n패키지 leaps 에 수록된 regsubsets() 함수를 이용하면 가능한 모든 모형에 대한 중요한 선택 기준들이 게산된다.\n\nrss: 잔차 제곱합\nrsq: \\(R^2\\)\nadjr2 : 수정된 \\(R^2\\)\ncp : 맬로우의 \\(C_p\\)\nbic : BIC\n\n교재 예제에 나타난 자료 houseprice는 독립변수의 수가 4개이므로 가능한 회귀식의 개수가 \\(2^4=16\\)개이므로 이러한 방법을 적용할 수 있다. 하지만 독립변수의 수가 10개만 되면 가능한 회귀식의 수가 1024개나 되고 20개가 모형의 수가 100만개가 넘으므로 이들 모두에 대한 통계량을 계산하는 것은 실제로 쉽지않다.\n이제 예제 자료 houseprice에 대하여 regsubsets함수를 사용하여 가능한 회귀식을 모두 적합하고 각 식에 대한 모형선택의 기준값을 계산해보자. regsubsets함수에서 nbest=6는 독립변수의 수가 같은 모형들 중에서 가장 좋은 6개의 모형만을 보여주라는 명령문이다.\n모든 가능한 회귀식에 대한 통계량은 summaryf함수를 통하여 볼 수 있다.\n\nhouseprice.rgs &lt;- regsubsets(price ~ . , data=houseprice, nbest=6)\nsummaryf(houseprice.rgs)\n\n         tax ground floor year        rss        rsq      adjr2         cp\n1  ( 1 )                *       182.66315 0.86271916 0.85722792  20.943616\n1  ( 2 )   *                    215.97764 0.83768158 0.83118885  28.958146\n1  ( 3 )          *             628.61671 0.52756188 0.50866435 128.227503\n1  ( 4 )                     * 1202.47186 0.09627993 0.06013112 266.280912\n2  ( 1 )   *            *        95.19550 0.92845564 0.92249361   1.901360\n2  ( 2 )                *    *  153.80917 0.88440442 0.87477146  16.002160\n2  ( 3 )          *     *       168.58785 0.87329747 0.86273892  19.557496\n2  ( 4 )   *      *             191.93607 0.85575007 0.84372925  25.174420\n2  ( 5 )   *                 *  214.64151 0.83868576 0.82524290  30.636710\n2  ( 6 )          *          *  626.80968 0.52891996 0.48966329 129.792782\n3  ( 1 )   *            *    *   92.31936 0.93061720 0.92156727   3.209442\n3  ( 2 )   *      *     *        93.26197 0.92990878 0.92076645   3.436208\n3  ( 3 )          *     *    *  150.10029 0.88719183 0.87247773  17.109908\n3  ( 4 )   *      *          *  187.51305 0.85907420 0.84069258  26.110366\n4  ( 1 )   *      *     *    *   91.44876 0.93127151 0.91877542   5.000000\n                bic\n1  ( 1 ) -47.022941\n1  ( 2 ) -42.499601\n1  ( 3 ) -13.654236\n1  ( 4 )   3.858312\n2  ( 1 ) -61.323304\n2  ( 2 ) -48.369244\n2  ( 3 ) -45.892146\n2  ( 4 ) -42.390102\n2  ( 5 ) -39.371316\n2  ( 6 ) -10.436125\n3  ( 1 ) -58.855794\n3  ( 2 ) -58.581513\n3  ( 3 ) -45.732450\n3  ( 4 ) -39.723741\n4  ( 1 ) -55.815783\n\n\n위의 결과를 보면 두 개의 변수 tax와 floor가 포함된 회귀식이 다음과 같은 통계량으로 가장 좋은 모형으로 나타난다.\n\nrss: 잔차제곱합(residual sum of square), \\(SSE = 95.20\\)\nrsq: 결정계수(\\(R^2_p\\)), \\(R^2=0.92846\\)\nadjr2: 수정된 결정계수(\\(R^2_{ap}\\)), \\(R^2_a=0.92249\\)\ncp: 맬로우즈 \\(C_p\\), \\(C_p = 1.901\\)\nbic: BIC(Bayesian Information Criteria), \\(BIC= -61.323\\)\n\n패키지 olsrr에 있는 함수 ols_step_all_possible()을 이용하면 가능한 모든 회귀모형에 대한 통계량을 구하고 여러 가지 통계량에 대한 그림을 plot() 함수를 이용하여 쉽게 그릴 수 있다.\n\nfit1 &lt;- lm(price ~ ., data=houseprice)\nhouseprice.rgs2 &lt;- ols_step_all_possible(fit1)\nhouseprice.rgs2 \n\n   Index N            Predictors   R-Square Adj. R-Square Mallow's Cp\n3      1 1                 floor 0.86271916    0.85722792  0.83399320\n1      2 1                   tax 0.83768158    0.83118885  0.76244434\n2      3 1                ground 0.52756188    0.50866435  0.36958913\n4      4 1                  year 0.09627993    0.06013112 -0.07517657\n6      5 2             tax floor 0.92845564    0.92249361  0.89147659\n10     6 2            floor year 0.88440442    0.87477146  0.84393314\n8      7 2          ground floor 0.87329747    0.86273892  0.82431609\n5      8 2            tax ground 0.85575007    0.84372925  0.73078316\n7      9 2              tax year 0.83868576    0.82524290  0.74339806\n9     10 2           ground year 0.52891996    0.48966329  0.31506912\n13    11 3        tax floor year 0.93061720    0.92156727  0.88456609\n11    12 3      tax ground floor 0.92990878    0.92076645  0.88037155\n14    13 3     ground floor year 0.88719183    0.87247773  0.82472001\n12    14 3       tax ground year 0.85907420    0.84069258  0.71427919\n15    15 4 tax ground floor year 0.93127151    0.91877542  0.87187470\n\n\n\nhouseprice.rgs2$result %&gt;% dplyr::select(mindex, n, predictors, rsquare, adjr, cp, aic, sbc) %&gt;% dplyr::arrange(sbc)\n\n   mindex n            predictors    rsquare       adjr         cp      aic\n1       5 2             tax floor 0.92845564 0.92249361   1.901360 118.6453\n2      11 3        tax floor year 0.93061720 0.92156727   3.209442 119.8169\n3      12 3      tax ground floor 0.92990878 0.92076645   3.436208 120.0912\n4      15 4 tax ground floor year 0.93127151 0.91877542   5.000000 121.5611\n5       6 2            floor year 0.88440442 0.87477146  16.002160 131.5993\n6       1 1                 floor 0.86271916 0.85722792  20.943616 134.2415\n7       7 2          ground floor 0.87329747 0.86273892  19.557496 134.0764\n8      13 3     ground floor year 0.88719183 0.87247773  17.109908 132.9403\n9       2 1                   tax 0.83768158 0.83118885  28.958146 138.7648\n10      8 2            tax ground 0.85575007 0.84372925  25.174420 137.5785\n11     14 3       tax ground year 0.85907420 0.84069258  26.110366 138.9490\n12      9 2              tax year 0.83868576 0.82524290  30.636710 140.5973\n13      3 1                ground 0.52756188 0.50866435 128.227503 167.6102\n14     10 2           ground year 0.52891996 0.48966329 129.792782 169.5324\n15      4 1                  year 0.09627993 0.06013112 266.280912 185.1227\n        sbc\n1  123.8286\n2  126.2961\n3  126.5704\n4  129.3361\n5  136.7827\n6  138.1290\n7  139.2598\n8  139.4195\n9  142.6523\n10 142.7618\n11 145.4282\n12 145.7806\n13 171.4977\n14 174.7158\n15 189.0102\n\n\n\nplot(houseprice.rgs2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPRESS 잔차는 함수 press() 로 구할 수 있다.\n\\[ \\text{PRESS}_p = \\sum_{i=1}^n ( y_i - \\hat y_{i(i)})^2 \\]\n또한 교과서 264 페이지에 나온 교차확인(cross validation)에 의거한 \\(R^2_{pred}\\) 은 다음과 같이 계산되며 press() 함수에 의해 주어진다.\n\\[ R^2_{pred} = 1 - \\frac{ \\text{PRESS}_p}{\\text{SST}} \\]\n\npress(houseprice.rgs)\n\n         tax ground floor year     PRESS pred.r.squared\n1  ( 1 )                *       220.8853      0.8339932\n1  ( 2 )   *                    316.0868      0.7624443\n1  ( 3 )          *             838.8121      0.3695891\n1  ( 4 )                     * 1430.6084      0.0000000\n2  ( 1 )   *            *       144.3991      0.8914766\n2  ( 2 )                *    *  207.6594      0.8439331\n2  ( 3 )          *     *       233.7615      0.8243161\n2  ( 4 )   *      *             358.2145      0.7307832\n2  ( 5 )   *                 *  341.4294      0.7433981\n2  ( 6 )          *          *  911.3553      0.3150691\n3  ( 1 )   *            *    *  153.5941      0.8845661\n3  ( 2 )   *      *     *       159.1752      0.8803716\n3  ( 3 )          *     *    *  233.2240      0.8247200\n3  ( 4 )   *      *          *  380.1744      0.7142792\n4  ( 1 )   *      *     *    *  170.4810      0.8718747\n\n\n\n\nK.2.2 이상치를 제거한 경우\n교과서에서 분석하였듯이 9,10,27 번째 자료가 이상점 또는 영향점일 가능성이 높으므로 이를 제거하고 모형의 선택 기준을 다시 계산해 보자.\n\nhousepriceEX &lt;- houseprice[-c(9,10,27),]\nhousepriceEX.lm &lt;- lm(price ~ . , data=housepriceEX)\nsummary(housepriceEX.lm)\n\n\nCall:\nlm(formula = price ~ ., data = housepriceEX)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4130 -1.0459 -0.2558  0.8593  2.8197 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  6.32538    2.14531   2.948  0.00825 **\ntax          0.06589    0.01874   3.516  0.00231 **\nground       0.00974    0.02168   0.449  0.65835   \nfloor        0.08578    0.09057   0.947  0.35547   \nyear        -0.11142    0.26781  -0.416  0.68205   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.543 on 19 degrees of freedom\nMultiple R-squared:  0.7819,    Adjusted R-squared:  0.7359 \nF-statistic: 17.02 on 4 and 19 DF,  p-value: 4.404e-06\n\n\n이상치를 제거하면 tax 만 포함된 모형이 최적모형으로 나타난다. 이상치와 영항점이 모형의 선택에도 영향을 주는 점에 꼭 유의하자.\n\nhousepriceEX.rgs &lt;- regsubsets(price ~ . , data=housepriceEX, nbest=6)\nsummaryf(housepriceEX.rgs)\n\n         tax ground floor year       rss       rsq     adjr2         cp\n1  ( 1 )   *                    48.17790 0.7675502 0.7569843  0.2458362\n1  ( 2 )                *      102.73566 0.5043188 0.4817878 23.1726808\n1  ( 3 )          *            125.87070 0.3926964 0.3650917 32.8947363\n1  ( 4 )                     * 174.34503 0.1588164 0.1205808 53.2651405\n2  ( 1 )   *            *       46.18667 0.7771576 0.7559345  1.4090560\n2  ( 2 )   *      *             47.39011 0.7713512 0.7495751  1.9147822\n2  ( 3 )   *                 *  48.11577 0.7678500 0.7457405  2.2197243\n2  ( 4 )                *    *  83.74651 0.5959380 0.5574559 17.1928590\n2  ( 5 )          *     *       85.27968 0.5885408 0.5493542 17.8371405\n2  ( 6 )          *          * 116.96847 0.4356480 0.3819002 31.1537464\n3  ( 1 )   *      *     *       45.62514 0.7798669 0.7468469  3.1730851\n3  ( 2 )   *            *    *  45.69348 0.7795371 0.7464677  3.2018056\n3  ( 3 )   *      *          *  47.34784 0.7715551 0.7372884  3.8970174\n3  ( 4 )          *     *    *  74.63053 0.6399210 0.5859092 15.3620431\n4  ( 1 )   *      *     *    *  45.21326 0.7818541 0.7359287  5.0000000\n                bic\n1  ( 1 ) -28.661838\n1  ( 2 ) -10.487628\n1  ( 3 )  -5.613326\n1  ( 4 )   2.205420\n2  ( 1 ) -26.496810\n2  ( 2 ) -25.879469\n2  ( 3 ) -25.514758\n2  ( 4 ) -12.214327\n2  ( 5 ) -11.778929\n2  ( 6 )  -4.195690\n3  ( 1 ) -23.612331\n3  ( 2 ) -23.576407\n3  ( 3 ) -22.722834\n3  ( 4 ) -11.802150\n4  ( 1 ) -20.651921\n\n\n\nhousepriceEX.rgs2 &lt;- ols_step_all_possible(lm(price ~ ., data=housepriceEX))\nhousepriceEX.rgs2$result %&gt;% as.data.frame() %&gt;% dplyr::select(mindex, n, predictors, rsquare, adjr, cp, aic, sbc) %&gt;% dplyr::arrange(sbc)\n\n   mindex n            predictors   rsquare      adjr         cp       aic\n1       1 1                   tax 0.7675502 0.7569843  0.2458362  90.83337\n2       5 2             tax floor 0.7771576 0.7559345  1.4090560  91.82034\n3       6 2            tax ground 0.7713512 0.7495751  1.9147822  92.43768\n4       7 2              tax year 0.7678500 0.7457405  2.2197243  92.80240\n5      11 3      tax ground floor 0.7798669 0.7468469  3.1730851  93.52677\n6      12 3        tax floor year 0.7795371 0.7464677  3.2018056  93.56269\n7      13 3       tax ground year 0.7715551 0.7372884  3.8970174  94.41627\n8      15 4 tax ground floor year 0.7818541 0.7359287  5.0000000  95.30913\n9       8 2            floor year 0.5959380 0.5574559 17.1928590 106.10283\n10     14 3     ground floor year 0.6399210 0.5859092 15.3620431 105.33695\n11      9 2          ground floor 0.5885408 0.5493542 17.8371405 106.53823\n12      2 1                 floor 0.5043188 0.4817878 23.1726808 109.00758\n13      3 1                ground 0.3926964 0.3650917 32.8947363 113.88188\n14     10 2           ground year 0.4356480 0.3819002 31.1537464 114.12146\n15      4 1                  year 0.1588164 0.1205808 53.2651405 121.70063\n         sbc\n1   94.36753\n2   96.53256\n3   97.14990\n4   97.51461\n5   99.41704\n6   99.45296\n7  100.30654\n8  102.37745\n9  110.81504\n10 111.22722\n11 111.25044\n12 112.54174\n13 117.41604\n14 118.83368\n15 125.23479\n\n\n\npress(housepriceEX.rgs)\n\n         tax ground floor year     PRESS pred.r.squared\n1  ( 1 )   *                    56.10399     0.72930829\n1  ( 2 )                *      117.36430     0.43373824\n1  ( 3 )          *            157.83378     0.23848023\n1  ( 4 )                     * 202.40162     0.02344834\n2  ( 1 )   *            *       57.66354     0.72178375\n2  ( 2 )   *      *             64.24819     0.69001399\n2  ( 3 )   *                 *  63.42708     0.69397570\n2  ( 4 )                *    * 108.09369     0.47846724\n2  ( 5 )          *     *      114.27185     0.44865875\n2  ( 6 )          *          * 160.37982     0.22619605\n3  ( 1 )   *      *     *       68.98320     0.66716837\n3  ( 2 )   *            *    *  65.44694     0.68423019\n3  ( 3 )   *      *          *  72.26745     0.65132247\n3  ( 4 )          *     *    * 109.76504     0.47040332\n4  ( 1 )   *      *     *    *  79.24660     0.61764930",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>R-실습: 모형의 선택</span>"
    ]
  },
  {
    "objectID": "qmd/practice-04.html#변수선택-방법",
    "href": "qmd/practice-04.html#변수선택-방법",
    "title": "부록 K — R-실습: 모형의 선택",
    "section": "K.3 변수선택 방법",
    "text": "K.3 변수선택 방법\n회귀식에서 변수를을 선택할 때 가장 큰 모형을 적합시키면 각 변수에 대한 중요도를 각 회귀계수에 대한 가설 검정 \\(H_o: \\beta_i=0\\)에 대한 t-통계량을 보고 판단할 수 있다.\n다시 모든 자료를 고려한 houseprice에 대하여 가장 큰 모형을 적합시킨 후 각 변수에 대한 t-검정을 실시해보자.\n\nsummary(fit1)\n\n\nCall:\nlm(formula = price ~ ., data = houseprice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4891 -1.3574  0.1337  1.0686  3.4938 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.21874    2.04661   0.595  0.55759    \ntax          0.05195    0.01383   3.756  0.00109 ** \nground       0.01159    0.02534   0.458  0.65169    \nfloor        0.34941    0.07268   4.807 8.41e-05 ***\nyear        -0.21894    0.33149  -0.660  0.51582    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.039 on 22 degrees of freedom\nMultiple R-squared:  0.9313,    Adjusted R-squared:  0.9188 \nF-statistic: 74.53 on 4 and 22 DF,  p-value: 1.817e-12\n\n\n위는 4개의 독립변수를 포함한 가장 큰 모형에 대하여 각 계수의 유의성 검정에 대한 결과이다. 변수 floor가 가장 유의한 변수이고 다음으로 tax가 유의함을 알 수 있다. 또한 2개의 변수 year와 ground는 유의하지 않음을 알 수 있다.\n이렇게 독립변수들은 반응변수를 설명하는 정도가 다르므로 모든 가능한 회귀식을 적합하여 모형을 선택하는 것보다 변수의 중요도를 고려하여 변수들을 유의한 정도에 따라 차례로 모형에 포함시키거나 제거하는 절차가 더 효율적이다. 이렇게 가장 단순한 모형(평균모형)에서 시작하여 설명력이 높은 변수들을 순차적으로 포함시키거나(forward selection; 전진선택) 가장 큰 모형(full model)에서 시작하여 설명력이 낮은 변수들을 차례로 제거하는 방법(backward elimination; 후방제거)을 단계별 회귀(stepwise regression)이라고 한다.\n단계별 회귀는 다음과 같은 세 가지 방법이 있다.\n\n전진선택 (forward selection)\n후방제거 (backward elimination)\n단계별 선택 (stepwise selection)\n\n단계별 선택은 전진선택과 후방제거를 결합한 형태로서 새로운 변수가 추가되는 경우마다(전진선택) 제거할 변수를 있는지 판단하여 유의하지 않다면 제거하는(후방제거) 방법이다.\n\nK.3.1 단계별 선택의 적용\n단계별 선택에서 전진선택과 후방제거는 add1()와 drop1()함수를 사용한다.\n\n평균모형\n\n\\[ y=\\beta_0 + \\epsilon \\]\n\nmodel0 &lt;- lm(price~1, houseprice)\nsummary(model0)\n\n\nCall:\nlm(formula = price ~ 1, data = houseprice)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.300 -4.275 -0.800  1.125 23.200 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   19.250      1.377   13.98 1.32e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.154 on 26 degrees of freedom\n\n\n\n첫번째 변수의 추가\n\n이제 가장 설명력있는 변수를 추가하는데 다음과 같은 2개의 모형을 비교하는 부분 F 검정을 이용하여 가장 유의한 변수를 추가한다.\n\\[ H_0: y=\\beta_0 + \\epsilon \\quad \\text{vs} \\quad H_1: y=\\beta_0 + \\beta_1 x_i + \\epsilon \\]\n\nadd1(model0, scope=~tax+ ground+ floor+ year, test=\"F\")\n\nSingle term additions\n\nModel:\nprice ~ 1\n       Df Sum of Sq     RSS     AIC  F value    Pr(&gt;F)    \n&lt;none&gt;              1330.58 107.233                       \ntax     1   1114.60  215.98  60.142 129.0183 2.310e-11 ***\nground  1    701.96  628.62  88.987  27.9170 1.792e-05 ***\nfloor   1   1147.92  182.66  55.619 157.1084 2.807e-12 ***\nyear    1    128.11 1202.47 106.500   2.6634    0.1152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n위의 결과에서 가장 유의한 변수가 floor이므로 회귀식에 추가한다. 변수를 추가하는 경우는 고려하는 변수를 포함한 모형과 포함하지 않는 모형이 가장 유의한 차이를 보이는 변수를 선택한다.\n\nmodel1 &lt;- update(model0, . ~ . + floor)\nsummary(model1)\n\n\nCall:\nlm(formula = price ~ floor, data = houseprice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6682 -2.1916 -0.1459  2.0517  4.9644 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.25296    1.52716    0.82     0.42    \nfloor        0.59511    0.04748   12.53 2.81e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.703 on 25 degrees of freedom\nMultiple R-squared:  0.8627,    Adjusted R-squared:  0.8572 \nF-statistic: 157.1 on 1 and 25 DF,  p-value: 2.807e-12\n\n\n위에서 update() 함수는 앞에서 적합한 모형 model0에 변수 floor를 추가한다.\n\n두 번째 변수의 추가\n\n\nadd1(model1, scope=~tax+ ground+ floor+ year, test=\"F\")\n\nSingle term additions\n\nModel:\nprice ~ floor\n       Df Sum of Sq     RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;              182.663 55.619                      \ntax     1    87.468  95.195 40.023 22.0517 8.997e-05 ***\nground  1    14.075 168.588 55.454  2.0037   0.16976    \nyear    1    28.854 153.809 52.977  4.5023   0.04437 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmodel2 &lt;- update(model1, . ~ . + tax)\nsummary(model2)\n\n\nCall:\nlm(formula = price ~ floor + tax, data = houseprice)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.009 -1.658  0.048  1.220  3.548 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.39497    1.13994   0.346    0.732    \nfloor        0.34835    0.06313   5.518 1.13e-05 ***\ntax          0.05742    0.01223   4.696 9.00e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.992 on 24 degrees of freedom\nMultiple R-squared:  0.9285,    Adjusted R-squared:  0.9225 \nF-statistic: 155.7 on 2 and 24 DF,  p-value: 1.798e-14\n\n\n위의 결과로부터 변수 tax가 가장 유의한 변수임을 알 수 있고 이를 모형에 추가한다.\n\n변수의 제거\n\n이제 독립변수가 두 개가(floor와 tax) 모형에 포함되었고 가장 최근에 포함된 변수 tax를 제외한 나머지 변수를 제거할 수 있는지 감정한다. 후방제거하는 경우는 제거할 변수가 포함된 모형과 포함하지 않는 모형의 설명력에 유의한 차이가 없어야 한다. 즉 F-검정이 유의하니 않으면 제거한다.\n\ndrop1(model2, test=\"F\")\n\nSingle term deletions\n\nModel:\nprice ~ floor + tax\n       Df Sum of Sq     RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;               95.195 40.023                      \nfloor   1   120.782 215.978 60.142  30.451 1.126e-05 ***\ntax     1    87.468 182.663 55.619  22.052 8.997e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n위의 함수 drop1()의 결과에서 모든 변수에 대한 F-통계량이 유의하므로 변수를 제거하지 않는다.\n\n세 번째 변수의 추가\n\n독립변수가 두 개가(floor와 tax) 모형에 새로운 변수를 추가하는 검정을 실시해 보자\n\nadd1(model2, scope=~tax+ ground+ floor+ year, test=\"F\")\n\nSingle term additions\n\nModel:\nprice ~ floor + tax\n       Df Sum of Sq    RSS    AIC F value Pr(&gt;F)\n&lt;none&gt;              95.195 40.023               \nground  1    1.9335 93.262 41.469  0.4768 0.4968\nyear    1    2.8761 92.319 41.194  0.7165 0.4060\n\n\n위의 결과에서 유의한 변수가 없으므로 더 이상 변수를 추가하지 않는다.\n\n최종 모형\n\n더 이상 추가할 변수와 제거할 변수가 없으면 단계별 선택을 중단한다. 따라서 단계별 선택법에 의한 최종 모형은 floor와 tax, 두 개의 독립변수를 포함하는 모형이다.\n이러한 단계별 회귀의 결과는 모든 가능한 회귀에 의한 방법과 모형이 일치한다. 독립변수의 수가 많는 경우 이러한 단계별 회귀 방법은 유용하게 사용된다.\n\n\nK.3.2 함수 step()\n위에서 논의한 세 종류의 변수선택법은 함수 step()을 이용하여 한 번에 결과를 얻을 수 있다.\n::: {.callout-caution}\n주의할 점은 함수 step()은 변수의 선택 기준이 F-검정이 아니 AIC 를 이용한다는 점이다.\n\n전진 선택\n\n\nmodel0 &lt;- lm(price~1, houseprice)\nstep(model0,  scope=~tax+ ground+ floor+ year, direction=\"forward\")\n\nStart:  AIC=107.23\nprice ~ 1\n\n         Df Sum of Sq     RSS     AIC\n+ floor   1   1147.92  182.66  55.619\n+ tax     1   1114.60  215.98  60.142\n+ ground  1    701.96  628.62  88.987\n+ year    1    128.11 1202.47 106.500\n&lt;none&gt;                1330.58 107.233\n\nStep:  AIC=55.62\nprice ~ floor\n\n         Df Sum of Sq     RSS    AIC\n+ tax     1    87.468  95.195 40.023\n+ year    1    28.854 153.809 52.977\n+ ground  1    14.075 168.588 55.454\n&lt;none&gt;                182.663 55.619\n\nStep:  AIC=40.02\nprice ~ floor + tax\n\n         Df Sum of Sq    RSS    AIC\n&lt;none&gt;                95.195 40.023\n+ year    1    2.8761 92.319 41.194\n+ ground  1    1.9335 93.262 41.469\n\n\n\nCall:\nlm(formula = price ~ floor + tax, data = houseprice)\n\nCoefficients:\n(Intercept)        floor          tax  \n    0.39497      0.34835      0.05742  \n\n\n\n후방 제거\n\n\nfit &lt;- lm(price~tax+ ground+ floor+ year, houseprice)\nstep(fit, direction=\"backward\")\n\nStart:  AIC=42.94\nprice ~ tax + ground + floor + year\n\n         Df Sum of Sq     RSS    AIC\n- ground  1     0.871  92.319 41.194\n- year    1     1.813  93.262 41.469\n&lt;none&gt;                 91.449 42.938\n- tax     1    58.652 150.100 54.318\n- floor   1    96.064 187.513 60.326\n\nStep:  AIC=41.19\nprice ~ tax + floor + year\n\n        Df Sum of Sq     RSS    AIC\n- year   1     2.876  95.195 40.023\n&lt;none&gt;                92.319 41.194\n- tax    1    61.490 153.809 52.977\n- floor  1   122.322 214.642 61.975\n\nStep:  AIC=40.02\nprice ~ tax + floor\n\n        Df Sum of Sq     RSS    AIC\n&lt;none&gt;                95.195 40.023\n- tax    1    87.468 182.663 55.619\n- floor  1   120.782 215.978 60.142\n\n\n\nCall:\nlm(formula = price ~ tax + floor, data = houseprice)\n\nCoefficients:\n(Intercept)          tax        floor  \n    0.39497      0.05742      0.34835  \n\n\n\n단계별 선택\n\n\nmodel0 &lt;- lm(price~1, houseprice)\nstep(model0,  scope=~tax+ ground+ floor+ year, direction=\"both\")\n\nStart:  AIC=107.23\nprice ~ 1\n\n         Df Sum of Sq     RSS     AIC\n+ floor   1   1147.92  182.66  55.619\n+ tax     1   1114.60  215.98  60.142\n+ ground  1    701.96  628.62  88.987\n+ year    1    128.11 1202.47 106.500\n&lt;none&gt;                1330.58 107.233\n\nStep:  AIC=55.62\nprice ~ floor\n\n         Df Sum of Sq     RSS     AIC\n+ tax     1     87.47   95.20  40.023\n+ year    1     28.85  153.81  52.977\n+ ground  1     14.08  168.59  55.454\n&lt;none&gt;                 182.66  55.619\n- floor   1   1147.92 1330.58 107.233\n\nStep:  AIC=40.02\nprice ~ floor + tax\n\n         Df Sum of Sq     RSS    AIC\n&lt;none&gt;                 95.195 40.023\n+ year    1     2.876  92.319 41.194\n+ ground  1     1.934  93.262 41.469\n- tax     1    87.468 182.663 55.619\n- floor   1   120.782 215.978 60.142\n\n\n\nCall:\nlm(formula = price ~ floor + tax, data = houseprice)\n\nCoefficients:\n(Intercept)        floor          tax  \n    0.39497      0.34835      0.05742  \n\n\n\n\nK.3.3 패키지 olsrr 의 이용\n패키지 olsrr 의 함수 ols_step_both_p() 이용하여 F-검정을 이용한 단계별 선택을 할 수 있다.\n\n\n\n\n\n\n볌수선택법에서의 유의수준\n\n\n\n함수 ols_step_both_p() 이용하여 F-검정에서는 유의수준을 지정해주지 않으면 유의 수준을 0.3 으로 사용한다.\n참고로 SAS 의 모형 선택에서 자동적으로 사용되는 유의수준은 전진선택에서는 0.5, 후방제거에서는 0.1 이다.\nols_step_forward_p(model, penter = 0.3,\n  progress = FALSE, details = FALSE, ...)\nols_step_backward_p(model, prem = 0.3,\n  progress = FALSE, details = FALSE, ...)\n\nForward Selection (FORWARD)\n\nThe p-values for these F statistics are compared to the SLENTRY= value that is specified in the MODEL statement (or to 0.50 if the SLENTRY= option is omitted). \n\nBackward Elimination (BACKWARD)\n\nF statistics significant at the SLSTAY= level specified in the MODEL statement (or at the 0.10 level if the SLSTAY= option is omitted). \n\n\n\nres &lt;- ols_step_both_p(fit1, details = TRUE)\n\nStepwise Selection Method \n-------------------------\n\nCandidate Terms: \n\n1. tax \n2. ground \n3. floor \n4. year \n\n\nStep   =&gt; 0 \nModel  =&gt; price ~ 1 \nR2     =&gt; 0 \n\nInitiating stepwise selection... \n\nStep      =&gt; 1 \nSelected  =&gt; floor \nModel     =&gt; price ~ floor \nR2        =&gt; 0.863 \n\nStep      =&gt; 2 \nSelected  =&gt; tax \nModel     =&gt; price ~ floor + tax \nR2        =&gt; 0.928 \n\n\nNo more variables to be added or removed.\n\nres\n\n\n                             Stepwise Summary                              \n-------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC        R2       Adj. R2 \n-------------------------------------------------------------------------\n 0      Base Model    185.856    188.448    105.725    0.00000    0.00000 \n 1      floor (+)     134.241    138.129     55.779    0.86272    0.85723 \n 2      tax (+)       118.645    123.829     43.032    0.92846    0.92249 \n-------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.964       RMSE                 1.878 \nR-Squared               0.928       MSE                  3.966 \nAdj. R-Squared          0.922       Coef. Var           10.346 \nPred R-Squared          0.891       AIC                118.645 \nMAE                     1.572       SBC                123.829 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                ANOVA                                 \n---------------------------------------------------------------------\n                Sum of                                               \n               Squares        DF    Mean Square       F         Sig. \n---------------------------------------------------------------------\nRegression    1235.385         2        617.692    155.728    0.0000 \nResidual        95.195        24          3.966                      \nTotal         1330.580        26                                     \n---------------------------------------------------------------------\n\n                                 Parameter Estimates                                  \n-------------------------------------------------------------------------------------\n      model     Beta    Std. Error    Std. Beta      t       Sig      lower    upper \n-------------------------------------------------------------------------------------\n(Intercept)    0.395         1.140                 0.346    0.732    -1.958    2.748 \n      floor    0.348         0.063        0.544    5.518    0.000     0.218    0.479 \n        tax    0.057         0.012        0.463    4.696    0.000     0.032    0.083 \n-------------------------------------------------------------------------------------\n\nplot(res)\n\n\n\n\n\n\n\n\n함수 ols_step_both_aic() 는 함수 step() 과 동일하게 AIC 를 이용하여 변수선택을 한다.\n\nols_step_forward_aic(fit1)\n\n\n                             Stepwise Summary                              \n-------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC        R2       Adj. R2 \n-------------------------------------------------------------------------\n 0      Base Model    185.856    188.448    105.725    0.00000    0.00000 \n 1      floor         134.241    138.129     55.779    0.86272    0.85723 \n 2      tax           118.645    123.829     43.032    0.92846    0.92249 \n-------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.964       RMSE                 1.878 \nR-Squared               0.928       MSE                  3.966 \nAdj. R-Squared          0.922       Coef. Var           10.346 \nPred R-Squared          0.891       AIC                118.645 \nMAE                     1.572       SBC                123.829 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                ANOVA                                 \n---------------------------------------------------------------------\n                Sum of                                               \n               Squares        DF    Mean Square       F         Sig. \n---------------------------------------------------------------------\nRegression    1235.385         2        617.692    155.728    0.0000 \nResidual        95.195        24          3.966                      \nTotal         1330.580        26                                     \n---------------------------------------------------------------------\n\n                                 Parameter Estimates                                  \n-------------------------------------------------------------------------------------\n      model     Beta    Std. Error    Std. Beta      t       Sig      lower    upper \n-------------------------------------------------------------------------------------\n(Intercept)    0.395         1.140                 0.346    0.732    -1.958    2.748 \n      floor    0.348         0.063        0.544    5.518    0.000     0.218    0.479 \n        tax    0.057         0.012        0.463    4.696    0.000     0.032    0.083 \n-------------------------------------------------------------------------------------",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>R-실습: 모형의 선택</span>"
    ]
  },
  {
    "objectID": "qmd/practice-04.html#연습문제-6.10",
    "href": "qmd/practice-04.html#연습문제-6.10",
    "title": "부록 K — R-실습: 모형의 선택",
    "section": "K.4 연습문제 6.10",
    "text": "K.4 연습문제 6.10\n자료 cars93은 1993년 미국에서 판매된 93가지 종류의 저동차에 대한 자료이다. 변수 MPG.highway를 반응변수로 하고 EngineSize, Weight, Price, Width, Length, Horsepower, Wheelbae 7개의 변수를 고려하여 가장 적합한 모형을 선택해보자.\n\ndat93 &lt;- Cars93[c(\"MPG.highway\",\"EngineSize\", \"Weight\", \"Price\", \"Width\", \"Length\", \"Horsepower\", \"Wheelbase\")]\nhead(dat93)\n\n  MPG.highway EngineSize Weight Price Width Length Horsepower Wheelbase\n1          31        1.8   2705  15.9    68    177        140       102\n2          25        3.2   3560  33.9    71    195        200       115\n3          26        2.8   3375  29.1    67    180        172       102\n4          26        2.8   3405  37.7    70    193        172       106\n5          30        3.5   3640  30.0    69    186        208       109\n6          31        2.2   2880  15.7    69    189        110       105\n\n\n\nK.4.1 완전모형\n\ncarfit &lt;- lm(MPG.highway~ . , data=dat93)\nsummary(carfit)\n\n\nCall:\nlm(formula = MPG.highway ~ ., data = dat93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3952 -1.9678 -0.1942  1.4516 10.1313 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 21.119770  12.809426   1.649   0.1029    \nEngineSize   0.483792   0.698257   0.693   0.4903    \nWeight      -0.012833   0.001726  -7.434 7.69e-11 ***\nPrice       -0.047157   0.059498  -0.793   0.4302    \nWidth        0.079770   0.228192   0.350   0.7275    \nLength       0.058766   0.043322   1.356   0.1785    \nHorsepower   0.013296   0.013443   0.989   0.3254    \nWheelbase    0.277231   0.118027   2.349   0.0212 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.95 on 85 degrees of freedom\nMultiple R-squared:  0.7172,    Adjusted R-squared:  0.694 \nF-statistic:  30.8 on 7 and 85 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nK.4.2 모든 가능한 회귀\n\ncarfit.rgs2 &lt;- ols_step_all_possible(carfit)\ncarfit.rgs2$result %&gt;% dplyr::select(mindex, n, predictors, rsquare, adjr, cp, aic, sbc) %&gt;% \n    dplyr::arrange(sbc) %&gt;%\n    head(10)\n\n   mindex n                         predictors   rsquare      adjr       cp\n1       8 2                      Weight Length 0.6921969 0.6853568 5.527269\n2       9 2                   Weight Wheelbase 0.6920227 0.6851788 5.579611\n3      29 3            Weight Length Wheelbase 0.7065024 0.6966092 3.226956\n4      30 3        EngineSize Weight Wheelbase 0.7045887 0.6946310 3.802229\n5      31 3             Weight Width Wheelbase 0.7039881 0.6940102 3.982755\n6      64 4 EngineSize Weight Length Wheelbase 0.7120588 0.6989706 3.556655\n7      32 3                Weight Width Length 0.6974384 0.6872397 5.951620\n8      65 4      Weight Width Length Wheelbase 0.7113915 0.6982729 3.757264\n9      33 3        Weight Horsepower Wheelbase 0.6965448 0.6863160 6.220248\n10     34 3           EngineSize Weight Length 0.6950183 0.6847380 6.679132\n        aic      sbc\n1  472.6393 482.7697\n2  472.6919 482.8223\n3  470.2133 482.8763\n4  470.8178 483.4808\n5  471.0066 483.6696\n6  470.4358 485.6314\n7  473.0419 485.7049\n8  470.6511 485.8467\n9  473.3162 485.9792\n10 473.7829 486.4459\n\n\n\nplot(carfit.rgs2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK.4.3 전진선택법\n\nmodel0 &lt;- lm(MPG.highway~1, data=dat93)\nstep(model0,  scope=~EngineSize+ Weight+ Price+ Width+ Length+ Horsepower+ Wheelbase, direction=\"forward\")\n\nStart:  AIC=312.3\nMPG.highway ~ 1\n\n             Df Sum of Sq     RSS    AIC\n+ Weight      1   1718.70  896.62 214.74\n+ Width       1   1072.43 1542.88 265.22\n+ EngineSize  1   1027.48 1587.83 267.89\n+ Horsepower  1   1002.23 1613.08 269.36\n+ Wheelbase   1    990.41 1624.90 270.04\n+ Price       1    822.16 1793.16 279.20\n+ Length      1    770.83 1844.48 281.82\n&lt;none&gt;                    2615.31 312.30\n\nStep:  AIC=214.74\nMPG.highway ~ Weight\n\n             Df Sum of Sq    RSS    AIC\n+ Length      1    91.615 805.00 206.72\n+ Wheelbase   1    91.160 805.46 206.77\n+ Width       1    53.010 843.61 211.07\n+ EngineSize  1    31.068 865.55 213.46\n&lt;none&gt;                    896.62 214.74\n+ Price       1     5.845 890.77 216.13\n+ Horsepower  1     2.334 894.28 216.50\n\nStep:  AIC=206.72\nMPG.highway ~ Weight + Length\n\n             Df Sum of Sq    RSS    AIC\n+ Wheelbase   1    37.413 767.59 204.29\n&lt;none&gt;                    805.00 206.72\n+ Width       1    13.708 791.29 207.12\n+ EngineSize  1     7.379 797.62 207.86\n+ Price       1     4.135 800.87 208.24\n+ Horsepower  1     0.207 804.79 208.69\n\nStep:  AIC=204.29\nMPG.highway ~ Weight + Length + Wheelbase\n\n             Df Sum of Sq    RSS    AIC\n&lt;none&gt;                    767.59 204.29\n+ EngineSize  1   14.5319 753.06 204.51\n+ Width       1   12.7865 754.80 204.73\n+ Horsepower  1    7.7950 759.79 205.34\n+ Price       1    1.0351 766.55 206.16\n\n\n\nCall:\nlm(formula = MPG.highway ~ Weight + Length + Wheelbase, data = dat93)\n\nCoefficients:\n(Intercept)       Weight       Length    Wheelbase  \n   26.31687     -0.01107      0.08170      0.21005  \n\n\n\n\nK.4.4 후방제거\n\nstep(carfit, direction=\"backward\")\n\nStart:  AIC=208.83\nMPG.highway ~ EngineSize + Weight + Price + Width + Length + \n    Horsepower + Wheelbase\n\n             Df Sum of Sq     RSS    AIC\n- Width       1      1.06  740.58 206.96\n- EngineSize  1      4.18  743.69 207.35\n- Price       1      5.47  744.98 207.51\n- Horsepower  1      8.51  748.02 207.89\n- Length      1     16.01  755.52 208.82\n&lt;none&gt;                     739.51 208.82\n- Wheelbase   1     48.00  787.51 212.67\n- Weight      1    480.82 1220.33 253.41\n\nStep:  AIC=206.96\nMPG.highway ~ EngineSize + Weight + Price + Length + Horsepower + \n    Wheelbase\n\n             Df Sum of Sq     RSS    AIC\n- EngineSize  1      7.73  748.31 205.93\n- Price       1      9.67  750.25 206.17\n- Horsepower  1     10.29  750.87 206.24\n&lt;none&gt;                     740.58 206.96\n- Length      1     19.54  760.12 207.38\n- Wheelbase   1     51.04  791.62 211.16\n- Weight      1    514.25 1254.83 254.00\n\nStep:  AIC=205.93\nMPG.highway ~ Weight + Price + Length + Horsepower + Wheelbase\n\n             Df Sum of Sq     RSS    AIC\n- Price       1     11.48  759.79 205.34\n&lt;none&gt;                     748.31 205.93\n- Horsepower  1     18.24  766.55 206.16\n- Length      1     32.26  780.57 207.85\n- Wheelbase   1     51.62  799.93 210.13\n- Weight      1    516.55 1264.86 252.74\n\nStep:  AIC=205.34\nMPG.highway ~ Weight + Length + Horsepower + Wheelbase\n\n             Df Sum of Sq     RSS    AIC\n- Horsepower  1      7.79  767.59 204.29\n&lt;none&gt;                     759.79 205.34\n- Length      1     33.84  793.63 207.39\n- Wheelbase   1     45.00  804.79 208.69\n- Weight      1    508.94 1268.73 251.03\n\nStep:  AIC=204.29\nMPG.highway ~ Weight + Length + Wheelbase\n\n            Df Sum of Sq     RSS    AIC\n&lt;none&gt;                    767.59 204.29\n- Wheelbase  1     37.41  805.00 206.72\n- Length     1     37.87  805.46 206.77\n- Weight     1    846.75 1614.34 271.43\n\n\n\nCall:\nlm(formula = MPG.highway ~ Weight + Length + Wheelbase, data = dat93)\n\nCoefficients:\n(Intercept)       Weight       Length    Wheelbase  \n   26.31687     -0.01107      0.08170      0.21005  \n\n\n\n\nK.4.5 단계별 선택\n\nmodel0 &lt;- lm(MPG.highway~1, data=dat93)\nstep(model0,  scope=~EngineSize+ Weight+ Price+ Width+ Length+ Horsepower+ Wheelbase, direction=\"both\")\n\nStart:  AIC=312.3\nMPG.highway ~ 1\n\n             Df Sum of Sq     RSS    AIC\n+ Weight      1   1718.70  896.62 214.74\n+ Width       1   1072.43 1542.88 265.22\n+ EngineSize  1   1027.48 1587.83 267.89\n+ Horsepower  1   1002.23 1613.08 269.36\n+ Wheelbase   1    990.41 1624.90 270.04\n+ Price       1    822.16 1793.16 279.20\n+ Length      1    770.83 1844.48 281.82\n&lt;none&gt;                    2615.31 312.30\n\nStep:  AIC=214.74\nMPG.highway ~ Weight\n\n             Df Sum of Sq     RSS    AIC\n+ Length      1     91.62  805.00 206.72\n+ Wheelbase   1     91.16  805.46 206.77\n+ Width       1     53.01  843.61 211.07\n+ EngineSize  1     31.07  865.55 213.46\n&lt;none&gt;                     896.62 214.74\n+ Price       1      5.85  890.77 216.13\n+ Horsepower  1      2.33  894.28 216.50\n- Weight      1   1718.70 2615.31 312.30\n\nStep:  AIC=206.72\nMPG.highway ~ Weight + Length\n\n             Df Sum of Sq     RSS    AIC\n+ Wheelbase   1     37.41  767.59 204.29\n&lt;none&gt;                     805.00 206.72\n+ Width       1     13.71  791.29 207.12\n+ EngineSize  1      7.38  797.62 207.86\n+ Price       1      4.14  800.87 208.24\n+ Horsepower  1      0.21  804.79 208.69\n- Length      1     91.62  896.62 214.74\n- Weight      1   1039.48 1844.48 281.82\n\nStep:  AIC=204.29\nMPG.highway ~ Weight + Length + Wheelbase\n\n             Df Sum of Sq     RSS    AIC\n&lt;none&gt;                     767.59 204.29\n+ EngineSize  1     14.53  753.06 204.51\n+ Width       1     12.79  754.80 204.73\n+ Horsepower  1      7.79  759.79 205.34\n+ Price       1      1.04  766.55 206.16\n- Wheelbase   1     37.41  805.00 206.72\n- Length      1     37.87  805.46 206.77\n- Weight      1    846.75 1614.34 271.43\n\n\n\nCall:\nlm(formula = MPG.highway ~ Weight + Length + Wheelbase, data = dat93)\n\nCoefficients:\n(Intercept)       Weight       Length    Wheelbase  \n   26.31687     -0.01107      0.08170      0.21005  \n\n\n\nols_step_forward_aic(carfit, details = TRUE)\n\nForward Selection Method \n------------------------\n\nCandidate Terms: \n\n1. EngineSize \n2. Weight \n3. Price \n4. Width \n5. Length \n6. Horsepower \n7. Wheelbase \n\n\nStep     =&gt; 0 \nModel    =&gt; MPG.highway ~ 1 \nAIC      =&gt; 578.2207 \n\nInitiating stepwise selection... \n\n                      Table: Adding New Variables                       \n-----------------------------------------------------------------------\nPredictor     DF      AIC        SBC       SBIC        R2       Adj. R2 \n-----------------------------------------------------------------------\nWeight         1    480.663    488.261    216.331    0.65717    0.65340 \nWidth          1    531.141    538.739    264.864    0.41006    0.40358 \nEngineSize     1    533.812    541.410    267.447    0.39287    0.38620 \nHorsepower     1    535.280    542.878    268.867    0.38322    0.37644 \nWheelbase      1    535.959    543.556    269.524    0.37870    0.37187 \nPrice          1    545.122    552.720    278.402    0.31436    0.30683 \nLength         1    547.746    555.344    280.948    0.29474    0.28699 \n-----------------------------------------------------------------------\n\nStep     =&gt; 1 \nAdded    =&gt; Weight \nModel    =&gt; MPG.highway ~ Weight \nAIC      =&gt; 480.6632 \n\n                      Table: Adding New Variables                       \n-----------------------------------------------------------------------\nPredictor     DF      AIC        SBC       SBIC        R2       Adj. R2 \n-----------------------------------------------------------------------\nLength         1    472.639    482.770    208.747    0.69220    0.68536 \nWheelbase      1    472.692    482.822    208.797    0.69202    0.68518 \nWidth          1    476.996    487.126    212.824    0.67744    0.67027 \nEngineSize     1    479.384    489.514    215.061    0.66905    0.66169 \nPrice          1    482.055    492.185    217.566    0.65940    0.65183 \nHorsepower     1    482.421    492.551    217.909    0.65806    0.65046 \n-----------------------------------------------------------------------\n\nStep     =&gt; 2 \nAdded    =&gt; Length \nModel    =&gt; MPG.highway ~ Weight + Length \nAIC      =&gt; 472.6393 \n\n                      Table: Adding New Variables                       \n-----------------------------------------------------------------------\nPredictor     DF      AIC        SBC       SBIC        R2       Adj. R2 \n-----------------------------------------------------------------------\nWheelbase      1    470.213    482.876    206.718    0.70650    0.69661 \nWidth          1    473.042    485.705    209.299    0.69744    0.68724 \nEngineSize     1    473.783    486.446    209.975    0.69502    0.68474 \nPrice          1    474.160    486.823    210.320    0.69378    0.68346 \nHorsepower     1    474.615    487.278    210.736    0.69228    0.68190 \n-----------------------------------------------------------------------\n\nStep     =&gt; 3 \nAdded    =&gt; Wheelbase \nModel    =&gt; MPG.highway ~ Weight + Length + Wheelbase \nAIC      =&gt; 470.2133 \n\n                      Table: Adding New Variables                       \n-----------------------------------------------------------------------\nPredictor     DF      AIC        SBC       SBIC        R2       Adj. R2 \n-----------------------------------------------------------------------\nEngineSize     1    470.436    485.631    207.247    0.71206    0.69897 \nWidth          1    470.651    485.847    207.438    0.71139    0.69827 \nHorsepower     1    471.264    486.460    207.982    0.70948    0.69628 \nPrice          1    472.088    487.283    208.714    0.70690    0.69358 \n-----------------------------------------------------------------------\n\n\nNo more variables to be added.\n\nVariables Selected: \n\n=&gt; Weight \n=&gt; Length \n=&gt; Wheelbase \n\n\n\n                             Stepwise Summary                              \n-------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC        R2       Adj. R2 \n-------------------------------------------------------------------------\n 0      Base Model    578.221    583.286    311.963    0.00000    0.00000 \n 1      Weight        480.663    488.261    216.331    0.65717    0.65340 \n 2      Length        472.639    482.770    208.747    0.69220    0.68536 \n 3      Wheelbase     470.213    482.876    206.718    0.70650    0.69661 \n-------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.841       RMSE                 2.873 \nR-Squared               0.707       MSE                  8.625 \nAdj. R-Squared          0.697       Coef. Var           10.097 \nPred R-Squared          0.665       AIC                470.213 \nMAE                     2.139       SBC                482.876 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n                Sum of                                              \n               Squares        DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression    1847.724         3        615.908    71.413    0.0000 \nResidual       767.588        89          8.625                     \nTotal         2615.312        92                                    \n--------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)    26.317         7.088                  3.713    0.000    12.233    40.401 \n     Weight    -0.011         0.001       -1.225    -9.909    0.000    -0.013    -0.009 \n     Length     0.082         0.039        0.224     2.095    0.039     0.004     0.159 \n  Wheelbase     0.210         0.101        0.269     2.083    0.040     0.010     0.410 \n----------------------------------------------------------------------------------------\n\n\n\nols_step_forward_p(carfit, details = TRUE)\n\nForward Selection Method \n------------------------\n\nCandidate Terms: \n\n1. EngineSize \n2. Weight \n3. Price \n4. Width \n5. Length \n6. Horsepower \n7. Wheelbase \n\n\nStep   =&gt; 0 \nModel  =&gt; MPG.highway ~ 1 \nR2     =&gt; 0 \n\nInitiating stepwise selection... \n\n                    Selection Metrics Table                      \n----------------------------------------------------------------\nPredictor     Pr(&gt;|t|)    R-Squared    Adj. R-Squared      AIC   \n----------------------------------------------------------------\nWeight         0.00000        0.657             0.653    480.663 \nWidth          0.00000        0.410             0.404    531.141 \nEngineSize     0.00000        0.393             0.386    533.812 \nHorsepower     0.00000        0.383             0.376    535.280 \nWheelbase      0.00000        0.379             0.372    535.959 \nPrice          0.00000        0.314             0.307    545.122 \nLength         0.00000        0.295             0.287    547.746 \n----------------------------------------------------------------\n\nStep      =&gt; 1 \nSelected  =&gt; Weight \nModel     =&gt; MPG.highway ~ Weight \nR2        =&gt; 0.657 \n\n                    Selection Metrics Table                      \n----------------------------------------------------------------\nPredictor     Pr(&gt;|t|)    R-Squared    Adj. R-Squared      AIC   \n----------------------------------------------------------------\nLength         0.00190        0.692             0.685    472.639 \nWheelbase      0.00195        0.692             0.685    472.692 \nWidth          0.01952        0.677             0.670    476.996 \nEngineSize     0.07563        0.669             0.662    479.384 \nPrice          0.44422        0.659             0.652    482.055 \nHorsepower     0.62912        0.658             0.650    482.421 \n----------------------------------------------------------------\n\nStep      =&gt; 2 \nSelected  =&gt; Length \nModel     =&gt; MPG.highway ~ Weight + Length \nR2        =&gt; 0.692 \n\n                    Selection Metrics Table                      \n----------------------------------------------------------------\nPredictor     Pr(&gt;|t|)    R-Squared    Adj. R-Squared      AIC   \n----------------------------------------------------------------\nWheelbase      0.04014        0.707             0.697    470.213 \nWidth          0.21761        0.697             0.687    473.042 \nEngineSize     0.36665        0.695             0.685    473.783 \nPrice          0.49960        0.694             0.683    474.160 \nHorsepower     0.88015        0.692             0.682    474.615 \n----------------------------------------------------------------\n\nStep      =&gt; 3 \nSelected  =&gt; Wheelbase \nModel     =&gt; MPG.highway ~ Weight + Length + Wheelbase \nR2        =&gt; 0.707 \n\n                    Selection Metrics Table                      \n----------------------------------------------------------------\nPredictor     Pr(&gt;|t|)    R-Squared    Adj. R-Squared      AIC   \n----------------------------------------------------------------\nEngineSize     0.19593        0.712             0.699    470.436 \nWidth          0.22536        0.711             0.698    470.651 \nHorsepower     0.34463        0.709             0.696    471.264 \nPrice          0.73113        0.707             0.694    472.088 \n----------------------------------------------------------------\n\nStep      =&gt; 4 \nSelected  =&gt; EngineSize \nModel     =&gt; MPG.highway ~ Weight + Length + Wheelbase + EngineSize \nR2        =&gt; 0.712 \n\n                    Selection Metrics Table                      \n----------------------------------------------------------------\nPredictor     Pr(&gt;|t|)    R-Squared    Adj. R-Squared      AIC   \n----------------------------------------------------------------\nWidth          0.46764        0.714             0.697    471.869 \nHorsepower     0.56977        0.713             0.697    472.088 \nPrice          0.61570        0.713             0.696    472.165 \n----------------------------------------------------------------\n\n\nNo more variables to be added.\n\nVariables Selected: \n\n=&gt; Weight \n=&gt; Length \n=&gt; Wheelbase \n=&gt; EngineSize \n\n\n\n                             Stepwise Summary                              \n-------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC        R2       Adj. R2 \n-------------------------------------------------------------------------\n 0      Base Model    578.221    583.286    311.963    0.00000    0.00000 \n 1      Weight        480.663    488.261    216.331    0.65717    0.65340 \n 2      Length        472.639    482.770    208.747    0.69220    0.68536 \n 3      Wheelbase     470.213    482.876    206.718    0.70650    0.69661 \n 4      EngineSize    470.436    485.631    207.247    0.71206    0.69897 \n-------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.844       RMSE                 2.846 \nR-Squared               0.712       MSE                  8.557 \nAdj. R-Squared          0.699       Coef. Var           10.057 \nPred R-Squared          0.664       AIC                470.436 \nMAE                     2.081       SBC                485.631 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n                Sum of                                              \n               Squares        DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression    1862.256         4        465.564    54.404    0.0000 \nResidual       753.056        88          8.557                     \nTotal         2615.312        92                                    \n--------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)    28.439         7.246                  3.925    0.000    14.040    42.839 \n     Weight    -0.012         0.001       -1.334    -8.960    0.000    -0.015    -0.009 \n     Length     0.063         0.041        0.172     1.511    0.134    -0.020     0.145 \n  Wheelbase     0.233         0.102        0.298     2.282    0.025     0.030     0.435 \n EngineSize     0.766         0.587        0.149     1.303    0.196    -0.402     1.933 \n----------------------------------------------------------------------------------------",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>R-실습: 모형의 선택</span>"
    ]
  },
  {
    "objectID": "qmd/practice-04.html#연습문제-6.12",
    "href": "qmd/practice-04.html#연습문제-6.12",
    "title": "부록 K — R-실습: 모형의 선택",
    "section": "K.5 연습문제 6.12",
    "text": "K.5 연습문제 6.12\n\nset.seed(13111)\n\nNsim &lt;- 10\nnum_of_ind_var &lt;- 50\nn &lt;- 100\nN &lt;- (num_of_ind_var+1)*n\n\n\nrandata &lt;- matrix(rnorm(N), ncol=num_of_ind_var+1)\nrandata &lt;- data.frame(randata)\nindvarname &lt;- names(randata)[1:50]\n\nvariables.set &lt;- NULL\nr2.set &lt;- NULL\nr2adj.set &lt;- NULL\np &lt;- NULL\n\nfor (i in 1:Nsim) {\n  \n  randata &lt;- matrix(rnorm(N), ncol=num_of_ind_var+1)\n  randata &lt;- data.frame(randata)\n  \n  # === 함수 step(..direction=\"backward\")는   AIC 를 이용\n  fit.lm &lt;- lm(X51 ~ ., data=randata)\n  res &lt;- step(fit.lm, direction=\"backward\", trace=0)\n  variable.entered &lt;- as.character(attr(res$terms, \"variables\"))[-c(1,2)]\n  \n  # ==== 함수 step(...direction = \"forward\"..) 은 AIC 를 이용\n  # fit.null &lt;- lm(X51 ~ 1, data=randata)\n  # res &lt;- step(fit.null, scope~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + \n  #  X13 + X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + \n  #  X23 + X24 + X25 + X26 + X27 + X28 + X29 + X30 + X31 + X32 + \n  #  X33 + X34 + X35 + X36 + X37 + X38 + X39 + X40 + X41 + X42 + \n  #  X43 + X44 + X45 + X46 + X47 + X48 + X49 + X50  , direction = \"forward\", trace=0)\n  # ariable.entered &lt;- as.character(attr(res$terms, \"variables\"))[-c(1,2)]\n\n\n  # === 함수 ols_step_backward_p 는 F-검정을 이용\n  # fit.lm &lt;- lm(X51 ~ ., data=randata)\n  # aa &lt;- ols_step_backward_p(fit.lm)\n  # variable.entered &lt;- names(aa$model$coefficients)[-1]\n  \n  # ===  함수 ols_step_forward_p 는 F-검정을 이용 \n  # fit.lm &lt;- lm(X51 ~ ., data=randata)\n  # aa &lt;- ols_step_forward_p(fit.lm)\n  # variable.entered &lt;- names(aa$model$coefficients)[-1]\n\n\n  variables.set &lt;- c(variables.set, variable.entered )\n  p &lt;- c(p, length(variable.entered))\n  r2.set  &lt;- c( r2.set , summary(res)$r.squared)\n  r2adj.set  &lt;- c( r2adj.set , summary(res)$adj.r.squared)\n}\n\nvariables.set &lt;- factor(variables.set, levels=indvarname)\ntable(variables.set)\n\nvariables.set\n X1  X2  X3  X4  X5  X6  X7  X8  X9 X10 X11 X12 X13 X14 X15 X16 X17 X18 X19 X20 \n  2   3   3   3   2   5   5   1   5   0   3   4   0   4   2   4   3   2   1   2 \nX21 X22 X23 X24 X25 X26 X27 X28 X29 X30 X31 X32 X33 X34 X35 X36 X37 X38 X39 X40 \n  6   3   4   2   4   4   3   1   4   3   2   3   2   1   3   4   2   1   2   5 \nX41 X42 X43 X44 X45 X46 X47 X48 X49 X50 \n  2   2   3   2   2   3   1   2   4   0 \n\nsummary(p)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    7.0     9.0    14.0    13.4    17.0    20.0 \n\nsummary(r2.set)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.2228  0.2909  0.3612  0.3570  0.4050  0.5094 \n\nsummary(r2adj.set)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1545  0.2094  0.2635  0.2595  0.2938  0.3852",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>R-실습: 모형의 선택</span>"
    ]
  }
]