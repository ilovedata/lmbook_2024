[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "통계적 예측모형",
    "section": "",
    "text": "Preface\n이 책은 통계적 예측모형에 대한 교재이며 일반 선형모형을 포함하여 예측에 사용되는 기본적인 통계 모형에 대한 이론을 최대 가능도 추정법의 관점에서 설명합니다. 또한 실제 예제를 통한 실습, 모형을 적합하는 계산방법과 연관된 행렬이론에 대하여 다루고자 합니다.\n\n\n\n\n\n\n노트\n\n\n\n이 책에서 사용된 기호, 표기법, 프로그램의 규칙과 쓰임은 다음과 같습니다.\n\n스칼라(scalar)와 일변량 확률변수는 일반적으로 보통 글씨체의 소문자로 표기한다. 특별한 이유가 있는 경우 대문자로 표시할 것이다.\n벡터, 행렬, 다변량 확률벡터는 굵은 글씨체로 표기한다.\n통계 프로그램은 R을 이용하였다. 각 예제에 사용된 R 프로그램은 코드 상자를 열면 나타난다.\n\n\n\n강의의 부교재는 강근석 와/과 유형조 (2016) 을 사용한다.\n이 교과서에서 이용하는 R 패키지는 다음과 같다.\n\nlibrary(here)           # file pathways\nlibrary(tidyverse)      # data management, summary, and visualization\nlibrary(MASS)\nlibrary(knitr)\nlibrary(kableExtra)\n\nlibrary(agricolae)\nlibrary(emmeans)\nlibrary(car)\n\nlibrary(plotly)\nlibrary(plot3D)\n\n# 아래 3 문장은 한글을 포함한 ggplot 그림이 포함된 HTML, PDF로 만드는 경우 사용\nlibrary(showtext)\nfont_add_google(\"Nanum Pen Script\", \"gl\")\nshowtext_auto()\n\n#강의 부교재 자료를 포함한 패키지 설치\ninstall.packages(\"remotes\")\nremotes::install_github(\"regbook/regbook\")\nlibrary(regbook)\n\n\n\n\n강근석, 와/과 유형조. 2016. R을 활용한 선형회귀분석. 1st ed. 교우사. https://github.com/regbook/regbook.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "qmd/lse.html",
    "href": "qmd/lse.html",
    "title": "1  선형 회귀모형의 소개",
    "section": "",
    "text": "1.1 예제-단순 회귀모형",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#예제-단순-회귀모형",
    "href": "qmd/lse.html#예제-단순-회귀모형",
    "title": "1  선형 회귀모형의 소개",
    "section": "",
    "text": "보기 1.1 (자동차의 제동거리) 자동차가 달리는 속도(speed,단위는 mph; mile per hour)와 제동거리(dist, 단위는 ft;feet)의 관계를 알아보기 위하여 50대의 자동차로 실험한 결과의 자료 cars 는 다음과 같다(처음 10개의 자료만 보여준다). 자료는 R 의 data.frame 형식으로 저장되어 있다.\n아래 자료를 보면 실험에서 2대의 자동차는 7 mph 로 달리다가 브레이크를 밟고 정지하는 경우 각각 4, 22 feet 의 제동거리가 필요한 것으로 나타났다. 또한 3대의 는 10 mph 로 달리다가 각각 18, 26, 34 feet 의 제동거리가 필요한 것으로 나타났다.\n\ncars %&gt;% head(n=10) \n\n   speed dist\n1      4    2\n2      4   10\n3      7    4\n4      7   22\n5      8   16\n6      9   10\n7     10   18\n8     10   26\n9     10   34\n10    11   17\n\n\n자동차의 속도와 제동거리에 대한 산포도는 아래와 같다.\n\nggplot(cars, aes(x=speed, y=dist)) + geom_point() + labs(x = \"속도\", y = \"거리\") +\n  labs(title=\"자동차의 속도와 제동거리의 관계\")\n\n\n\n\n\n\n\n\n위와 같은 자료를 이용하여 자동차의 속도가 주어졌을 경우 제동거리를 예측하려고 한다면 어떤 방법을 사용해야 할까?\n\n\n보기 1.2 (아파트 판매가격) 다음 살펴볼 자료는 2019년 거래된 서울 아파트의 실거래 데이터 중 4개의 구(동대문구, 서초구, 관악구, 노원구)에서 거래된 아파트 중 1000개의 아파트를 임의로 추출한 자료이다.\n\napart_2019 &lt;- read.csv(here(\"data\", \"seoul_apartment_2019_sample.csv\"), header = T)\nhead(apart_2019,10)\n\n       gu year  area price\n1  관악구 1974 65.09   450\n2  관악구 1978 56.86   276\n3  관악구 1982 91.24   599\n4  관악구 1982 91.24   560\n5  관악구 1984 60.27   425\n6  관악구 1984 60.27   420\n7  관악구 1985 38.92   217\n8  관악구 1988 61.74   485\n9  관악구 1991 71.90   328\n10 관악구 1991 84.44   438\n\n\n아파트의 면적(area;제곱미터)에 따른 거래가격(price;백만원)의 변화는 다음 그림과 같다.\n\nggplot(apart_2019, aes(x=area, y=price)) + geom_point() + labs(x = \"면적(제곱미터)\", y = \"거래가격(백만원)\") +\n   labs(title = \"아파트의 면적과 거래가격의 관계\")\n\n\n\n\n\n\n\n\n만약 아파트의 면적(x)과 거래가격(y) 대신 각각의 로그값(log(x), `log(y))을 시용하면 다음과 같은 산포도가 나타난다.\n\n# log scale for y\nggplot(apart_2019, aes(x=area, y=price)) + geom_point() + labs(x = \"면적(제곱미터)\", y = \"거래가격(백만원)\") +\n  scale_y_log10() + \n  scale_x_log10() +\n  labs(title = \"아파트의 면적과 거래가격의 관계 (로그스케일)\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#선형-회귀모형",
    "href": "qmd/lse.html#선형-회귀모형",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.2 선형 회귀모형",
    "text": "1.2 선형 회귀모형\n회귀모형(regression model)는 변수들의 함수적 관계를 분석하는 통계적 방법이다. 일반적으로 한 개 또는 여러 개의 설명변수들(explanatory variables, x)이 관심있는 반응변수(response variable, y)에 어떤 형태로 영향을 미치는지에 파악하고 설명변수와 반응변수의 함수 관계를 통계적으로 추론하는 것이 회귀분석의 목적이다.\n위에서 살펴본 두 예제에서 자동차의 속도(x)가 증가하면 제동거리(y)가 증가하는 경향이 있다는 것을 알 수 있으며, 아파트의 면적(x)과 거래가격(y)도 유사한 관계임을 알 수 있다.\n이러한 두 변수의 관계를 다음과 같은 반응변수 \\(y\\) 와 설명변수의 선형 에측식(linear predictor)으로 나타내어 보자. 이러한 관계는 반응변수의 값의 변화를 근사적으로 설명변수의 선형식으로 예측할 수 있다는 의미이다.\n\\[ y \\approx \\beta_0 + \\beta_1 x \\]\n위와 같은 근사적인 관계를 더 구체화하여 다음과 같이 반응변수의 평균값이 설명변수의 선형식으로 나타나는 것을 가정할 수 있으며 이를 선형 회귀모형(linear regression model)이라고 한다.이\n\\[\nE(y|x) = \\beta_0 + \\beta_1 x\n\\tag{1.1}\\]\n식 1.1 은 반응변수 \\(y\\)의 평균이 설명변수 \\(x\\) 의 선형 예측식으로 나타나는 관계를 가정한 것이며 절편 \\(\\beta_0\\) 와 기울기 \\(\\beta_1\\) 는 회귀계수(regression coefficient)라고 부르는 모수(parameter)로서 추정해야 한다.\n특별히 하나의 설명변수를 사용하는 회귀 모형을 단순선형 회귀모형(simple linear regression model)이라고 한다.\n위에서 본 두 예제와 같이 \\(n\\) 개의 자료 \\((x_1,y_1),(x_2,y_2),..,(x_n, y_n)\\)을 독립적으로 추출하였다면 자료의 생성 과정을 다음과 같은 단순선형 회귀모형으로 나타낼 수 있다. 반응변수 \\(y_i\\)는 설명변수 \\(x_i\\)의 선형함수로 표현된 선형 예측식 식 1.1 과 임의의 오차항 (random error) \\(e_i\\) 의 합으로 나타내어진다고 가정하자.\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + e_i, \\quad i=1,2,\\dots,n\n\\tag{1.2}\\]\n여기서 오차항 \\(e_i\\)는 평균이 \\(0\\)이고 분산이 \\(\\sigma^2\\) 인 임의의 확률분포를 따르며 서로 독립이라고 가정한다.\n\\[ E(e_i)=0, \\quad V(e_i) = \\sigma^2 \\quad i=1,2,\\dots,n \\]\n오차항의 분산 \\(\\sigma^2\\)도 추정해야할 모수(parameter)이다.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#최소제곱법",
    "href": "qmd/lse.html#최소제곱법",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.3 최소제곱법",
    "text": "1.3 최소제곱법\n앞에서 언급한 것과 같이 선형회귀모형 식 1.2 에서 모수 \\(\\beta_0\\)와 \\(\\beta_1\\)를 회귀계수라고 하며 자료를 이용하여 추정해야 한다. \\(n\\)개의 자료를 이용하여 회귀계수 \\(\\beta_0\\)와 \\(\\beta_1\\)를 추정하려고 할 때 사용할 수 있는 방법들 중에서 가장 쉽고 유용한 방법은 최소제곱법(least square method)이다.\n회귀모형 식 1.2 에서 \\(\\beta_0\\)와 \\(\\beta_1\\)의 값이 주어졌다면 설명변수 \\(x_i\\) 에서 반응변수의 관측값 \\(y_i\\)에 가장 합리적인 예측값은 무었일까? 가장 합리적인 예측값은 주어진 \\(x_i\\)에서 반응변수의 평균값인 \\(E(y_i | x_i)=\\beta_0 + \\beta_1 x_i\\)이다. 여기서 실제 관측하여 얻어진 값 \\(y_i\\)와 예측값 \\(\\beta_0 + \\beta_1 x_i\\) 사이에는 오차에 의해서 차이가 발생할 수 있다. 그 차이를 잔차(residual)라고 하며 \\(r_i\\) 라고 표기한다.\n\\[  r_i = y_i - E(y_i|x_i) = y_i - (  \\beta_0 +  \\beta_1 x_i) \\]\n잔차는 위에 식에서 알 수 있듯이 관측값과 회귀식을 통한 예측값의 차이를 나타낸 것이다. 그러면 자료를 가장 잘 설명할 수 있는 회귀직선을 얻기 위해서는 잔차 \\(r_i\\)를 가장 작게하는 회귀모형을 세워야 한다. 잔차들을 최소로 하는 방법들 중 하나인 최소제곱법은 잔차들의 제곱합을 최소로 하는 회귀계수 \\(\\beta_0\\)와 \\(\\beta_1\\)를 추정하는 방법이다. 잔차들의 제곱합은 다음과 같이 표현된다.\n\\[\nS(\\beta_0 , \\beta_1) = \\sum^n_{i=1}r^2_i = \\sum^n_{i=1}[y_i-(\\beta_0 + \\beta_1 x_i)]^2\n\\tag{1.3}\\]\n\n\n\n\n\n\n노트\n\n\n\n식 1.3 를 잔차제곱합(residusl sum of square)이라고 부른다. 일반적으로 회귀계수의 값이 특정지어져서 실제로 잔차를 계산할 수 있는 경우 잔차제곱합이라고 부른다. 뒤에 분산분석에서는 잔차제곱합을 SSE(sum of square error)라고 부른다.\n잔차제곱합을 최소로 하는 회귀계수의 값을 찾는 최적화의 목표로 잔차제곱합이 제시될 때 이를 오차제곱합(error sum of square)이라고 부른다.\n\n\n위의 오차제곱합 \\(S(\\beta_0 , \\beta_1)\\) 을 최소화하는 \\(\\beta_0\\)와 \\(\\beta_1\\)의 값을 구하는 방법은 오차제곱합이 \\(\\beta_0\\)와 \\(\\beta_1\\)의 미분 가능한 2차 함수이고 아래로 볼록한 함수(convex function)임을 이용한다.\n\ngridnum &lt;- 60\nsizing &lt;- 5\nextrascale &lt;- 10\nextrascale2 &lt;- 0.7\nb0 &lt;- seq(-17.6-sizing*extrascale,  -17.6+sizing*extrascale, length=gridnum )\nb1 &lt;- seq(4-sizing*extrascale2, 4+sizing*extrascale2, length=gridnum )\n\nSSE &lt;- matrix(0, gridnum, gridnum )\nfor (i in 1:gridnum ) {\n  for (j in 1:gridnum ){\n    r &lt;- cars$dist- b0[i] -b1[j]*cars$speed\n    SSE[i,j]  &lt;- (sum(r^2))/1000\n  }\n}\n\npersp3D(b0, b1, SSE, theta =10, phi = 20, expand = 1)\n\n## Interactive 3d graph\n#fig &lt;- plot_ly(z = ~SSE)\n#fig &lt;- fig %&gt;% add_surface()\n#fig\n\n\n\n\n\n\n오차제곱합의 함수 형태\n\n\n\n\n위의 그림을 보면 볼록한 모양이 너무 평평하여 오차제곱합이 최소가 되는 \\(\\beta_0\\)와 \\(\\beta_1\\)의 위치가 명확하지 않다.\n이제 모든 변수들을 표준화하고 표준화된 변수들에 단순회귀모형에 대한 오차제곱합을 \\(\\beta_0\\)와 \\(\\beta_1\\)의 함수로서 그림을 그리면 아래와 같다.\n\\[\nv_i = \\beta_0 + \\beta_1 w_i + e_i, \\quad i=1,2,\\dots,n\n\\tag{1.4}\\]\n여기서\n\\[ v_i  = \\frac{y_i -\\bar y}{s_y}, \\quad w_i = \\frac{x_i -\\bar x}{s_x} \\]\n\n# 변수들을 표준화!\nstd_cars &lt;- as.data.frame(scale(cars))\ngridnum &lt;- 60\nsizing &lt;- 1\nb0 &lt;- seq(0-sizing,  0+sizing, length=gridnum )\nb1 &lt;- seq(1-sizing, 1+sizing, length=gridnum )\n\nSSE &lt;- matrix(0, gridnum, gridnum )\nfor (i in 1:gridnum ) {\n  for (j in 1:gridnum ){\n    r &lt;- std_cars$dist- b0[i] -b1[j]*std_cars$speed\n    SSE[i,j]  &lt;- sum(r^2)\n  }\n}\n\npersp3D(b0, b1, SSE, theta =40, phi = 15, expand = 1)\n\n## Interactive 3d graph\n#fig &lt;- plot_ly(z = ~SSE)\n#fig &lt;- fig %&gt;% add_surface()\n#fig\n\n\n\n\n\n\n표준화 시 오차제곱합의 함수 형태\n\n\n\n\n위위 같이 변수들을 표준화하면 오차제곱합 함수의 볼록한 정도가 덜 평평하게 변하여 최적값을 더 확실하게 보인다. 기계학습이나 인공지능 모형에서 적합하기 전에 모든 변수를 표준화하는 이유가 위의 그림에서 나타난다.\n식 1.3 의 오차제곱합을 각 회귀계수에 대해서 편미분을 하고 0으로 놓으면 아래와 같이 두 방정식이 얻어진다.\n\\[\n\\begin{aligned}\n\\pardiff{ S(\\beta_0 , \\beta_1)}{\\beta_0} & = \\sum^n_{i=1}(-2)[y_i-(\\beta_0+\\beta_1 x_i)]=0    \\\\ \\notag\n\\pardiff{ S(\\beta_0 , \\beta_1)}{\\beta_1}  & = \\sum^n_{i=1}(-2 x_i)[y_i-(\\beta_0+\\beta_1 x_i)]=0\n\\end{aligned}\n\\tag{1.5}\\]\n위의 연립방정식을 행렬식으로 표시하면 다음과 같이 나타낼 수 있다.\n\\[\n\\begin{bmatrix}\nn & \\sum_i x_i \\\\\n\\sum_i x_i & \\sum_i x^2_i\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sum_i y_i \\\\\n\\sum_i x_i y_i\n\\end{bmatrix}\n\\]\n위의 방정식을 풀어서 구한 회귀계수의 추정치를 \\(\\hat \\beta_0\\), \\(\\hat \\beta_1\\) 이라고 하면 다음과 같이 주어진다.\n\\[\n\\begin{aligned}\n\\hat \\beta_0 &= \\bar y - \\hat \\beta_1 \\bar x  \\\\\n  \\hat \\beta_1 &=  \\frac{ \\sum_i (x_i - \\bar x)(y_i - \\bar y)}{\\sum_i (x_i - \\bar x)^2}\n\\end{aligned}\n\\]\n최소제곱법에서 얻어진 회귀계수의 추정량 \\(\\hat \\beta_0\\)과 \\(\\hat \\beta_1\\) 을 이용한 반응변수 \\(y_i\\) 에 대한 예측값 \\(\\hat y_i\\)는 다음과 같이 정의되고\n\\[ \\hat y_i = \\hat E(y_i|x_i) = \\hat \\beta_0 + \\hat \\beta_1 x_i \\]\n\n\n\n\n\n\n표준화 전과 후\n\n\n\n두 개의 회귀방정식 식 1.2 과 식 1.4 에서 각각 최소제곱법으로 구한 기울기의 추정치 \\(\\hat \\beta_1\\) 이 동일하게 나타나는 경우는 어떤 경우일까 생각해보자.\n\n\n잔차 \\(r_i\\)는 다음과 같이 계산한다.\n\\[\nr_i = y_i - (\\hat \\beta_0 + \\hat \\beta_1 x_i) = y_i -\\hat y_i  \n\\tag{1.6}\\]\n잔차 \\(r_i\\)는 다음과 같은 성질을 가진다.\n\\[ \\sum_{i=1}^n r_i = 0 \\]\n\\[ \\sum_{i=1}^n x_i r_i = 0 \\]\n이제 위에서 본 cars 자료를 가지고 선형회귀모형 식 1.2 에 나타난 회귀계수를 추정해보자. 아래는 R 프로그램에서 함수 lm을 이용한 추정결과이다.\n\nlm_car &lt;- lm(dist~speed, data=cars)\nsummary(lm_car)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n위에서 주어진 선형회귀모형 식 1.2 에 대한 추정 결과를 이용하면 자동차의 속도(\\(x\\) = speed)와 제동거리(\\(y\\) = dist)의 관계는 다음과 같은 회귀식으로 나타낼 수 있다.\n\\[ \\hat E(y | x) = −17.58 + 3.93 x \\]\n\nggplot(cars, aes(x=speed, y=dist)) + geom_point() + labs(x = \"속도\", y = \"거리\") +\n  labs(title=\"자동차의 속도와 제동거리의 관계\") + \n  geom_abline(intercept = -17.58, slope = 3.93, color = \"red\")\n\n\n\n\n\n\n\n\n위의 추정식을 이용하면 주어진 자동차의 속도에서 제동거리를 예측할 수 있다. 예를 들어 자동차의 속도가 25 mph인 경우에는 제동거리의 평균이 80.73 mph 임을 알 수 있다.\n\\[ E(y|x=25) = −17.58 + 3.93 (25) = 80.73 \\]\n\nnewcars &lt;- data.frame(speed = c(25))\npredict(lm_car, newdata=newcars)\n\n       1 \n80.73112 \n\n\n기울기의 추정값 \\(\\hat \\beta_1 = 3.93\\) 은 자동차의 속도 (\\(x\\))가 1 mph 증가할 때 평균 제동거리 (\\(E(y|x)\\))가 3.93 ft 증가한다는 의미이다.\n이제 아파트 거애 가격에 대한 단순선형회귀모형을 적합해보자. 이 경우 면적과 가격대신 각각의 로그값을 사용하여 회귀모형을 적합해 보자. 아래는 아파트의 면적과 거래가격에 대한 단순선형 회귀모형을 적합한 결과이다.\n\napart_2019_log &lt;- apart_2019 %&gt;% \n  mutate(log_area = log10(area), log_price = log10(price)) %&gt;%\n  dplyr::select(log_area, log_price)\n\nhead(apart_2019_log,10)\n\n   log_area log_price\n1  1.813514  2.653213\n2  1.754807  2.440909\n3  1.960185  2.777427\n4  1.960185  2.748188\n5  1.780101  2.628389\n6  1.780101  2.623249\n7  1.590173  2.336460\n8  1.790567  2.685742\n9  1.856729  2.515874\n10 1.926548  2.641474\n\n\n\nlm_apart &lt;- lm( log_price~ log_area, data=apart_2019_log)\nsummary(lm_apart)\n\n\nCall:\nlm(formula = log_price ~ log_area, data = apart_2019_log)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45348 -0.12132 -0.04075  0.07531  0.62358 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.76902    0.05217   14.74   &lt;2e-16 ***\nlog_area     1.08797    0.02843   38.27   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1894 on 998 degrees of freedom\nMultiple R-squared:  0.5948,    Adjusted R-squared:  0.5944 \nF-statistic:  1465 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n위의 결과는 다음과 같이 나타낼 수 있다.\n\\[ \\hat E( \\log10 y | x) = 0.769 + 1.0797 \\log10 (x) \\]\n\nggplot(apart_2019_log, aes(x=log_area, y=log_price)) + geom_point() + labs(x = \"로그 면적(제곱미터)\", y = \"로그 거래가격(백만원)\") +\n  labs(title = \"아파트의 면적과 거래가격의 관계 (로그스케일)\") + \n  geom_abline(intercept = 0.769, slope = 1.0797, color = \"red\")\n\n\n\n\n\n\n\n\n이제 위의 결과를 응용하면 아파트의 면적이 100 제곱미터인 경우의 아파트의 평균 거래가격을 880(백만원)으로 예측할 수 있다.\n\nnewapart &lt;- data.frame(log_area = c(log10(100)))\npred_y &lt;- 10^predict(lm_apart, newdata=newapart)\npred_y\n\n       1 \n880.9605",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#결정계수",
    "href": "qmd/lse.html#결정계수",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.4 결정계수",
    "text": "1.4 결정계수\n고려한 설명변수와 반응변수에 대하여 제시된 회귀식을 적합한 후 회귀모형이 두 변수의 관계를 얼마나 잘 설명하는지에 대한 기준이 필요하다. 회귀식의 적합에 대한 기준으로서 결정계수(coefficient of determination; \\(R^2\\))가 있다. 결정계수는 적합의 정도(degree of fitting)를 측정한다. 즉 “설명변수는 반응변수를 얼마나 잘 예측하느냐”에 대한 정도를 수치로 표현한 것이다.\n회귀분석에서 설명변수와 반응변수 간에 전혀 관계가 없다면 당연히 반응변수의 값은 설명변수 값의 변동 여하에 전혀 영향을 받지 않아야 한다. 단순회귀모형에서 설명변수 \\(x\\)의 값의 변화를 반응변수 \\(y\\)로 값으로 표현하는것이 바로 기울기 \\(\\beta_1\\)이다. 이렇게 고려한 설명변수 \\(x\\)가 반응변수 \\(y\\)를 예측하는데 전혀 소용이 없다면 이는 기울기에 대한 회귀계수가 0 \\(\\beta_1=0\\) 이라는 것을 의미이다. 이러한 경우에 대하여 다음과 같은 모형을 생각할 수 있다.\n\\[\ny_i = \\beta_0 +e_i, \\quad e_i \\sim (0,\\sigma^2)\n\\tag{1.7}\\]\n기울기에 대한 회귀계수가 0인 경우에 대한 모형을 식 1.7 과 같이 표현할 수 있으며 평균 모형(mean model)이라고 부른다. 평균 모형은 우리가 생각할 수 있는 모형 중에서 가장 간단한, 하지만 별로 쓸모없는 모형이라고 할 수 있다.\n이러한 평균 모형에 대한 최소제곱법을 적용하여 \\(\\beta_0\\)의 추정량을 구하면 추정량 \\(\\hat \\beta_0\\)는 \\(\\bar y\\)가 된다. 그 이유는 위의 모형에 오차제곱합을 구해보면 다음과 같은 형식이 된다\n\\[ S(\\beta_0)=  \\sum^n_{i=1}[y_i-\\beta_0]^2 \\]\n여기서 \\(\\beta_0\\)에 대하여 최고로 하는 지점을 찾아보면 다음과 같은 방정식을 얻을 수 있다.\n\\[\n\\frac{\\partial S(\\beta_0)}{\\partial \\beta_0}  = 0  \\Rightarrow  \\sum^n_{i=1}[y_i - \\beta_0] = 0\n\\]\n이 방정식을 풀면 \\(\\hat \\beta_0 = \\bar y\\)가 됨을 알 수 있다. 결국 설명변수가 반응변수에 아무른 영향을 주지 못하게 되면 \\(y\\)의 예측값은 평균 \\(\\bar y\\) 임을 알 수 있다. 참고로 평균모형 식 1.7 경우 \\(\\bar y\\)는 \\(\\beta_0\\) 의 최소제곱추정량이다.\n여기서 주목해야할 점은 평균 모형 식 1.7 에서의 잔차 \\(r_{0i}\\)는 다음과 같이 정의된다.\n\\[ r_{0i} = y_i -\\hat \\beta_0 = y_i - \\bar y \\]\n주어진 회귀식이 유의한 경우, 즉 회귀식의 기울기가 0이 아닌 경우 (\\(\\beta_1 \\ne 0\\)) 적합된 회귀식에 대한 잔차는 식 1.6 과 같이 나타난다. 만약 회귀식이 유의하다면 식 1.6 으로 구해진 잔차 \\(r_i = y_i -\\hat \\beta_0 - \\hat \\beta_1 x_i\\) 와 평균 모형에서 구해지는 잔차 \\(r_{0i}  = y_i - \\bar y\\) 간의 어떤 차이가 있을까?\n아래의 그림은 앞의 예제 cars 자료에 대하여 설명변수가 없는 평균 모형(파란 선)과 설명변수가 있는 회귀모형(빨간 선)을 나타낸 그림이다. 잔차는 적합된 직선과 반응 변수 간의 차이를 의미하며 차이의 절대값이 작을 수록 좋은 모형이다.\n\nggplot(cars, aes(x=speed, y=dist)) +\n     geom_point() +\n     labs(x = \"속도\", y = \"거리\") +\n     geom_smooth(method = lm, color='red', se = FALSE) +\n     geom_hline(yintercept = mean(cars$dist), color='blue')\n\n\n\n\n\n\n\n\n잔차의 절대값보다 제곱한 양이 다루기가 쉬우므로(why?) 평균 모형과 회귀 모형의 적합도를 비교하는 양으로서 다음과 같은 각각의 모형에서 나온 두개의 잔차제곱합을 생각할 수 있다.\n먼저 평균 모형은 예측에 사용할 변수가 없는 경우로서 이때의 잔차는 각 관측값에 대한 예측값이 관측값의 평균이다 . 이러한 경우 잔차는 관측값 자체가 가지고 있는 변동으로 생각할 수 있다. 이러한 평균모형에서의 잔차 또는 관측값이 가지고 있는 변동을 총제곱합(Total Sum of Squares ; \\(SST\\))이다\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n r^2_{0i} & = \\sum_{i=1}^n (y_i -\\bar y)^2 \\\\\n  & =  \\text{ Residual Sum of Squares from mean model }  \\\\\n   & = \\text{ Variation of response variables} \\\\\n   & = \\text{ Total Sum of Squares } \\\\\n   & = SST\n\\end{aligned}\n\\]\n이제 설명변수가 있는 회귀모형에서 예측치 \\(\\hat y_i=\\hat \\beta_0 + \\hat \\beta_1 x_i\\)를 고려하면 이 경우의 잔차들의 제곱합은 회귀식의 잔차제곱합(Residual Sum of Squares; \\(SSE\\))이라고 부르며 아래와 같이 정의한다.\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n r_i^2 & = \\sum_{i=1}^n (y_i -\\hat \\beta_0 - \\hat \\beta_1 x_i) \\\\\n  & = \\text{Residual Sum of Squares from linear regression model } \\\\\n  & = \\text{ Residual Sum of Squares } \\\\\n  & = SSE\n\\end{aligned}\n\\]\n만약 회귀식에서 고려한 설명변수가 반응변수를 예측하는데 매우 적합하다면 회귀모형에서 구한 잔차들의 제곱합이 평균모형에서 구한 잔차들의 제곱합보다 작을 것이다. 이러한 차이를 비교하려면 두 제곱합 \\(SST\\) 와 \\(SSE\\)의 관계를 이해하는 것이 중요하다.\n두 제곱합 \\(SST\\) 와 \\(SSE\\)의 관계를 보기 위하여 먼저 두 잔차 \\(r^0_i\\) 와 \\(r_i\\)의 차이를 비교해 보자\n\\[ r^0_i - r_i = (y_i - \\bar y) - (y_i - \\hat y_i) = \\hat y_i - \\bar y \\]\n위의 식에서 두 잔차의 차이 \\(\\hat y_i - \\bar y\\)는 예측값과 평균 간 차이로서 그 절댸값이 크면 회귀직선이 반응변수를 설명할 수 있는 능력이 크다는 것을 의미한다.\n위의 식을 다시 쓰면 다음과 같다.\n\\[  (y_i - \\bar y) = (y_i - \\hat y_i) + (\\hat y_i - \\bar y) \\]\n즉, (평균모형의 잔차)=(회귀모형의 잔차))+(회귀모형의 설명부분)으로 분해되는 것으로 이해할 수 있다. 이 분해에서 회귀모형의 잔차가 작을수록 회귀 모형의 예측 능력, 즉 적합도가 커지는 것을 알 수 있다.\n이제 총제곱합은 다음과 같이 분해할 수 있다.\n\\[\n\\begin{aligned}\n\\sum^n_{i=1}(y_i - \\bar y)^2 &=  \\sum^n_{i=1}[(y_i-\\hat y_i)+(\\hat y_i - \\bar y)]^2 \\\\\n&= \\sum^n_{i=1}(y_i-\\hat y_i)^2+\\sum^n_{i=1}(\\hat y_i - \\bar y)^2\n        + 2\\sum^n_{i=1}(y_i-\\hat y_i)(\\hat y_i-\\bar y)  \\\\\n&= \\sum^n_{i=1}(y_i-\\hat y_i)^2+\\sum^n_{i=1}(\\hat y_i - \\bar y)^2 + 0 \\quad \\text{(why?)}\n\\end{aligned}\n\\]\n따라서 다음과 같은 제곱합의 분해를 얻게 된다.\n\\[\n\\sum^n_{i=1}(y_i - \\bar y)^2 = \\sum^n_{i=1}(y_i-\\hat y_i)^2+\\sum^n_{i=1}(\\hat y_i - \\bar y)^2\n\\] 여기서 모형제곱합(regression sum of square; SSR)를 다음과 같이 정의하면\n\\[  SSR = \\sum^n_{i=1}(\\hat y_i-\\bar y_i)^2 \\]\n총제곱합은 잔차제곱합 과 모형제곱합으로 분해된다.\n\\[\nSST = SSE + SSR\n\\tag{1.8}\\]\n관측값들이 보여주는 총 변동인 총제곱합(SST)에서 회귀모형으로 설명할 수 있는 변동, 즉 모형제곱합(SSR)이 차지하는 비율을 결정계수(coefficient of determination)라 하며 \\(R^2\\)으로 표현한다.\n\\[\nR^2 = \\frac{SSR}{SST} =  1 -\\frac{SSE}{SST}  =1- \\frac{\\sum^n_{i=1}(y_i-\\hat y_i)^2}{ \\sum^n_{i=1}(y_i - \\bar y)^2}\n\\]\n위에서 정의된 \\(R^2\\) 는 평균 모형의 잔차제곱합 \\(SST\\)과 회귀모형의 잔차제곱합 \\(SSE\\)의 비율로 정의되는 것으로 해석할 수 있다. 즉,\n\\[  \nR^2 = 1 -\\frac{SSE}{SST} =1 -\\frac{\\text{Residual SS from regression model}}{\\text{Residual SS from mean model}}\n\\]\n결정계수의 정의를 보면 회귀모형의 잔차제곱합(\\(SSE\\))가 평균 모형의 잔차제곱합(\\(SST\\))에 대하여 상대적으로 작아질수록 결정계수가 커진다. 결정계수 \\(R^2\\)는 언제나 0 이상 1 이하의 값을 갖는다. 회귀모형이 데이터에 아주 잘 적합되면 결정계수의 값은 1 에 가깝게 된다.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#중회귀-모형",
    "href": "qmd/lse.html#중회귀-모형",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.5 중회귀 모형",
    "text": "1.5 중회귀 모형\n일반적으로 회귀모형에서 반응변수의 수는 하나인 경우가 많지만 설명변수의 수는 여러 개인 경우가 많다. 이런 경우 중회귀 모형(multiple linear regression)은 다음과 같이 표현할 수 있고, \\(p-\\)개의 설명변수가 있다고 가정하고 \\((x_1, x_2, \\cdots, x_{p-})\\) 표본의 크기 \\(n\\)인 자료가 얻어지면 선형회귀식을 행렬로 다음과 같이 표현할 수 있다.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{p} x_{i,p} +  e_i = \\pmb x^t_i \\pmb \\beta +  e_i\n\\tag{1.9}\\]\n위의 식을 다시 표현하면 다음과 같이 쓸 수 있다.\n\\[\ny_i  = \\pmb x^t_i \\pmb \\beta  + e_i  =\n\\begin{bmatrix}\n1 & x_{i1} & x_{i2} & \\cdots & x_{i,p}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_{0} \\\\\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\vdots \\\\\n\\beta_{p}\n\\end{bmatrix}\n+ e_i\n\\]\n이제 \\(n\\)개의 관측치 \\(y_1,y_2, \\dots, y_n\\) 으로 이루어진 관측값 벡터 \\(\\pmb y\\)를 고려하면 n개의 관측치에 대한 회귀식을 행렬식으로 다음과 같이 표현할 수 있다.\n\\[\n\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_{11} & \\cdots & x_{1,p} \\\\\n1 & x_{21} & \\cdots & x_{2,p} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & x_{n1} & \\cdots & x_{n,p}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_{0} \\\\\n\\beta_{1} \\\\\n\\vdots \\\\\n\\beta_{p}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\ne_{1} \\\\\ne_{2} \\\\\n\\vdots \\\\\ne_{n}\n\\end{bmatrix}\n\\]\n위의 식을 간단히 행렬식으로 표시하면 다음과 같다.\n\\[\n\\pmb y = \\pmb X \\pmb \\beta + \\pmb e\n\\tag{1.10}\\]\n위의 행렬식에서 각 벡터와 행렬의 차원은 다음과 같다.\n\n\\(\\pmb y\\): \\(n \\times 1\\)\n\\(\\pmb X\\): \\(n \\times (p+1)\\)\n\\(\\pmb \\beta\\): \\((p+1) \\times 1\\)\n\\(\\pmb e\\): \\(n \\times 1\\)\n\n여기서 회귀분석의 오차항 \\(e_i\\)은 서로 설명이고 동일한 분산을 갖는다. 즉, 오차항은 다음의 분포를 따른다.\n\\[ \\pmb e  \\sim (\\pmb 0,\\sigma^2 \\pmb I_n) \\]\n따라서 관측값 벡터 \\(\\pmb y\\)의 평균은 다음과 같고\n\\[\nE(\\pmb y|\\pmb X) = E(\\pmb X \\pmb \\beta+\\pmb e)= \\pmb X \\pmb \\beta + E(\\pmb e) = \\pmb X \\pmb \\beta\n\\tag{1.11}\\]\n\\(\\pmb y\\)의 분산은 아래와 같이 주어진다.\n\\[\nV( \\pmb y| \\pmb X) = E[( \\pmb y -  \\pmb X \\pmb \\beta)( \\pmb y -  \\pmb X \\pmb \\beta)^t] = E( \\pmb  e   \\pmb  e^t) = \\sigma^2 \\pmb I_n\n\\tag{1.12}\\]\n여기서 오차항이 정규분포를 따른다면\n\\[ \\pmb  e   \\sim N(\\pmb 0,\\sigma^2 \\pmb I_n) \\]\n관측값 벡터 \\(\\pmb y\\) 또한 정규분포를 따른다\n\\[  \\pmb y \\sim N( \\pmb X \\pmb \\beta, \\sigma^2 \\pmb I_n) \\]\n\n1.5.1 최소제곱추정\n이제 중회귀모형 식 1.10 에서 회귀계수벡터 \\(\\pmb \\beta\\)의 추정량을 구하기 위하여 최소제곱법을 적용해보자.\n\\[\n\\min_{\\pmb \\beta} \\sum_{i=1}^n (y_i -  \\pmb x_i^t \\pmb \\beta )^2 = \\min_{\\pmb \\beta } ( \\pmb y -  \\pmb X \\pmb \\beta )^t( \\pmb y -  \\pmb X \\pmb \\beta )\n\\tag{1.13}\\]\n\n1.5.1.1 방법 1\n\\(\\hat {\\pmb \\beta}\\)는 잔차의 제곱합 식 1.13 을 최소로 하는 최소제곱 추정량이다. 잔차의 제곱합을 \\(S( \\pmb \\beta)\\)이라고 하면\n\\[\n\\begin{aligned}\nS( \\pmb \\beta ) & =  ( \\pmb y -  \\pmb X \\pmb \\beta)^t( \\pmb y -  \\pmb X \\pmb \\beta ) \\notag \\\\\n  & = \\pmb y^t \\pmb y - \\pmb y^t \\pmb X \\pmb \\beta - \\pmb \\beta^t \\pmb X^t \\pmb y\n    + \\pmb \\beta^t \\pmb X^t \\pmb X \\pmb \\beta \\notag \\\\\n  & = \\pmb y^t \\pmb y -2  \\pmb \\beta^t \\pmb X^t \\pmb y\n    + \\pmb \\beta^t \\pmb X^t \\pmb X \\pmb \\beta\n\\end{aligned}\n\\tag{1.14}\\]\n여기서 \\(S( \\pmb \\beta)\\)를 최소로 하는 회귀계수벡터의 값을 구하기 위하여 \\(S( \\pmb \\beta)\\)를 회귀계수벡터 \\(\\pmb \\beta\\)로 미분한후 \\(\\pmb 0\\) 으로 놓고 선형 방정식을 풀어야 한다.\n벡터미분을 이용하면\n\\[\n\\begin{aligned}\n\\pardifftwo{ S( {\\pmb \\beta})}{\\pmb \\beta} & =\n\\pardifftwo{}{\\pmb \\beta} (\\pmb y^t \\pmb y -2  \\pmb \\beta^t \\pmb X^t \\pmb y\n    + \\pmb \\beta^t \\pmb X^t \\pmb X \\pmb \\beta) \\\\\n& = \\pmb 0 -2 \\pmb X^t \\pmb y + 2 \\pmb X^t \\pmb X \\pmb \\beta \\\\\n& =\\pmb 0\n\\end{aligned}\n\\]\n최소제곱 추정량을 구하기 위한 정규방정식은 다음과 같이 쓸 수 있다.\n\\[\n\\pmb X^t \\pmb X \\pmb \\beta =  \\pmb X^t \\pmb y\n\\tag{1.15}\\]\n방정식 식 1.15 를 정규방정식(normal equation)이라고 한다. 만약 \\(\\pmb X^t \\pmb X\\)가 정칙행렬일 경우 최소제곱법에 의한 회귀계수 추정량 \\(\\hat {\\pmb \\beta}\\) 다음과 같다.\n\\[\n\\hat {\\pmb \\beta} = ( \\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y\n\\tag{1.16}\\]\n예측값 벡터 \\(\\hat {\\pmb y}\\) 는 \\(E(\\pmb y | \\pmb X)\\)의 추정치로서 다음과 같다.\n\\[ \\hat E(\\pmb y | \\pmb X)= \\hat {\\pmb y} = \\pmb X \\hat {\\pmb \\beta} = \\pmb  X(\\pmb X^t \\pmb X)^{-1} \\pmb X^t y \\]\n만약 \\(\\pmb X^t \\pmb X\\)가 정칙행렬이 아닐 경우 최소제곱법에 의한 회귀계수 추정량 \\(\\hat {\\pmb \\beta}\\)은 \\(\\pmb X^t \\pmb X\\)의 일반화 역행렬 \\((\\pmb X^t \\pmb X)^-\\)를 이용하여 다음과 같이 구한다. 이 경우 일반화 역행렬이 유일하지 않기 때문에 회귀계수 추정량도 유일하지 않다.\n\\[\n  \\hat {\\pmb \\beta} = ( \\pmb X^t \\pmb X)^{-} \\pmb X^t \\pmb y\n\\]\n\n\n1.5.1.2 방법 2\n식 1.13 에서 나오는 오차벡터를 정의하고 \\(\\pmb e = (\\pmb y - \\pmb X \\pmb \\beta)\\) 오차벡터를 모수벡터 \\(\\pmb \\beta\\)로 미분하면 다음과 같은 결과를 얻는다.\n\\[\n\\pardifftwo{\\pmb e}{\\pmb \\beta} = \\pardifftwo{ (\\pmb y - \\pmb X \\pmb \\beta)}{ \\pmb \\beta} =\n- \\pardifftwo{ \\pmb X \\pmb \\beta}{ \\pmb \\beta} \\equiv\n- \\pardifftwo{\\pmb \\beta^t \\pmb X^t }{\\pmb \\beta} = -\\pmb X^t\n\\]\n이제 오차제곱합 \\(S( {\\pmb \\beta})=\\pmb e^t \\pmb e\\) 를 모수벡터로 미분하면 이차형식의 미분공식과 합성함수 미분공식을 차례로 적용하면 된다.\n\\[  \n\\pardifftwo{ S( {\\pmb \\beta})}{\\pmb \\beta}=\\pardifftwo{\\pmb e^t \\pmb e}{\\pmb \\beta} =  \\pardifftwo{\\pmb e }{\\pmb \\beta} \\pardifftwo{\\pmb e^t  \\pmb e}{\\pmb e} = -\\pmb X^t \\left( 2 \\pmb e \\right ) = -2 \\pmb X^t (\\pmb y - \\pmb X \\pmb \\beta)  \n\\]\n위의 방정식을 \\(\\pmb 0\\)으로 놓으면 최소제곱 추정량 (열)벡터를 구한다.\n\\[\n\\pmb X^t \\pmb y - \\pmb X^t  \\pmb X \\pmb \\beta = \\pmb 0 \\quad \\rightarrow \\quad\n\\hat{\\pmb \\beta}  = (\\pmb X^t  \\pmb X)^{-1} \\pmb X^t  \\pmb y\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#예제-중회귀모형",
    "href": "qmd/lse.html#예제-중회귀모형",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.6 예제-중회귀모형",
    "text": "1.6 예제-중회귀모형\n\n보기 1.3 (중고차 가격자료) 강의 부교재의 usedcars 자료를 이용하여 중회귀모형을 적합해보자. 자료를 구성하는 변수는 다음과 같다.\n\nprice : 자동차 가격\nyesr :연식\nmileage : 주행거리\ncc : 엔진 크기\nautomatic : 자동 변속기 여부\n\n\nusedcars %&gt;% head(n=10) \n\n   price year mileage   cc automatic\n1    790   78  133462 1998         1\n2   1380   39   33000 2000         1\n3    270  109  120000 1800         0\n4   1190   20   69727 1999         1\n5    590   70  112000 2000         0\n6   1120   58   39106 1998         1\n7    815   53   95935 1800         1\n8    450   68  120000 1800         0\n9   1290   15   20215 1798         1\n10   420   96  140000 1800         0\n\n\n자동차의 가격을 반응변수로 한고 나머지 변수를 설명변수로 설정한 중회귀 모형에 대한 모수의 추정 결과는 다음과 같다.\n\nusedcars_lm &lt;- lm(price ~ year + mileage + cc + automatic, data=usedcars)\nsummary(usedcars_lm)\n\n\nCall:\nlm(formula = price ~ year + mileage + cc + automatic, data = usedcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-177.35  -63.91   -0.99   70.34  212.69 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.253e+02  3.998e+02   1.314 0.200823    \nyear        -5.800e+00  9.283e-01  -6.247 1.55e-06 ***\nmileage     -2.263e-03  7.211e-04  -3.138 0.004324 ** \ncc           3.888e-01  2.022e-01   1.923 0.065958 .  \nautomatic    1.653e+02  3.986e+01   4.147 0.000339 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 101.1 on 25 degrees of freedom\nMultiple R-squared:  0.9045,    Adjusted R-squared:  0.8892 \nF-statistic: 59.21 on 4 and 25 DF,  p-value: 2.184e-12",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#최소제곱-추정량의-분포",
    "href": "qmd/lse.html#최소제곱-추정량의-분포",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.7 최소제곱 추정량의 분포",
    "text": "1.7 최소제곱 추정량의 분포\n회귀식을 추정하기 위한 회귀계수 추정값인 \\(\\hat {\\pmb \\beta}\\)의 분포를 알아보기 위해서 우선 선형추정량을 보면 다음과 같다.\n\\[ \\hat {\\pmb \\beta}  = (\\pmb X^t\\pmb X)^{-1}\\pmb X' \\pmb y \\equiv  \\pmb M \\pmb y \\] 따라서 최소제곱 추정량은 관측값들의 선형 변환이다. 회귀계수 추정값 \\(\\hat {\\pmb \\beta}\\)의 기대값은\n\\[\n\\begin{aligned}\nE( \\hat {\\pmb \\beta}  ) &= E( \\pmb M \\pmb y) = E(( \\pmb X^t \\pmb X)^{-1} \\pmb X' \\pmb y) \\\\\n&= ( \\pmb X^t \\pmb X)^{-1} \\pmb X'E( \\pmb y) \\\\\n&= ( \\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb X \\pmb \\beta \\\\\n  &= \\pmb \\beta\n\\end{aligned}\n\\]\n따라서 최소제곱 추정량 \\(\\hat {\\pmb \\beta}\\)는 \\(\\pmb \\beta\\)의 불편추정량이다. 최소제곱 추정량 \\(\\hat {\\pmb \\beta}\\)의 공분산 행렬을 전개해보면\n\\[\n\\begin{aligned}\nVar( \\hat {\\pmb \\beta} ) &= Var(( \\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y) \\\\\n&= ( \\pmb X^t \\pmb X)^{-1} \\pmb X^t ~ Var( \\pmb y) ~ \\pmb X ( \\pmb X^t \\pmb X)^{-1} \\\\\n&=( \\pmb X^t \\pmb X)^{-1} \\pmb X^t[\\sigma^2  \\pmb I_n] \\pmb X( \\pmb X^t \\pmb X)^{-1} \\\\\n&= \\sigma^2( \\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb X( \\pmb X^t \\pmb X)^{-1} \\\\\n&= \\sigma^2( \\pmb X^t \\pmb X)^{-1} \\\\\n\\end{aligned}\n\\]\n위에서 최소제곱 추정량의 평균과 공분산을 구할 때에는 정규성 가정이 필요하지않다. 만일 \\(\\pmb y\\)가 정규분포를 따른다면 \\(\\pmb y\\)의 선형변환으로부터 얻어진 \\(\\hat {\\pmb \\beta}\\)의 분포는 정규분포이며 다음과 같다.\n\\[  \\hat {\\pmb \\beta}  \\sim N \\left (\\pmb \\beta, \\sigma^2( \\pmb X^t \\pmb X)^{-1} \\right ) \\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#가우스-마코브-정리",
    "href": "qmd/lse.html#가우스-마코브-정리",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.8 가우스-마코브 정리",
    "text": "1.8 가우스-마코브 정리\n\n정리 1.1 (가우스-마코브 정리) 선형회귀모형 \\(\\pmb y =  \\pmb X  \\pmb \\beta +  \\pmb e\\)에서 \\(E( \\pmb e)=0, Var( \\pmb e)=\\sigma^2 \\pmb I\\)이 성립하면 최소제곱 추정량\n\\[ \\hat{\\pmb \\beta}=(\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y\\]\n는 \\(\\pmb \\beta\\)의 최소분산 선형 불편추정량이다.\n\n위의 정리를 가우스-마코브 정리 (Gauss-Markov Theorem)라고 하며 이는 회귀계수 \\(\\pmb \\beta\\)의 모든 선형 불편 추정량들 중에 최소제곱 추정량 \\(\\hat {\\pmb \\beta}=(\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y\\)이 가장 작은 분산을 가짐을 뜻한다 (Best Linear Unbiased Estimator; BLUE).\n가우스-마코브 정리를 정확하게 표현하면 \\(E(\\pmb L \\pmb y) = \\pmb \\beta\\)를 만족하는 모든 \\(n \\times n\\) 차원의 행렬 \\(\\pmb L\\)과 임의의 벡터 \\(\\pmb c\\)에 대하여 다음이 성립한다.\n\\[ V(\\pmb c^t \\hat {\\pmb \\beta}) \\le V(\\pmb c^t \\pmb L \\pmb y)  \\]\n이제 가우스-마코브 정리를 증명해보자. 관측벡터 \\(\\pmb y\\)에 대한 임의의 선형 추정량 \\(\\pmb \\beta^* = \\pmb L \\pmb y\\)를 생각해보면 다시 다음의 형태로 표시할 수 있다.\n\\[  \n\\pmb \\beta^* =  \\pmb L  \\pmb y = (\\pmb M + \\pmb L -\\pmb M ) \\pmb y = ( \\pmb M +  \\pmb A)  \\pmb y\n\\]\n여기서 \\(\\pmb M = ( \\pmb X^t \\pmb X)^{-1} \\pmb X^t\\) 이고 \\(\\pmb A= \\pmb L- \\pmb M\\) 이다. 임의의 선형 추정량 \\(\\pmb \\beta^*\\)가 불편 추정량일 조건을 구해보자\n\\[\n\\begin{aligned}\nE( \\pmb \\beta^*) & = E[( \\pmb M+ \\pmb A) \\pmb y] \\\\\n            & = ( \\pmb M+ \\pmb A)E( \\pmb y) \\\\\n            & = ( \\pmb M+ \\pmb A)X \\pmb \\beta \\\\\n            & = ( \\pmb X^t  \\pmb X)^{-1} \\pmb X^t  \\pmb X  \\pmb \\beta +  \\pmb A  \\pmb X  \\pmb \\beta \\\\\n            & =  \\pmb \\beta+AX \\pmb \\beta\\\\\n\\end{aligned}\n\\]\n여기서 불편추정량이 되기 위해서는 \\(E( \\pmb \\beta^*)= \\pmb \\beta\\) 조건을 만족 해야되며 따라서 \\(\\pmb A \\pmb X=0\\)이되어야한다 (이 조건은 \\(\\pmb A=0\\)를 의미하는 것은 아니다).\n이제 최소분산을 가지기 위해서 \\(\\pmb A \\pmb X=0\\)을 만족하는 행렬 \\(\\pmb A\\)중에서 \\(Var( \\pmb \\beta^*)\\)을 최소로하는 행렬 \\(\\pmb A\\)를 구해야 한다. \\(\\pmb \\beta^*\\)의 공분산 행렬은 \\(AX=0\\)이므로\n\\[\n\\begin{aligned}\nV( \\pmb \\beta^*) & = ( \\pmb M+ \\pmb A)V( \\pmb y)( \\pmb M+ \\pmb A)^t\\\\\n            & = ( \\pmb M+ \\pmb A)\\sigma^2 I_n( \\pmb M+ \\pmb A)^t\\\\\n            & = \\sigma^2 ( \\pmb M \\pmb M^t+ \\pmb A \\pmb M^t+ \\pmb M  \\pmb A^t+ \\pmb A  \\pmb A^t)\\\\\n            & = \\sigma^2 [ ( \\pmb X^t  \\pmb X)^{-1}  \\pmb X^t  \\pmb X( \\pmb X^t \\pmb X)^{-1} + \\pmb A  \\pmb X ( \\pmb X^t  \\pmb X)^{-1}+( \\pmb X^t  \\pmb X)^{-1} \\pmb X^t  \\pmb A^t+  \\pmb A  \\pmb A^t ]\\\\\n            & = \\sigma^2[( \\pmb X^t  \\pmb X)^{-1} +  \\pmb A  \\pmb A^t ]  \\\\\n            & = V( \\hat {\\pmb \\beta} ) + \\sigma^2 \\pmb A \\pmb A^t\\\\\n\\end{aligned}\n\\]\n이제 임의의 벡터 \\(\\pmb c\\)에 대하여\n\\[\n\\begin{aligned}\nV( \\pmb c^t \\pmb \\beta^*)  & = \\pmb c^t V (\\pmb \\beta^*) \\pmb c \\\\\n   & =  \\pmb c^t V( \\hat {\\pmb \\beta} ) \\pmb c + \\sigma^2 \\pmb c^t \\pmb A \\pmb A^t \\pmb c \\\\\n    & =  V( \\pmb c^t  \\hat {\\pmb \\beta} ) + \\sigma^2 \\pmb c^t \\pmb A \\pmb A^t \\pmb c\n\\end{aligned}\n\\]\n다음이 성립하므로\n\\[ \\pmb c^t \\pmb A \\pmb A^t \\pmb c = \\pmb u^t \\pmb u = \\sum_{i=1}^n u_i^2 \\ge 0 \\]\n임의의 벡터 \\(\\pmb c\\)에 대하여\n\\[ V( \\pmb c^t \\pmb \\beta^*) \\ge  V( \\pmb c^t  \\hat {\\pmb \\beta} ) \\]\n이제 \\(V( \\pmb c^t \\pmb \\beta^*)\\) 이 \\(V( \\pmb c^t \\hat {\\pmb \\beta} )\\)과 같으려면 다음 조건이 성립해야 하며\n\\[ \\pmb u = \\pmb c^t \\pmb A = \\pmb 0 \\]\n임의의 모든 벡터 \\(\\pmb c\\)에 대해서 위의 조건 성립해야 하므로 이는 \\(\\pmb A = \\pmb 0\\) 이 성립해야 한다. 또한 이조건은 \\(\\pmb A \\pmb X=0\\)도 만족 시켜준다. 따라서 \\(\\pmb \\beta\\)의 최소분산 선형 불편추정량은 최소제곱법으로 구한 추정량이다.\n여기서 주의할 점은 가우스-마코브 정리에서 관측값 \\(\\pmb y\\)에 대한 가정은 평균과 공분산의 가정만 주어졌으며 \\(\\pmb y\\)의 분포에 대한 가정이 없다. 참고로 만약에 \\(\\pmb y\\)가 정규분포를 따른다면 최소제곱 추정량은 최소분산 불편추정량이다.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/lse.html#최대가능도-추정",
    "href": "qmd/lse.html#최대가능도-추정",
    "title": "1  선형 회귀모형의 소개",
    "section": "1.9 최대가능도 추정",
    "text": "1.9 최대가능도 추정\n관측값 벡터 \\(\\pmb y\\) 가 다음과 같이 선형모형이며 정규분포를 따른다고 가정하자.\n\\[\n\\pmb y \\sim N( \\pmb X \\pmb \\beta, \\sigma^2 \\pmb I_n)\n\\tag{1.17}\\]\n선형모형 식 1.17 에 대한 가능도 함수는 다음과 같이 주어진다.\n\\[\n\\begin{aligned}\nL_n( \\pmb \\theta ;  \\pmb y) & = L( \\pmb \\beta,\\sigma^2|  \\pmb y) \\\\\n   & = \\prod^n_{i=1} f(y_i)\\\\\n   & = \\prod^n_{i=1}(2 \\pi \\sigma^2)^{-\\frac{1}{2}} \\exp \\left [-\\frac{1}{2\\sigma^2} (y_i- {\\pmb x}_i^t \\pmb \\beta)^2 \\right ] \\\\\n   & = (2\\pi\\sigma^2)^{-\\frac{n}{2}} \\exp \\left [ -\\frac{1}{2\\sigma^2}( \\pmb y- \\pmb  X  \\pmb \\beta)^t( \\pmb y- \\pmb X  \\pmb \\beta) \\right ]\n\\end{aligned}\n\\]\n또한 분산에 대한 모수를 \\(\\tau=\\sigma^2\\) 과 같이 쓰면 로그 가능도함수는 다음과 같다.\n\\[\n\\begin{aligned}\n\\ell_n( \\pmb \\theta; \\pmb y) & = -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\sigma^2 -\\frac { ( \\pmb y- \\pmb X  \\pmb \\beta)^t (\\pmb  y- \\pmb X  \\pmb \\beta) }{2\\sigma^2} \\\\\n   &= -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac { ( \\pmb y- \\pmb X  \\pmb \\beta)^t ( \\pmb y- \\pmb X  \\pmb \\beta) }{2\\tau}  \n\\end{aligned}\n\\]\n이제 로그가능도함수로부터 구할 수 있는 스코어함수 \\(s( \\pmb \\theta;\\pmb y)\\) 와 그에 대한 관측 피셔정보 \\(J_n( \\pmb \\theta; \\pmb y)\\) 은 다음과 같이 주어진다.\n\\[\n\\begin{aligned}\ns( \\pmb \\theta;  \\pmb y) & =  \\pardifftwo{}{ \\pmb \\theta}\\ell_n( \\pmb \\theta;  \\pmb y ) \\\\\n  & =  \\begin{bmatrix}\n    \\pardifftwo{}{ \\pmb  \\beta}\\ell_n( \\pmb \\theta;  \\pmb y ) \\\\\n    \\pardifftwo{}{\\tau}\\ell_n( \\pmb \\theta;  \\pmb y )\n  \\end{bmatrix} \\\\\n  & =\n  \\begin{bmatrix}\n     \\pmb X^t ( \\pmb y- \\pmb X  \\pmb \\beta)/\\tau \\\\\n    -\\frac{n}{2\\tau} +\\frac { ( \\pmb y- \\pmb X  \\pmb \\beta)^t ( \\pmb y- \\pmb X  \\pmb \\beta) }{2\\tau^2}\n  \\end{bmatrix}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nJ_n( \\pmb \\theta;  \\pmb y) & =  -\\pardiffdd{}{ \\pmb \\theta}{ {\\pmb \\theta}} \\ell_n( \\pmb \\theta;\\pmb y ) \\\\\n   & =\n  - \\begin{bmatrix}\n    \\pardiffdd{}{\\pmb \\beta}{ {\\pmb \\beta}}\\ell_n( \\pmb \\theta;\\pmb y ) & \\pardiffdd{}{ \\pmb \\beta}{\\tau} \\ell_n( \\pmb \\theta;\\pmb y )  \\\\\n    \\pardiffdd{}{\\tau}{ {\\pmb \\beta}} \\ell_n( \\pmb \\theta;\\pmb y )  & \\pardiffdd{}{\\tau}{\\tau}\\ell_n( \\pmb \\theta; \\pmb y )\n  \\end{bmatrix} \\\\\n   & =\n  \\begin{bmatrix}\n      {\\pmb X}^t  \\pmb X /\\tau &  - {\\pmb X}^t ( \\pmb y- \\pmb X  \\pmb \\beta)/\\tau^2   \\\\\n    - ( \\pmb y- \\pmb X  \\pmb \\beta)^t {\\pmb X} /\\tau^2  &  - \\frac{n}{2\\tau^2} +\\frac { ( \\pmb y- \\pmb X \\pmb  \\beta)^t ( \\pmb y- \\pmb X  \\pmb \\beta) }{\\tau^3}\n  \\end{bmatrix}\n\\end{aligned}\n\\]\n이제 중회귀모형에서 회귀계수 \\(\\pmb \\beta\\)에 대한 최대가능도 추정량은 스코어함수로 부터 얻어진 방정식 \\(s( \\pmb \\theta; y)= 0\\) 으로부터 얻어지며 다음과 같은 형태를 가진다.\n\\[ \\hat { \\beta} = ( {\\pmb X}^t  \\pmb X)^{-1} {\\pmb  X}^t  \\pmb y \\]\n\\[   \\hat \\sigma^2 = \\hat \\tau = ( \\pmb y-\\pmb  X  \\hat {\\pmb \\beta})^t ( \\pmb y- \\pmb X  \\hat {\\pmb \\beta})/n = \\frac{SSE(\\hat{\\pmb  \\beta})}{n} \\]\n여기서 유의할 점은 회귀계수 \\(\\pmb \\beta\\) 의 최대가능도 추정량은 최소제곱법으로 구한 추정량과 동일하다. 따라서 \\(\\hat {\\pmb  \\beta}\\)은 최소분산 불편 추정량이다. 하지만 오차항의 분산 \\(\\sigma^2\\) 에 대한 최대가능도 추정량은 불편추정량이 아니다.\n\\[ E(\\hat \\sigma^2)  = E \\left [ ( \\pmb y- \\pmb X  {\\hat {\\pmb \\beta}})^t ( \\pmb y-\\pmb  X  {\\hat {\\pmb \\beta}})/n \\right ] = E \\left [ \\frac{SSE}{n} \\right ]  \\ne \\sigma^2 \\]\n참고로 오차항의 분산 \\(\\sigma^2\\)에 대한 불편추정량은 \\(SSE/(n-p)\\)이다. 오차항의 분산에 대한 불편추정량은 다음 장에서 논의할 것이다.\n최대가능도 추정량의 점근적 분포를 이용하면 다음과 같이 말할 수 있다. 오차항이 정규분포인 선형모형인 경우 아래의 분포는 점근분포가 아닌 정확한 분포이다.\n\\[ \\hat { \\pmb \\theta}  \\sim  N( \\pmb \\theta ,  I_n^{-1}( \\pmb \\theta)) \\]\n여기서\n\\[  I_n( \\pmb \\theta) = E[ J( \\pmb \\theta;  y)] =  \n\\begin{bmatrix}\n      {\\pmb X}^t \\pmb X /\\tau & \\pmb 0   \\\\\n    \\pmb 0^t  &   \\frac{n}{2\\tau^2}\n  \\end{bmatrix}\n\\] 그리고 \\[  I_n^{-1}( \\pmb \\theta) =\n\\begin{bmatrix}\n     \\tau( {\\pmb X}^t  \\pmb X)^{-1}  & \\pmb 0   \\\\\n    \\pmb 0^t  &   \\frac{2\\tau^2}{n}\n  \\end{bmatrix}\n  =\n\\begin{bmatrix}\n     \\sigma^2( {\\pmb X}^t  \\pmb X)^{-1}  & \\pmb 0   \\\\\n    \\pmb 0^t  &   \\frac{2\\sigma^4}{n}\n  \\end{bmatrix}\n\\]\n따라서 회귀계수 추정량 \\(\\hat { \\beta}\\)의 분포는 평균이 \\(\\pmb \\beta\\) 이고 공분산이 \\(\\sigma^2( {\\pmb X}^t - \\pmb X)^{-1}\\) 인 정규분포를 따른다.\n여기거 주목할 점은 가능도함수에 최대가능도추정량을 대입하면 그 값이 \\(SSE(\\hat { \\beta})\\)의 함수로 나타난다.\n\\[\n\\begin{aligned}\nL_n(\\hat { \\pmb \\theta} ) & = L_n(\\hat { \\pmb \\beta} ,\\hat \\sigma^2 ) \\notag \\\\\n&=  (2\\pi\\hat \\sigma^2)^{-\\frac{n}{2}} \\exp \\left [-\\frac{1}{2 \\hat \\sigma^2}( \\pmb y- \\pmb X \\hat { \\pmb \\beta})^t( \\pmb y- \\pmb X \\hat { \\pmb \\beta} ) \\right ] \\notag  \\\\\n& = (2\\pi\\hat \\sigma^2)^{-\\frac{n}{2}} \\exp \\left [-\\frac{n}{2} \\right ]  \\notag  \\\\\n& = \\left (2\\pi \\frac{SSE(\\hat { \\pmb \\beta})}{n} \\right )^{-\\frac{n}{2}} \\exp \\left [-\\frac{n}{2} \\right ]\n\\end{aligned}\n\\tag{1.18}\\] 또한 가능도함수의 값은 다음과 같다.\n\\[\nl_n(\\hat { \\pmb \\theta} ) = l_n(\\hat { \\pmb \\beta} ,\\hat \\sigma^2 )\n= \\text{constant}  - \\frac{n}{2} \\log \\frac{SSE(\\hat { \\pmb \\beta})}{n}\n\\tag{1.19}\\]\n따라서 잔차제곱함 \\(SSE(\\hat { \\pmb \\beta})\\) 작아지면 가능도함수는 커진다.\n앞 절에서 언급한 평균 모형 식 1.7 에서 최대가능도 추정을 알아보자. 관측값 벡터는 다음과 같은 분포를 따른다.\n\\[\n\\pmb y \\sim N( \\beta_0 \\pmb 1 , \\sigma^2 \\pmb I_n)\n\\tag{1.20}\\]\n선형모형 식 1.17 에 대한 로그 가능도 함수는 다음과 같이 주어진다. 분산에 대한 모수를 \\(\\tau=\\sigma^2\\) 로 바꾸어 사용하면 모수 벡터는 \\(\\pmb \\theta = (\\beta_0, \\tau)^t\\)이다.\n\\[\n\\ell_n( \\pmb \\theta; \\pmb y) = -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac { ( \\pmb y- \\beta_0 \\pmb 1  )^t ( \\pmb y-  \\beta_0 \\pmb 1) }{2\\tau}  \n\\]\n두 개의 모수 \\(\\beta_0\\)와 \\(\\tau\\)에 대하여 미분하여 가능도 방정식을 구하면 다음과 같다.\n\\[\n\\begin{aligned}\ns( \\pmb \\theta;  \\pmb y) & =  \\pardifftwo{}{ \\pmb \\theta}\\ell_n( \\pmb \\theta;  \\pmb y ) \\\\\n  & =  \\begin{bmatrix}\n    \\pardifftwo{}{ \\beta_0}\\ell_n( \\pmb \\theta;  \\pmb y ) \\\\\n    \\pardifftwo{}{\\tau}\\ell_n( \\pmb \\theta;  \\pmb y )\n  \\end{bmatrix} \\\\\n  & =\n  \\begin{bmatrix}\n     \\pmb 1^t ( \\pmb y-  \\beta_0 \\pmb 1)/\\tau \\\\\n    -\\frac{n}{2\\tau} +\\frac { ( \\pmb y- \\beta_0 \\pmb 1))^t ( \\pmb y- \\beta_0 \\pmb 1) }{2\\tau^2}\n  \\end{bmatrix} \\\\\n  & = \\pmb 0\n\\end{aligned}\n\\]\n위의 방정식을 풀면 다음과 같은 최대가능도 추정량을 구할 수 있다.\n\\[\n\\hat \\beta_0 = \\bar y, \\quad {\\hat \\sigma}^2 = \\frac{\\sum_{i=1}^n (y_i - \\bar y)^2}{n} = \\frac{SST}{n}\n\\]\n그리고 가능도함수에 최대가능도추정량을 대입하면 그 값이 다음과 같다.\n\\[\nL_n(\\hat { \\pmb \\theta} )  = L_n(\\hat { \\beta_0} ,\\hat \\sigma^2 )\n= \\left (2\\pi \\frac{SST}{n} \\right )^{-\\frac{n}{2}} \\exp \\left [-\\frac{n}{2} \\right ]\n\\tag{1.21}\\]\n두 개의 모형, 즉 선형회귀모형 식 1.17 과 평균모형 식 1.7 의 가능도 함수의 비, 즉 식 1.18 과 식 1.21 의 비율을 구해보면 결정계수 \\(R^2\\)외의 관계를 볼 수 있다.\n\\[  \n\\frac{ L_n(\\hat { \\beta_0} ,\\hat \\sigma^2 )  }{L_n(\\hat { \\pmb \\beta} ,\\hat \\sigma^2 ) }\n= \\left (2\\pi \\frac{SST}{n} \\right )^{-\\frac{n}{2}} / \\left (2\\pi \\frac{SSE}{n} \\right )^{-\\frac{n}{2}}\n\\propto  \\left [ \\frac{SSE}{SST} \\right ]^{\\frac{n}{2}} = \\left [ 1-R^2 \\right ]^{\\frac{n}{2}}  \n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>선형 회귀모형의 소개</span>"
    ]
  },
  {
    "objectID": "qmd/inference.html",
    "href": "qmd/inference.html",
    "title": "2  선형회귀에서의 추론",
    "section": "",
    "text": "2.1 제곱합의 분포\n앞 장의 중회귀 모형 식 1.10 에서 관측값 벡터 \\(\\pmb y\\)가 다변량 정규분포 \\(N(\\pmb X \\pmb \\beta, \\sigma^2 \\pmb I)\\)를 따를 때 회귀계수의 추정량 \\(\\hat {\\pmb \\beta}=(\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y\\) 은 다음과 같은 분포를 따르는 것을 보였다.\n\\[ \\hat {\\pmb \\beta} \\sim N(\\pmb \\beta, \\sigma^2 (\\pmb X^t  \\pmb X)^{-1}) \\]\n반응변수의 추정값을 구하는 식에서 다음과 같은 모자행렬(hat matrix) \\(\\pmb H = \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t\\) 을 정의하자. 여기서 중요한 점은 모자행렬은 대칭인 멱등행렬 (\\(\\pmb H \\pmb H =\\pmb H\\))이며 이는 모자행렬이 사영행렬임을 의미한다.\n\\[\n\\hat {\\pmb y} = \\pmb X \\hat {\\pmb \\beta} = \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y = \\pmb H \\pmb y\n\\tag{2.1}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>선형회귀에서의 추론</span>"
    ]
  },
  {
    "objectID": "qmd/inference.html#제곱합의-분포",
    "href": "qmd/inference.html#제곱합의-분포",
    "title": "2  선형회귀에서의 추론",
    "section": "",
    "text": "2.1.1 잔차제곱합의 분포\n이제 제곱합들의 분포를 알아보기로 하자. 먼저 잔차제곱합 \\(SSE\\)를 이차 형식으로 표시해보자.\n\\[\n\\begin{aligned}\nSSE & = \\sum_{i=1}^n (y_i - \\hat y_i) \\\\\n   & = (\\pmb y - \\pmb X \\hat {\\pmb \\beta})^t (\\pmb y - \\pmb X \\hat {\\pmb \\beta}) \\\\\n   & = (\\pmb y - \\pmb H \\pmb y)^t (\\pmb y - \\pmb H \\pmb y) \\\\\n   & = \\pmb y^t (\\pmb I - \\pmb H)^t (\\pmb I - \\pmb H) \\pmb y \\\\\n   & = \\pmb y^t (\\pmb I - \\pmb H) (\\pmb I - \\pmb H) \\pmb y \\\\\n   & = \\pmb y^t (\\pmb I - \\pmb H) \\pmb y \\\\\n\\end{aligned}\n\\]\n위의 식에서 \\(\\pmb I - \\pmb H\\)는 멱등행렬이고 다음이 성립한다.\n\\[\n(\\pmb I - \\pmb H) \\pmb X = \\pmb X - \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb X = \\pmb 0\n\\]\n따라서\n\\[\n\\pmb \\mu^t (\\pmb I - \\pmb H)  \\pmb \\mu = \\pmb \\beta^t \\pmb X^t (\\pmb I - \\pmb H) \\pmb X \\beta =0\n\\]\n이므로 비중심 모수는 0이다.\n또한\n\\[\n\\begin{aligned}\nr(\\pmb I - \\pmb H) & = tr(\\pmb I - \\pmb H) \\\\\n& = tr(\\pmb I_n) - tr \\left [ \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\right ] \\\\\n& = n-tr \\left [ (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb X \\right ]\n\\\\\n&= n-tr (\\pmb I_p ) \\\\\n& = n-p\n\\end{aligned}\n\\]\n이므로 부록의 정리에 의하여 \\(SSE\\)는 다음과 같이 중심 카이제곱 분포를 따른다.\n\\[\n\\frac{SSE}{\\sigma^2} \\sim \\chi^2(n-p)\n\\tag{2.2}\\]\n\n\n2.1.2 회귀제곱합의 분포\n다음으로 회귀제곱합 \\(SSR\\)의 분포를 유도해보자.\n\\[\n\\begin{aligned}\nSSR & = \\sum_{i=1}^n (\\hat y_i - \\bar y) \\\\\n   & = (\\pmb X \\hat {\\pmb \\beta} - \\bar y \\pmb 1 )^t (\\pmb X \\hat {\\pmb \\beta} - \\bar y \\pmb 1 ) \\\\\n   & = \\left ( \\pmb X \\hat {\\pmb \\beta} -  \\pmb 1 (\\pmb 1^t \\pmb y)/n \\right )^t   \\left ( \\pmb X \\hat {\\pmb \\beta} -  \\pmb 1 (\\pmb 1^t \\pmb y)/n \\right )\\\\\n   & = \\left ( \\pmb H \\pmb y-  \\tfrac{1}{n} \\pmb 1 \\pmb 1^t \\pmb y \\right )^t   \\left ( \\pmb H \\pmb y-  \\tfrac{1}{n} \\pmb 1 \\pmb 1^t \\pmb y \\right ) \\\\\n    & =  \\pmb y^t \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right )^t   \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right ) \\pmb y \\\\\n    & = \\pmb y^t \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right )   \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right ) \\pmb y \\\\\n     & = \\pmb y^t  \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right ) \\pmb y \\\\\n\\end{aligned}\n\\]\n위의 유도식에서 다음 두 가지 성질을 이용하였다. 첫 번째 성질은 모자행렬이 사영행렬이며 모자행렬이 투영하는 공간은 일벡터 \\(\\pmb 1\\)을 포함한 공간이다. 이는 계획 행렬 \\(\\pmb X\\)의 첫 번째 열이 절편에 대한 값으로 모두 1인 것 때문이다. 따라서\n\\[\n\\begin{aligned}\n\\pmb H \\pmb J & =  \\pmb H  \\pmb 1 \\pmb 1^t \\\\\n   & =  [ \\pmb H  \\pmb 1 ]  \\pmb 1^t \\\\\n  & = \\left [  \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t  \\pmb 1 \\right ] \\pmb 1^t \\\\\n  & =  \\pmb 1 \\pmb 1^t \\\\\n  & = \\pmb J  \n\\end{aligned}\n\\]\n두 번째로 다음과 같이 \\(\\pmb J \\pmb J = n \\pmb J\\)이므로 \\(\\tfrac{1}{n} \\pmb J\\)는 멱등행렬이다.\n\\[\n\\begin{aligned}\n   \\pmb J \\pmb J & =   \\pmb 1 \\pmb 1^t   \\pmb 1 \\pmb 1^t \\\\\n     & =  \\pmb 1   [ \\pmb 1^t  \\pmb 1 ]  \\pmb 1^t \\\\\n    & = \\pmb 1   [ n ]  \\pmb 1^t \\\\\n    & =  n \\pmb 1 \\pmb 1^t \\\\\n    & =  n \\pmb J  \n\\end{aligned}\n\\]\n\n\n\n\n\n\n노트\n\n\n\n참고로 평균모형 식 1.7 에서 \\(\\pmb X = \\pmb 1\\)으므로 이 경우 모자행렬이 다음과 같다.\n\\[ H_0 = \\pmb 1 ({\\pmb 1}^t \\pmb 1)^{-1} {\\pmb 1}^t = \\tfrac{1}{n} \\pmb J \\]\n\n\n다음으로 비중심 모수를 유도하자.\n\\[\n\\begin{aligned}\n\\pmb \\mu^t \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right )  \\pmb \\mu\n  & =  \\pmb \\beta^t  \\pmb X^t \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right ) \\pmb X \\pmb \\beta  \\\\\n  & =  \\pmb \\beta^t  \\left ( \\pmb X^t \\pmb H \\pmb X -  \\tfrac{1}{n} \\pmb X^t  \\pmb J \\pmb X \\right )  \\pmb \\beta  \\\\\n& =  \\pmb \\beta^t  \\left ( \\pmb X^t \\pmb X -  \\tfrac{1}{n} \\pmb X^t  \\pmb J \\pmb X \\right )  \\pmb \\beta  \\\\\n& =  \\pmb \\beta^t \\pmb X^t  \\left ( \\pmb I -  \\tfrac{1}{n}  \\pmb J \\right )  \\pmb X \\pmb \\beta  \\\\\n& \\equiv \\delta(\\pmb \\beta)\n\\end{aligned}\n\\]\n또한\n\\[\n\\begin{aligned}\nr\\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right )  \n      & = tr(\\pmb H) - tr \\left [ \\tfrac{1}{n} \\pmb J \\right ]  \\\\\n      & = p -\\tfrac{1}{n} tr (\\pmb 1 \\pmb 1^t) \\\\\n      & = p -\\tfrac{1}{n} tr ( \\pmb 1^t \\pmb 1) \\\\\n      &=  p -\\tfrac{1}{n} n \\\\\n      & = p-1 \\\\\n      & = p-1\n\\end{aligned}\n\\]\n위의 결과를 종합하면 회귀제곱합 \\(SSR\\)은 다음과 같은 분포를 따른다.\n\\[\n\\frac{SSR}{\\sigma^2} \\sim \\chi^2(p-1, \\lambda^2),\n\\tag{2.3}\\]\n위에서 비중심 모수는 다음과 같다.\n\\[\n  \\lambda^2 = \\tfrac{1}{\\sigma^2} \\delta(\\pmb \\beta) =\n\\tfrac{1}{\\sigma^2} \\pmb \\beta^t \\pmb X^t  \\left ( \\pmb I -  \\tfrac{1}{n}  \\pmb J \\right )  \\pmb X \\pmb \\beta\n\\tag{2.4}\\]\n\n\n2.1.3 잔차제곱합과 회귀제곱합의 독립\n잔차제곱합과 회귀제곱합에서 나타난 이차형식의 두 멱등행렬의 곱은 \\(\\pmb 0\\)이다.\n\\[\n\\begin{aligned}\n(\\pmb I - \\pmb H) \\left ( \\pmb H  -  \\tfrac{1}{n} \\pmb J \\right )  \n& = \\pmb H -  \\tfrac{1}{n} \\pmb J  - \\pmb H \\pmb H  + \\tfrac{1}{n} \\pmb H \\pmb J \\\\\n  & = \\pmb H -  \\tfrac{1}{n} \\pmb J  -  \\pmb H  + \\tfrac{1}{n}  \\pmb J \\\\\n  & = \\pmb 0\n\\end{aligned}\n\\]\n따라서 부록의 정리에 의하여 잔차제곱합(\\(SSE\\))과 회귀제곱합(\\(SSR\\))은 서로 독립이다.\n\n\n2.1.4 총제곱합의 분포\n총제곱합 \\(SST\\)의 분포는 위의 결과들을 이용하면 쉽게 구할 수 있다.\n\\[\nSST = \\sum_{i=1}^n (y_i - \\bar y)^2 = \\pmb y^t  \\left ( \\pmb I -  \\tfrac{1}{n}  \\pmb J \\right ) \\pmb y\n\\]\n위의 결과를 종합하면 회귀제곱합 \\(SST\\)은 다음과 같은 분포를 따른다.\n\\[\n\\frac{SST}{\\sigma^2} \\sim \\chi^2(n-1, \\lambda^2),\n\\tag{2.5}\\]\n위에서 비중심 모수 \\(\\lambda^2\\)은 식 식 2.4 과 같다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>선형회귀에서의 추론</span>"
    ]
  },
  {
    "objectID": "qmd/inference.html#모분산의-추정",
    "href": "qmd/inference.html#모분산의-추정",
    "title": "2  선형회귀에서의 추론",
    "section": "2.2 모분산의 추정",
    "text": "2.2 모분산의 추정\n최소제곱법을 통해서 회귀분석을 실시하였을때 우리는 적합된 회귀선이 얼마나 실제 관측값들을 잘 설명하고 있는지를 파악하는 것이 모형의 유용성을 판단하는데 중요한 작업이다. 즉, 적합된 회귀선이 관측값을 예측할 때의 변동성을 측정하는 것이 중요하다. 그 변동의 정도를 나타내는 것이 모분산 \\(\\sigma^2\\)의 추정이다.\n식 식 2.2 에 나타난 잔차제곱합의 분포를 이용하면 다음과 같은 결과를 얻는다.\n\\[ E \\left [ \\frac{SSE}{\\sigma^2} \\right ] = n-p \\]\n위의 방정식에 적률법(Method of Moments)를 적용하면 모분산 \\(\\sigma^2\\)에 대한 불편추정량을 얻을 수 있다. 평균 잔차 제곱합(mean residual sum of square; \\(S^2\\) 또는 MSE)를 다음과 같이 정의하자.\n\\[\nMSE = \\frac{SSE}{n-p} = \\frac{\\sum r^2_i}{n-p}  = \\frac{\\sum(y_i-\\hat y_i)^2}{n-p} \\equiv s^2\n\\tag{2.6}\\]\n\\(S^2=MSE\\)은 모분산의 불편 추정량이다.\n\\[ E(s^2) = E(MSE) =\\sigma^2 \\]\n모분산의 추정량이 작을수록 관측값 \\(y\\)의 변동 중 회귀식이 설명할 수 변동이 크다는 것을 나타낸다. 관측값들이 회귀식으로부터 멀리 떨어져 있으면 \\(MSE\\) 는 커진다.\n회귀계수들의 공분산을 추정하는 경우에도 \\(s^2\\)이 사용된다.\n\\[ \\hat V ( \\hat {\\pmb \\beta }) = \\hat \\sigma^2 (\\pmb X^t \\pmb X)^{-1} = s^2(\\pmb X^t \\pmb X)^{-1} \\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>선형회귀에서의 추론</span>"
    ]
  },
  {
    "objectID": "qmd/inference.html#최소제곱-추정량의-성질",
    "href": "qmd/inference.html#최소제곱-추정량의-성질",
    "title": "2  선형회귀에서의 추론",
    "section": "2.3 최소제곱 추정량의 성질",
    "text": "2.3 최소제곱 추정량의 성질\n최소제곱 추정량의 분포에 대한 성질은 다음과 같다.\n\n\\(\\hat {\\pmb \\beta} \\sim N(\\pmb \\beta, \\sigma^2 (\\pmb X^t \\pmb X)^{-1} )\\)\n\\(\\hat {\\pmb \\beta}\\)와 \\(SSE\\)는 독립이다.\n잔차제곱합(\\(SSE\\))과 회귀제곱합(\\(SSR\\))은 서로 독립이다.\n\\(SSE/\\sigma^2\\)는 자유도가 \\(n-p\\)인 카이제곱 분포를 따른다.\n\\((\\hat {\\pmb \\beta} -\\pmb \\beta)^t (\\pmb X^t \\pmb X) (\\hat {\\pmb \\beta} -\\pmb \\beta) /\\sigma^2\\) 는 자유도가 \\(p\\)인 카이제곱분포를 따른다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>선형회귀에서의 추론</span>"
    ]
  },
  {
    "objectID": "qmd/inference.html#모형의-적합도-검정과-분산분석",
    "href": "qmd/inference.html#모형의-적합도-검정과-분산분석",
    "title": "2  선형회귀에서의 추론",
    "section": "2.4 모형의 적합도 검정과 분산분석",
    "text": "2.4 모형의 적합도 검정과 분산분석\n회귀식을 적합하고 가장 먼저 고려해야할 사항은 적합된 회귀식이 유의한 의미를 가지는지 알아보는 것이다. 회귀식이 가지고 있는 의미는 설명변수의 변화에 따라서 반응변수가 변한다는 것이다. 따라서 회귀 모형이 유의하다는 것은 최소한 하나 이상의 설명변수가 반응변수의 변화를 예측하는데 의미가 있다는 것을 뜻한다. 모든 회귀계수의 값이 0이면 반응변수를 예측하는데 모든 설명변수가 필요가 없다는 것을 의미한다. 이러한 무의미한 모형은 앞장에서 나온 평균모형 식 1.7 이다.\n이제 제시된 회귀식이 유의한 지에 대한 검정은 다음과 같은 두 가설 중 하나를 선택하는 것이다.\n\\[\nH_0: \\text{mean model} \\quad vs. \\quad H_1: \\text{ not } H_0\n\\]\n위의 가설을 바꾸어 쓰면 선형 회귀모형의 유의성 또는 적합도을 검정하는 가설이 된다.\n\\[\nH_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_{p-1} =0 \\quad vs. \\quad H_1: \\text{ At least one of } \\beta_i \\text{ is not equal to } 0\n\\tag{2.7}\\]\n위의 가설 식 2.7 를 검정하는 방법이 분산분석표를 이용한 F-검정이다.\n가설 식 2.7 에서 귀무가설 \\(H_0\\)가 참인 경우는\n\\[\n\\pmb X \\pmb \\beta = [ \\pmb 1 ~ \\pmb x_1 ~ \\dots ~ \\pmb x_{p-1} ]\n\\begin{bmatrix}\n\\beta_0 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix}\n=\\beta_0 \\pmb 1\n\\]\n이 성립하여 식 식 2.4 에 나타난 비중심 모수가 0이 된다.\n\\[  \\lambda^2 = \\tfrac{1}{\\sigma^2} \\pmb \\beta^t \\pmb X^t  \\left ( \\pmb I -  \\tfrac{1}{n}  \\pmb J \\right )  \\pmb X \\pmb \\beta = \\tfrac{\\beta_0^2}{\\sigma^2}  \\pmb 1^t \\left ( \\pmb I -  \\tfrac{1}{n}  \\pmb J \\right )  \\pmb 1 =  \\pmb 0\n\\]\n따라서 귀무가설에서는 회귀제곱합이 자유도가 \\(p-1\\)인 중신ㅁ 카이제곱 분포를 따르게 되고 잔차제곱합과 독립이므로 다음의 통계량 \\(F_0\\)가 자유도가 \\(p-1\\)가ㅗ \\(n-p\\)를 가지는 F-분포를 따른다.\n\\[\nF_0 = \\frac{ SSR/(p-1)}{SSE/(n-p)} = \\frac{MSR}{MSE} \\sim F(p-1, n-p) \\quad \\text{ under } H_0\n\\tag{2.8}\\]\n따라서 위의 검정 통계량의 p-값이 유의수준보다 크면 적합성 검정에 대한 가설 식 2.7 의 귀무가설을 기각한다. 귀무가설의 기각은 회귀모형의 계수 중 적어도 하나는 0이 아니므로 회귀 모형이 유의하다는 의미이다.\n위에서 안급한 F-검정을 위한 통계량들은 다음과 같은 분산분석(Analysis of Variance; ANOVA) 표를 사용하면 쉽게 계산할 수 있다.\n\n적합도 검정을 위한 분산분석표\n\n\n\n\n\n\n\n\n\n\n요인\n제곱합\n자유도\n평균제곱합\nF-통계량\np-값\n\n\n\n\n회귀\n\\(SSR\\)\n\\(p-1\\)\n\\(MSR\\)\n\\(F_0 =\\frac{MSR}{MSE}\\)\n\\(P(F&gt;F_0)\\)\n\n\n오차\n\\(SSE\\)\n\\(n-p\\)\n\\(MSE\\)\n\n\n\n\n전체\n\\(SST\\)\n\\(n-1\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>선형회귀에서의 추론</span>"
    ]
  },
  {
    "objectID": "qmd/inference2.html",
    "href": "qmd/inference2.html",
    "title": "3  모형의 비교",
    "section": "",
    "text": "3.1 직교하는 설명 변수\n다음과 같은 2개의 설명변수(\\(x_1\\), \\(x_2\\)) 와 반응변수 \\(y\\) 를 가진 자료(데이터프레임)이 있다고 하자.\nx1 &lt;- c(1,  1, 1,  1, -1, -1, -1, -1)\nx2 &lt;- c(1, -1, 1, -1, 1, -1, 1, -1)\ny &lt;- c( 2, 5, 3, 4, 6, 9, 5, 10)\ndf &lt;- data.frame(x1, x2, y)\ndf\n\n  x1 x2  y\n1  1  1  2\n2  1 -1  5\n3  1  1  3\n4  1 -1  4\n5 -1  1  6\n6 -1 -1  9\n7 -1  1  5\n8 -1 -1 10\n이제 위의 자료로 선형회귀모형을 적합해 보자.\n\\[\ny_i= \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} +e_i\n\\tag{3.1}\\]\nfm1 &lt;- lm(y ~ x1 + x2, data=df)\nsummary(fm1)$coefficients\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)      5.5  0.3162278 17.392527 0.0000115141\nx1              -2.0  0.3162278 -6.324555 0.0014565818\nx2              -1.5  0.3162278 -4.743416 0.0051344617\n이제 모형 식 3.1 에서 각각 \\(x_1\\)과 \\(x_2\\)를 제거한 축소된 모형을 적합해보자\n\\[\ny_i= \\beta_0 + \\beta_1 x_{i1} +e_i  , \\quad \\quad y_i= \\beta_0 +  \\beta_2 x_{i2} +e_i\n\\tag{3.2}\\]\nfm21 &lt;- lm(y ~ x1 , data=df)\nsummary(fm21)$coefficients\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)      5.5  0.6770032  8.124038 0.0001867963\nx1              -2.0  0.6770032 -2.954196 0.0254739283\nfm22 &lt;- lm(y ~ x2 , data=df)\nsummary(fm22)$coefficients\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)      5.5  0.8660254  6.350853 0.0007143845\nx2              -1.5  0.8660254 -1.732051 0.1339745962\n두 개의 독립변수가 있는 모형 식 3.1 에서 하나의 독립변수를 제거해도 남아 있는 독립 변수의 회귀계수 추정량은 모형 식 3.1 과 같은 것을 알 수 있다. 이렇게 여러 개의 독립변수가 있는 모형에서 하나의 변수를 제거해도 다른 복립변수의 추정에 영향을 미치지 않는 경우는 어떤 경우일까?\n이제 모형 식 3.1 의 설계행렬(\\(\\pmb  X\\), design matrix)를 구해서 \\(\\pmb  X^t \\pmb  X\\)를 구해보자.\nX &lt;- model.matrix(fm1)\nX\n\n  (Intercept) x1 x2\n1           1  1  1\n2           1  1 -1\n3           1  1  1\n4           1  1 -1\n5           1 -1  1\n6           1 -1 -1\n7           1 -1  1\n8           1 -1 -1\nattr(,\"assign\")\n[1] 0 1 2\n\nt(X) %*% X\n\n            (Intercept) x1 x2\n(Intercept)           8  0  0\nx1                    0  8  0\nx2                    0  0  8\n모형 식 3.1 의 설계행렬 \\(\\pmb  X\\)의 각 열들은 서로 직교하는것을 알 수 있다.\n만약 여러 개의 독립변수를 가진 선형모형에서 모든 설명 변수들의 열들이 모두 서로 직교한다면(절편에 대한 열도 포함해서) 회귀계수의 추정값은 독립변수가 줄어든 축소돤 모형에서도 원래의 모형과 같은 것을 알 수 있다.\n선형 모형에서 설계행렬 \\(\\pmb  X\\)의 각 열벡터를 각각 \\(\\pmb  x_1,\\dots, \\pmb  x_{p}\\)라고 하자. 만약 모든 열들이 서로 직교한다면 (즉 \\({\\pmb  x}_i^t \\pmb  x_j =0\\) for \\(i \\ne j\\)) 선형회귀 모형에서 회귀계수의 추정치는 설명 변수의 유무에 관계없이 일정하게 나타난다.\n이러한 상황을 모형식으로 다시 써보자. 만약 다음이 성립하면 \\[\n\\pmb  X^t \\pmb  X =  \n\\begin{bmatrix}\n{\\pmb  x}_1^t \\\\\n{\\pmb  x}_2^t \\\\\n\\vdots \\\\\n{\\pmb  x}_{p}^t \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\pmb  x_{1} &  \\pmb  x_{2} & \\dots & {\\pmb  x}_{p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n{\\pmb  x}_1^t {\\pmb  x}_1 & 0 & 0 & \\cdots & 0 \\\\\n0 & {\\pmb  x}_2^t {\\pmb  x}_2  & 0 & \\cdots & 0 \\\\\n0  & 0 & {\\pmb  x}_3^t {\\pmb  x}_3  & \\cdots & 0 \\\\\n\\vdots  &  \\vdots  & \\vdots  & \\vdots    & 0 \\\\\n0 & 0 & \\cdots & 0 & {\\pmb  x}_{p}^t {\\pmb  x}_{p}\n\\end{bmatrix}\n\\] 회귀계수의 추정량은 다음과 같이 나타난다.\n\\[\n\\left ( {\\pmb  X}^t \\pmb  X \\right )^{-1} {\\pmb  X}^t \\pmb  y =  \n\\begin{bmatrix}\n\\tfrac{1} {{\\pmb  x}_1^t {\\pmb  x}_1} & 0 & 0 & \\cdots & 0 \\\\\n0 & \\tfrac{1}{{\\pmb  x}_2^t {\\pmb  x}_2}  & 0 & \\cdots & 0 \\\\\n0  & 0 & \\tfrac{1}{{\\pmb  x}_3^t {\\pmb  x}_3}  & \\cdots & 0 \\\\\n\\vdots  &  \\vdots  & \\vdots  & \\vdots    & 0 \\\\\n0 & 0 & \\cdots & 0 & \\tfrac{1}{{\\pmb  x}_{p}^t {\\pmb  x}_{p} }\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\pmb  x}_1^t \\pmb  y \\\\\n{\\pmb  x}_2^t \\pmb  y \\\\\n{\\pmb  x}_3^t \\pmb  y \\\\\n\\vdots \\\\\n{\\pmb  x}_{p}^t \\pmb  y\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\tfrac{{\\pmb  x}_1^t \\pmb  y}{{\\pmb  x}_1^t {\\pmb  x}_1 } \\\\\n\\tfrac{{\\pmb  x}_2^t \\pmb  y}{{\\pmb  x}_2^t {\\pmb  x}_2 } \\\\\n\\vdots \\\\\n\\tfrac{{\\pmb  x}_{p}^t \\pmb  y}{{\\pmb  x}_{p}^t {\\pmb  x}_{p} }\n\\end{bmatrix}\n\\]\n위의 결과를 조금 더 일반화해보자. 만약 계획행렬 \\(\\pmb  X\\)를 다음과 같은 \\(p\\)개의 부분 계획행렬 \\(\\pmb  X_1, \\pmb  X_2, \\dots, \\pmb  X_{p}\\)로 나누고\n\\[ \\pmb  y = \\pmb  X \\pmb  \\beta + \\pmb  e = \\sum_{k=1}^{p} \\pmb  X_k \\pmb  \\beta_k + \\pmb  e \\]\n부분 계획행렬들이 다음과 같은 성질을 가지고 있다고 하자.\n\\[  \n\\pmb  X = [\\pmb  X_1~ \\pmb  X_2~ \\dots~ \\pmb  X_{p} ] \\quad \\text{ and } \\quad {\\pmb  X}_i^t {\\pmb  X}_j =\\pmb  0, i \\ne j\n\\tag{3.3}\\]\n이러한 조건 하에서는 회귀계수의 추정량이 다음과 같이 나타난다.\n\\[\n\\begin{aligned}\n\\hat { \\pmb  \\beta} &  = \\left ( {\\pmb  X}^t \\pmb  X \\right )^{-1} {\\pmb  X}^t \\pmb  y \\\\\n& =\n\\begin{bmatrix}\n( {\\pmb  X}_1^t {\\pmb  X}_1 )^{-1} & 0 & 0 & \\cdots & 0 \\\\\n0 & ( {\\pmb  X}_2^t {\\pmb  X}_2 )^{-1}  & 0 & \\cdots & 0 \\\\\n0  & 0 & ( {\\pmb  X}_3^t {\\pmb  X}_3 )^{-1}  & \\cdots & 0 \\\\\n\\vdots  &  \\vdots  & \\vdots  & \\vdots    & 0 \\\\\n0 & 0 & \\cdots & 0 & ( {\\pmb  X}_{p}^t {\\pmb  X}_{p} )^{-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\pmb  X}_1^t \\pmb  y \\\\\n{\\pmb  X}_2^t \\pmb  y \\\\\n{\\pmb  X}_3^t \\pmb  y \\\\\n\\vdots \\\\\n{\\pmb  X}_{p}^t \\pmb  y\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n( {\\pmb  X}_1^t {\\pmb  X}_1 )^{-1} {\\pmb  X}_1^t \\pmb  y \\\\\n( {\\pmb  X}_2^t {\\pmb  X}_2 )^{-1} {\\pmb  X}_2^t \\pmb  y \\\\\n\\vdots \\\\\n( {\\pmb  X}_{p}^t {\\pmb  X}_{p} )^{-1} {\\pmb  X}_{p}^t \\pmb  y\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n\\hat {\\pmb  \\beta}_1 \\\\\n\\hat {\\pmb  \\beta}_2 \\\\\n\\hat {\\pmb  \\beta}_3 \\\\\n\\vdots \\\\\n\\hat {\\pmb  \\beta}_{p}\n\\end{bmatrix}\n\\end{aligned}\n\\]\n위의 결과는 전체 모형에서의 추정량 \\(\\hat {\\pmb  \\beta}\\)의 \\(j\\) 번째 부분 \\(\\hat {\\pmb  \\beta}_j\\)이 \\(({\\pmb  X}_j^t {\\pmb  X}_j )^{-1} {\\pmb  X}_j^t \\pmb  y\\)로 구성되며 다른 \\(\\pmb  X_i\\)와는 관계가 없다.\n이러한 결과를 이용하면 설명변수들이 서로 직교하는 조건 식 3.3 를 만족하면 축소모형\n\\[ \\pmb  y = \\pmb  X_j \\pmb  \\beta_j + \\pmb  e \\]\n에서 계수 추정치 \\(\\hat {\\pmb  \\beta}_j =({\\pmb  X}_j^t {\\pmb  X}_j )^{-1} {\\pmb  X}_j^t \\pmb  y\\) 는 모든 설명 변수를 고려한 완전 모형에서의 추정치와 같은 것을 알 수 있다. 설명변수들이 서로 직교하는 조건 식 3.3 을 만족하면 하나의 축소모형에 대한 추정량는 직교하는 다른 설명변수들의 영향을 받지 않는다.\n더 나아가서 직교하는 설명변수들이 회귀제곱합에 미치는 영향은 각 축소모형들의 기여도를 단순하게 더한 결과와 같다.\n\\[\n\\begin{aligned}\nSSR & = \\pmb  y^t  \\left ( \\pmb  H  -  \\tfrac{1}{n} \\pmb  J \\right ) \\pmb  y \\\\\n  & = \\pmb  y^t  \\pmb  H \\pmb  y - \\tfrac{1}{n} \\pmb  y^t \\pmb  J  \\pmb  y \\\\\n  & = \\pmb  y^t \\pmb  X (\\pmb  X^t \\pmb  X)^{-1} \\pmb  X^t \\pmb  y  - n (\\bar y)^2 \\\\\n  & = \\pmb  y^t \\pmb  X  (\\pmb  X^t \\pmb  X)^{-1} (\\pmb  X^t \\pmb  X) (\\pmb  X^t \\pmb  X)^{-1} \\pmb  X^t \\pmb  y  - n (\\bar y)^2 \\\\\n  & = {\\hat {\\pmb  \\beta}}^t (\\pmb  X^t \\pmb  X) \\hat {\\pmb  \\beta} - n (\\bar y)^2 \\\\\n  & = \\sum_{k=1}^{p} {\\hat {\\pmb  \\beta}}_k^t ({\\pmb  X}_k^t {\\pmb  X}_k) \\hat {\\pmb  \\beta}_k - n (\\bar y)^2\n\\end{aligned}\n\\]\n또한 회귀계수 추정량의 분산을 보면 부분 회귀계수 추정량 \\(\\hat {\\pmb  \\beta}_j\\) 들은 서로 독립이며 축소 모형에서의 분산과 동일함을 알 수 있다.\n\\[\n\\begin{aligned}\nCov(\\hat {\\pmb  \\beta} ) & = \\sigma^2 (\\pmb  X^t \\pmb  X)^{-1} \\\\\n  & =\n  \\begin{bmatrix}\n\\sigma^2 ( {\\pmb  X}_1^t {\\pmb  X}_1 )^{-1} & 0 & 0 & \\cdots & 0 \\\\\n0 & \\sigma^2 ( {\\pmb  X}_2^t {\\pmb  X}_2 )^{-1}  & 0 & \\cdots & 0 \\\\\n0  & 0 & \\sigma^2 ( {\\pmb  X}_3^t {\\pmb  X}_3 )^{-1}  & \\cdots & 0 \\\\\n\\vdots  &  \\vdots  & \\vdots  & \\vdots    & 0 \\\\\n0 & 0 & \\cdots & 0 & \\sigma^2 ( {\\pmb  X}_{p}^t {\\pmb  X}_{p} )^{-1}\n\\end{bmatrix} \\\\\n& =  \n  \\begin{bmatrix}\nCov( \\hat {\\pmb  \\beta}_1 ) & 0 & 0 & \\cdots & 0 \\\\\n0 &  Cov( \\hat {\\pmb  \\beta}_2 )  & 0 & \\cdots & 0 \\\\\n0  & 0 &  Cov( \\hat  {\\pmb  \\beta}_3 )   & \\cdots & 0 \\\\\n\\vdots  &  \\vdots  & \\vdots  & \\vdots    & 0 \\\\\n0 & 0 & \\cdots & 0 &  Cov( \\hat  {\\pmb  \\beta}_p )\n\\end{bmatrix}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>모형의 비교</span>"
    ]
  },
  {
    "objectID": "qmd/inference2.html#설명변수의-추가",
    "href": "qmd/inference2.html#설명변수의-추가",
    "title": "3  모형의 비교",
    "section": "3.2 설명변수의 추가",
    "text": "3.2 설명변수의 추가\n먼저 계획행렬 \\(\\pmb  X_1\\)을 고려한 선형모형을 고려하자.\n\\[\n\\pmb  y  = \\pmb  X_1 {\\pmb  \\beta}_{1*}  + \\pmb e\n\\tag{3.4}\\]\n이 경우 회귀계수의 최소제곱추정량은 \\(\\hat {\\pmb  \\beta}_{1*} = ({\\pmb  X}_1^t {\\pmb  X}_1)^{-1} {\\pmb  X}_1^t \\pmb  y\\) 이다.\n이제 위의 모형 식 3.4 에 설명변수를 추가한 모형을 생각해 보자. 추가된 설명변수로 이루어진 계획행렬을 \\(\\pmb X_2\\)라고 하면 다음과 같이 쓸수 있다.\n\\[\n\\begin{aligned}\n\\pmb  y & = \\pmb  X_1 {\\pmb  \\beta}_1 + \\pmb  X_2 {\\pmb  \\beta}_2  + \\pmb  e  \\\\\n    & = [ \\pmb  X_1 ~ \\pmb  X_2]\n    \\begin{bmatrix}\n    {\\pmb  \\beta}_1 \\\\\n    {\\pmb  \\beta}_2\n    \\end{bmatrix} + \\pmb  e \\\\\n    & =  {\\pmb   X } {\\pmb  \\beta} + \\pmb  e\n\\end{aligned}\n\\tag{3.5}\\]\n위의 식에서\n\\[\n\\pmb X = [ \\pmb X_1 ~ \\pmb X_2] \\quad \\text{ and } \\quad \\pmb  \\beta =\n\\begin{bmatrix}\n{\\pmb  \\beta}_1 \\\\\n{\\pmb  \\beta}_2\n\\end{bmatrix}\n\\]\n설명변수를 추가한 확대 모형 식 3.5 에서 회귀계수의 최소제곱추정량은 \\(\\hat {\\pmb  \\beta} = (\\pmb  X^t \\pmb  X)^{-1} \\pmb  X^t \\pmb  y\\)이다.\n여기서 주의할 점은 식 3.4 의 회귀 계수 \\({\\pmb \\beta}_{1*}\\) 의 추정량과 식 3.5 의 회귀 계수 \\({\\pmb \\beta}_{1}\\) 의 추정량은 일반적으로 같지 않다.\n확대모형 식 3.5 의 회귀계수 추정량을 구하려면 최소제곱법을 다시 확장모형에 적용해야 하지만 원래의 모형 식 3.4 에서 구해진 추정량 \\(\\hat {\\pmb  \\beta}_{1*} = ({\\pmb  X}_1^t {\\pmb  X}_1)^{-1} {\\pmb  X}_1^t \\pmb  y\\) 을 이용하여 유도할 수 있다.\n이제 원래의 모형 식 3.4 에서 모자행렬을\n\\[ \\pmb  H_1 = {\\pmb  X}_1 ({\\pmb  X}_1^t {\\pmb  X}_1)^{-1} {\\pmb  X}_1^t \\]\n라고 하고 확대 모형 식 3.5 에 대하여 다음과 같이 모형을 다시 표현해보자.\n\\[\n\\begin{aligned}\n\\pmb  y & = \\pmb  X_1 {\\pmb  \\beta}_1 + \\pmb  X_2 {\\pmb  \\beta}_2  + \\pmb  e  \\\\\n  & = \\pmb  X_1 {\\pmb  \\beta}_1 + ( \\pmb  H_1  + \\pmb  I -\\pmb  H_1  ) \\pmb  X_2 {\\pmb  \\beta}_2  + \\pmb  e \\\\\n  & = \\pmb  X_1 {\\pmb  \\beta}_1 +  \\pmb  H_1  \\pmb  X_2 {\\pmb  \\beta}_2 + (\\pmb  I -\\pmb  H_1  ) \\pmb  X_2 {\\pmb  \\beta}_2  + \\pmb  e \\\\\n  & = [ \\pmb  X_1 {\\pmb  \\beta}_1 + \\pmb  H_1 \\pmb  X_2 {\\pmb  \\beta}_2 ]  + (\\pmb  I -\\pmb  H_1  ) \\pmb  X_2    {\\pmb  \\beta}_2  + \\pmb  e \\\\\n  & = \\pmb  X_1  [ {\\pmb  \\beta}_1 +  ({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t   \\pmb  X_2 {\\pmb  \\beta}_2]  + \\tilde {\\pmb  X}_2  {\\pmb  \\beta}_2  + \\pmb  e \\\\\n  & = \\pmb  X_1  \\tilde {\\pmb  \\beta}_1  + \\tilde {\\pmb  X}_2  {\\pmb  \\beta}_2  + \\pmb  e\n\\end{aligned}\n\\]\n이제 다음과 같은 변환된 모형을 고려하자.\n\\[\n\\pmb y =\\pmb  X_1  \\tilde {\\pmb  \\beta}_1  + \\tilde {\\pmb  X}_2  {\\pmb  \\beta}_2  + \\pmb  e\n\\tag{3.6}\\]\n위의 식에서 다음과 같이 새로운 계수벡터 \\(\\tilde {\\pmb  \\beta}_1\\) 와 변환된 계획행렬 \\(\\tilde{\\pmb  X}_2\\)를 정의하였다.\n\\[\n\\tilde {\\pmb  \\beta}_1  = {\\pmb  \\beta}_1 +  ({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t   \\pmb  X_2 {\\pmb  \\beta}_2,  \\quad\n\\tilde{\\pmb  X}_2 =   (\\pmb  I -\\pmb  H_1  ) \\pmb  X_2\n\\tag{3.7}\\]\n이제 변환된 모형 식 3.6 에서 두 계획행렬 \\(\\pmb  X_1\\)과 \\(\\tilde{\\pmb  X}_2\\)가 서로 직교하는 것을 알 수 있다.\n\\[  {\\pmb  X_1}^t \\tilde{\\pmb  X}_2 =  {\\pmb  X}_1^t (\\pmb  I -\\pmb  H_1  ) \\pmb  X_2=  {\\pmb  X}_1^t (\\pmb  I -{\\pmb  X}_1 ({\\pmb  X}_1^t {\\pmb  X}_1)^{-1} {\\pmb  X}_1^t ) \\pmb  X_2= \\pmb  0 \\]\n이제 확대된 모형 식 3.6 의 두 계획행렬 \\(\\pmb  X_1\\)과 \\(\\tilde{\\pmb  X}_2\\)가 서로 직교하므로 앞에서 나온 직교하는 계획행렬에 대한 회귀계수에 대한 결과를 이용하면 다음과 같이 회귀계수 추정량을 얻을 수 있다.\n이제 모형 식 3.6 의 회귀계수 \\(\\tilde {\\pmb  \\beta_1}\\) 과 \\(\\pmb  \\beta_2\\)의 추정량을 구해보면 다음과 같다.\n\\[\n\\hat {\\tilde{\\pmb  \\beta_1}} =  ({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t \\pmb  y, \\quad \\hat {\\pmb  \\beta}_2 = ({\\tilde {\\pmb  X}}_2^t \\tilde {\\pmb  X}_2)^{-1} {\\tilde {\\pmb  X}_2}^t \\pmb  y\n\\tag{3.8}\\]\n먼저 식 3.6 에서 \\({\\pmb  \\beta}_2\\) 에 대한 추정량 \\(\\hat {\\pmb  \\beta}_2\\)는 반응변수 \\(\\pmb  y\\) 를 변환된 계획 행렬 \\(\\tilde {\\pmb  X}_2 =  (\\pmb  I -\\pmb  H_1  ) \\pmb  X_2\\)로 적합할 때의 회귀계수이다.\n\\[ y = [(\\pmb  I -\\pmb  H_1  ) \\pmb  X_2]  {\\pmb  \\beta}_2 + \\pmb  e \\]\n식 3.8 에 주어진 추정량 \\(\\hat {\\pmb  \\beta}_2\\) 를 다시 다음과 같이 유도할 수 있다.\n\\[\n\\begin{aligned}\n\\hat {\\pmb  \\beta}_2 & = ({\\tilde {\\pmb  X}}_2^t \\tilde {\\pmb  X}_2)^{-1} {\\tilde {\\pmb  X}_2}^t \\pmb  y \\\\\n   & =   [ {\\pmb  X}_2^t  (\\pmb  I -\\pmb  H_1  )  (\\pmb  I -\\pmb  H_1  )   \\pmb  X_2]^{-1}  {\\pmb  X}_2^t (\\pmb  I -\\pmb  H_1  ) \\pmb  y \\\\\n   & =    [ {\\pmb  X}_2^t  (\\pmb  I -\\pmb  H_1  )  (\\pmb  I -\\pmb  H_1  )   \\pmb  X_2]^{-1}  {\\pmb  X}_2^t (\\pmb  I -\\pmb  H_1  ) (\\pmb  I -\\pmb  H_1  ) \\pmb  y \\\\\n    & = ({\\tilde {\\pmb  X}_2}^t \\tilde {\\pmb  X}_2)^{-1} {\\tilde {\\pmb  X}_2}^t  [(\\pmb  I -\\pmb  H_1  ) \\pmb  y] \\\\\n    & = ({\\tilde {\\pmb  X}_2}^t \\tilde {\\pmb  X}_2)^{-1} {\\tilde {\\pmb  X}_2}^t  \\tilde {\\pmb y}  \n\\end{aligned}\n\\] 위의 유도를 보면 반응변수 \\(\\pmb  y\\) 에서 \\(\\pmb  X_1\\)로 적합한 후에 구한 잔차벡터 \\(\\tilde {\\pmb  y} = (\\pmb  I -\\pmb  H_1  ) \\pmb  y\\) 를 새로운 반응변수로 고려한 후, 추가된 변수에 대한 계획행렬 \\(\\pmb  X_2\\) 에서 먼저 고려한 변수의 계획행렬 \\(\\pmb X_1\\) 의 효과를 제거한 \\((\\pmb  I -\\pmb  H_1  )\\pmb  X_2\\) 로 적합한 경우의 회귀계수로 나타난다.\n\\[ \\tilde {\\pmb  y} = \\tilde {\\pmb  X}_2 \\pmb \\beta_2 + \\pmb e \\quad \\rightarrow \\quad (\\pmb  I -\\pmb  H_1  )y = [(\\pmb  I -\\pmb  H_1  ) \\pmb  X_2]  {\\pmb  \\beta}_2 + \\pmb  e \\]\n또한 식 3.6 의 회귀계수 \\(\\tilde{\\pmb  \\beta_1}\\)의 추정량은 직교성에 의하여 \\(({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t \\pmb  y\\) 으로 주어지며 이는 모형 식 3.4 에서 구한 회귀계수의 추정량과 같다.\n\\[ \\hat {\\tilde{\\pmb  \\beta_1}} = \\hat {{\\pmb  \\beta}}_{1*} = ({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t \\pmb  y\\]\n이제 식 3.7 의 관계를 이용하면 모형 식 3.5 에서 나타난 회귀계수 \\({\\pmb  \\beta}_1\\) 의 추정량을 다음과 같이 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\hat {\\pmb  \\beta}_1 & = \\hat {\\tilde {\\pmb  \\beta}}_1  - ({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t   \\pmb  X_2 \\hat {\\pmb  \\beta}_2 \\\\\n& = \\hat {{\\pmb  \\beta}}_{1*}  - ({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t   \\pmb  X_2 \\hat {\\pmb  \\beta}_2 \\\\\n& =({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X}_1^t (\\pmb  y - \\pmb  X_2 \\hat {\\pmb  \\beta}_2)\n\\end{aligned}\n\\tag{3.9}\\]\n식 3.9 에 주어진 회귀계수 \\({\\pmb  \\beta}_1\\) 의 추정량은 새로운 변수를 추기하기 전의 모형에서 구한 추정량 \\(\\hat {{\\pmb  \\beta}}_{1*}\\) 을 새로운 변수를 추가한 후의 모형에서 추가된 변수에 대한 회귀계수의 추정량 \\(\\hat {\\pmb  \\beta}_2\\) 으로 보정힌 형태이다.\n이제 간단한 예제를 통하여 위에서 유도한 공식을 적용해 보자.\n먼저 3 개의 설명변수 \\(x_1, x_2, x_3\\) 를 가진 10 개의 자료를 임의로 만들어 보자.\n\nset.seed(23123)\nx1 &lt;- c(1,2,3,4,5,6,7,8,9,9)\nx2 &lt;- c(1,4,2,5,3,2,4,3,1,2)\nx3 &lt;- c(6,3,2,3,1,4,5,3,2,1)\ny &lt;- 2 + 3*x1 + 4*x2 + 5*x3 + rnorm(10)\n\ndf &lt;- data.frame(x1,x2,x3,y)\ndf\n\n   x1 x2 x3        y\n1   1  1  6 38.99584\n2   2  4  3 38.17609\n3   3  2  2 27.13590\n4   4  5  3 49.86576\n5   5  3  1 32.88329\n6   6  2  4 48.43808\n7   7  4  5 63.30271\n8   8  3  3 53.54604\n9   9  1  2 41.86736\n10  9  2  1 43.30445\n\n\n먼저 2개의 독립변수 \\(x_1\\) 과 \\(x_2\\) 가 있는 모형을 적합해 보자.\n\\[ y = \\beta_{0*} + \\beta_{1*} x_1 + \\beta_{2*} x_2 + e  \\tag{3.10}\\]\n\nlm1s &lt;- lm(y ~ x1 + x2, data=df)\nlm1s$coefficients\n\n(Intercept)          x1          x2 \n  22.964557    1.948430    3.802026 \n\n\n또한 모형 식 3.10 에서 사용한 계획행렬 \\(\\pmb X_1\\) 을 구해보자\n\nX1 &lt;- model.matrix(lm1s)\nX1\n\n   (Intercept) x1 x2\n1            1  1  1\n2            1  2  4\n3            1  3  2\n4            1  4  5\n5            1  5  3\n6            1  6  2\n7            1  7  4\n8            1  8  3\n9            1  9  1\n10           1  9  2\nattr(,\"assign\")\n[1] 0 1 2\n\n\n다음으로 모든 독립변수가 있는 모형을 적합해 보자.\n\\[ y = \\beta_{0} + \\beta_{1} x_1 + \\beta_{2} x_2 + \\beta_{3} x_3 + e  \\tag{3.11}\\]\n\nlmAll &lt;- lm(y ~ x1 + x2 + x3, data=df)\nlmAll$coefficients\n\n(Intercept)          x1          x2          x3 \n-0.05735613  3.15491311  4.16172306  5.17857500 \n\n\n또한 모형 식 3.11 에서 모든 독립변수를 사용한 경우 계획행렬 \\(\\pmb X\\) 와 추가된 변수 \\(x_3\\) 에 대한 열만 가지는 계획행렬 \\(\\pmb X_2\\) 를 구해보자\n\nX &lt;- model.matrix(lmAll)\nX\n\n   (Intercept) x1 x2 x3\n1            1  1  1  6\n2            1  2  4  3\n3            1  3  2  2\n4            1  4  5  3\n5            1  5  3  1\n6            1  6  2  4\n7            1  7  4  5\n8            1  8  3  3\n9            1  9  1  2\n10           1  9  2  1\nattr(,\"assign\")\n[1] 0 1 2 3\n\n\n\nX2 &lt;- X[,4]\nX2\n\n 1  2  3  4  5  6  7  8  9 10 \n 6  3  2  3  1  4  5  3  2  1 \n\n\n이제 식 3.7 에 주어진 \\(\\tilde {\\pmb X}_2\\) 를 계산하고 이를 이용하여 \\(\\hat {\\pmb \\beta}_2\\) 를 구해보자.\n\nH1 &lt;- X1 %*% solve(t(X1) %*% X1) %*% t(X1)\nX2t &lt;- (diag(10) - H1) %*% X2\nX2t\n\n         [,1]\n1   1.8568267\n2  -0.7018216\n3  -1.6077630\n4  -0.1664113\n5  -2.0723527\n6   1.0911645\n7   2.4630575\n8   0.6265747\n9  -0.2793667\n10 -1.2099081\n\n\n\nbeta2 &lt;- solve(t(X2t) %*% X2t) %*% t(X2t) %*% y\nbeta2\n\n         [,1]\n[1,] 5.178575\n\n\n위에서 구한 회귀계수 beta2 는 모든 독립 변수가 있는 모형에서의 \\(x_3\\) 에 대한 회귀계수 추정량과 같다.\n이제 절편, \\(x_1\\), \\(x_2\\) 만 있는 모형에서 구한 회귀계수를 위에서 구한 beta2를 이용하여 보정해 보자. 아래 보정된 회귀계수 추정량은 모든 독립변수를 고려한 모형에서의 회귀계수 추정량과 같다.\n\nbeta1 &lt;- lm1s$coefficients - solve(t(X1) %*% X1) %*% t(X1) %*% X2 %*% beta2\nbeta1\n\n                   [,1]\n(Intercept) -0.05735613\nx1           3.15491311\nx2           4.16172306\n\n\n이제 앞에서 본 예제와 같이 특별하게 1개의 설명변수를 추가하는 경우를 알아보자. 이 경우는 추가된 변수에 대한 계획행렬 \\(\\pmb  X_2 = \\pmb  x_{p}\\)는 하나의 벡터이다. 따라서\n\\[  \n  \\pmb  y = \\pmb  X_1  \\pmb  \\beta_1  + \\pmb  X_2 \\pmb  \\beta_2 = \\pmb  X_1  \\pmb  \\beta  + {\\pmb  x}_p  \\beta_p  + \\pmb  e\n\\tag{3.12}\\]\n위의 식 3.8 에서\n\\[\n\\tilde {\\pmb X}_2 =  (\\pmb  I -{\\pmb  H}_1  ) {\\pmb  x}_p \\equiv \\tilde {\\pmb x}_p\n\\] 로 정의하면 회귀식 식 3.12 에서 하나 추가된 설명변수에 대한 회귀계수의 추정량은 다음과 같다.\n\\[\n\\begin{aligned}\n\\hat { \\beta}_p & = ({\\tilde {\\pmb  X}_2}^t \\tilde {\\pmb  X}_2)^{-1} {\\tilde {\\pmb  X}_2}^t   \\pmb  y  \\\\\n  & = [ \\tilde {\\pmb  x}_p^t  \\pmb  x_p ]^{-1}  \\tilde {\\pmb  x}_p^t \\pmb  y \\\\\n  & = [ \\tilde {\\pmb  x}_p^t  \\pmb  x_p ]^{-1}  {\\pmb  x}_p^t ( \\pmb  I - \\pmb  H_1)  \\pmb  y \\\\\n  & = [ \\tilde {\\pmb  x}_p^t  \\pmb  x_p ]^{-1}  {\\pmb  x}_p^t ( \\pmb  I - \\pmb  H_1) (\\pmb  I -\\pmb  H_1  ) \\pmb  y \\\\\n  & = [ \\tilde {\\pmb  x}_p^t  \\pmb  x_p ]^{-1} \\tilde {\\pmb  x}_p^t ( \\pmb  I - \\pmb  H_1) \\pmb  y \\\\\n  & = [ \\tilde {\\pmb  x}_p^t  \\pmb  x_p ]^{-1} \\tilde {\\pmb  x}_p^t \\tilde {\\pmb  y} \\\\\n  & = \\frac{ \\tilde {\\pmb  x}_p^t \\tilde {\\pmb  y} }{ \\tilde {\\pmb  x}_p^t \\tilde {\\pmb  x}_p }\n\\end{aligned}\n\\]\n위의 식에서 \\(\\tilde {\\pmb  y} =  ( \\pmb  I - \\pmb  H_1){\\pmb  y}\\) 이다.\n또한 식 3.12 과 같이 새로운 변수 \\(x_p\\) 가 추가되면 새로운 변수가 추가된 후에는 회귀계수 추정량이 다음과 같아 보정된다.\n\\[\n\\begin{aligned}\n\\hat {\\pmb  \\beta}_1 & =({\\pmb  X}_1^t \\pmb  X_1)^{-1} {\\pmb  X_1}^t [\\pmb  y -  \\pmb  X_2 \\hat {\\pmb  \\beta}_2  ] \\\\\n  & = ({\\pmb  X_1}^t \\pmb  X_1)^{-1} {\\pmb  X_1}^t \\pmb  y -  ({\\pmb  X_1}^t \\pmb  X)_1^{-1} {\\pmb  X}_1^t \\pmb  x_p \\frac{ \\tilde {\\pmb  x}_p^t \\tilde {\\pmb  y} }{ \\tilde {\\pmb  x}_p^t \\tilde {\\pmb  x}_p }  \\\\\n  & = ({\\pmb  X}_1^t \\pmb  X)_1^{-1} {\\pmb  X}_1^t \\pmb  y -  ({\\pmb  X}_1^t \\pmb  X)_1^{-1} {\\pmb  X}_1^t [\\pmb  x_p (\\tilde {\\pmb  x}_p^t \\tilde {\\pmb  x}_p )^{-1} \\tilde {\\pmb  x}_p^t ] \\tilde {\\pmb  y} \\\\\n  & = \\hat {\\pmb \\beta}_{1*} -  ({\\pmb  X}_1^t \\pmb  X)_1^{-1} {\\pmb  X}_1^t [\\pmb  x_p (\\tilde {\\pmb  x}_p^t \\tilde {\\pmb  x}_p )^{-1} \\tilde {\\pmb  x}_p^t ] \\tilde {\\pmb  y}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>모형의 비교</span>"
    ]
  },
  {
    "objectID": "qmd/inference2.html#부분-f-검정과-가능도비-검정",
    "href": "qmd/inference2.html#부분-f-검정과-가능도비-검정",
    "title": "3  모형의 비교",
    "section": "3.3 부분 F-검정과 가능도비 검정",
    "text": "3.3 부분 F-검정과 가능도비 검정\n앞 절에서 회귀모형에 새로운 독립변수를 1개 이상 추가할 경우 회귀계수 추정량과 제곱합의 변화를 살펴보있다.\n실제 자료를 분석하여 회귀 모형식을 만드는 경우 일반적으로 중요한 몇 개의 설명변수부터 모형에 포함시키고 다른 변수들을 추가한다. 반대로 중요한 변수에 대한 사전 정보가 없다면 가능한 모든 변수를 모두 포함시킨 후에 중요하지 않은 변수들을 제거하기도 한다. 이런 두 가지 방법 모두 축차적으로 변수를 추가 또는 제거하는 방법으로 고려하는 모형들이 포함 관계를 가진다.\n이렇게 포함관계를 가지는 두 모형을 고려해 보자. 먼저 설명변수의 수가 많은 모형을 최대 모형(full model)이라고 부르자.\n\\[ y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots \\beta_{p-1} x_{i,p} + \\beta_p x_{i,p} + \\dots + \\beta_{p+q} + e_i \\]\n위의 식은 모두 절편을 제외하면 모두 \\(p+q\\) 개의 설명변수를 가진 선형 모형이다. 위의 최대모형을 다음과 같은 행렬식으로 써보자\n아래에 정의된 최대\n\\[\n\\pmb  y= \\pmb  X {\\pmb  \\beta} + \\pmb  e, , \\quad \\pmb  e \\sim N(\\pmb  0, \\sigma^2 \\pmb  I_n)  \\quad \\text{ Full model}\n\\tag{3.13}\\]\n이제 축소된 모형으로 최대모형에서 마자막 \\(q\\)개의 설명변수가 모형에 포함되지 않은 경우를 생각하자.\n\\[ y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots \\beta_{p-1} x_{i,p} + \\beta_p x_{i,p} + e_i \\]\n축소모형은 다음과 같은 행렬식으로 표시한다.\n\\[\n\\pmb  y= \\pmb  X_1 {\\pmb  \\beta_1} + \\pmb  e, , \\quad \\pmb  e \\sim N(\\pmb  0, \\sigma^2 \\pmb  I_n)  \\quad \\text{ Reduced Model}\n\\tag{3.14}\\]\n참고로 최대모형 식 3.13 의 계획행렬 \\(\\pmb  X = [\\pmb  X_1, \\pmb  X_2]\\) 의 차원은 \\(n \\times (p+q+1)\\)이고 축소 모형 식 3.14 의 계획행렬 \\(\\pmb  X_1\\) 의 차원은 \\(n \\times (p+1)\\) 이다.\n여기서 유의할 점은 최대 모형은 축소모형을 포함한다는 것이다. 만약 최대모형에서 마지막 \\(q\\)개의 설명변수들에 대한 회귀계수들이 모두 0이면, 즉 \\(\\beta_{p+1} = \\cdots \\beta_{p+q}=0\\) 이면 축소모형이 된다.\n교재에서는 추가제곱합을 이용한 부분 F-검정(교과서 p.158-161)을 설명한다. 즉, 다음과 같은 가설을 검정하고자 한다.\n\\[\nH_0: \\beta_{p+1} = \\beta_{p+2} =\\cdots = \\beta_{p+q}=0 \\text{ (Reduced)} \\quad \\text{ vs.} \\quad H_1: \\text{ not } H_0 \\text{  (Full)}  \n\\tag{3.15}\\]\n위와 같은 가설을 검정하기 위한 부분 F-검정의 검정 통계량 \\(F_0\\) 는 다음과 같이 주어진다 (교과서 식 4.4).\n\\[\nF_0 = \\frac{[SSE(R) - SSE(F)]/(df_R - df_F)}{SSE(F)/df_F}   \n\\]\n만약 \\(H_0\\) 가 참이면 검정 통계량 \\(F_0\\) 는 자유도가 각각 \\(df_R-df_F\\)와 \\(df_F\\) 를 가지는 F-분포를 따르므로 이를 이용하여 모형에 새로운 변수를 추가하는 검정을 수행하는 부분 F-검정을 실시할 수 있다.\n참고로 위의 식들에서 자유도 \\(df_R\\) 과 \\(df_F\\)는 다음과 같이 주어진다.\n\\[ df_R = n - (p+q+1), \\quad df_F = n - (p+1)\\]\n선형모형에 대한 최대 가능도 추정법은 장 1 에서 설명하였다. 위의 최대 모형과 축소모형에 대한 최대가능도 추정법을 설명하기 위하여 다음과 같은 식을 사용할 것이다.\n임의의 벡터 \\(\\pmb  v\\)에 대하여 노름 \\(\\norm{\\pmb  v}^2\\) 을 다음과 같아 정의한다. \\[ \\norm{\\pmb  v}^2 = \\pmb  v^t \\pmb  v \\] 최대모형에 대한 계획행렬 \\(\\pmb  X\\) 에 대한 모자행렬을 이용하여 사영행렬 \\(\\pmb  P\\) 와 \\(\\pmb  Q\\)를 다음과 같이 정의한다.\n\\[ \\pmb  P \\equiv \\pmb  H(\\pmb  X) = \\pmb X ({\\pmb X}^t {\\pmb X} )^{-1} {\\pmb X}^t , \\quad \\pmb  Q = \\pmb  I - \\pmb  P  \\]\n또한 축소모형의 계획행렬 \\(\\pmb  X_1\\)에 대한 모자행렬을 이용하여 사영행렬 \\(\\pmb  P_1\\) 와 \\(\\pmb  Q_1\\)를 다음과 같이 정의한다.\n\\[ \\pmb  P_1 \\equiv \\pmb  H_1(\\pmb  X_1) = \\pmb X_1 ({\\pmb X}_1^t {\\pmb X}_1 )^{-1} {\\pmb X}_1^t,  \\quad \\pmb  Q_1 = \\pmb  I - \\pmb  P_1 \\]\n분산에 대한 모수를 \\(\\tau=\\sigma^2\\) 과 같이 쓰고 편의상 반응 벡터의 평균을 다음과 같이 표시하자\n\\[ \\pmb  \\mu = \\pmb  X  \\pmb  \\beta, \\quad  \\pmb  \\mu_1 = \\pmb  X_1  \\pmb  \\beta_1  \\]\n이제 최대모형 식 3.13 에 대한 최대 가능도 추정을 생각해 보자.\n\\[\n\\begin{aligned}\n\\ell_n( \\pmb  \\theta_F; \\pmb  y)\n   &= -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac{1}{2\\tau} ( \\pmb  y- \\pmb  X  \\pmb  \\beta)^t ( \\pmb  y- \\pmb  X  \\pmb  \\beta)   \\\\\n   & =  -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac {1}{2\\tau} \\norm{ \\pmb  y- \\pmb  X  \\pmb  \\beta }^2   \\\\\n   & =  -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac {1}{2\\tau} \\norm{ \\pmb  y- \\pmb  \\mu }^2    \\\\\n   & =  -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac {1}{2\\tau} \\norm{ \\pmb  y- \\pmb  P \\pmb  y + \\pmb  P \\pmb  y - \\pmb  \\mu }^2   \\\\\n   & =  -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac {1}{2\\tau} \\left [ \\norm{ \\pmb  y- \\pmb  P \\pmb  y}^2 + \\norm{\\pmb  P \\pmb  y - \\pmb  \\mu }^2 \\right ]   \\\\\n      & =  -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac {1}{2\\tau} \\left [ \\norm{ \\pmb  Q \\pmb  y}^2 + \\norm{\\pmb  P \\pmb  y - \\pmb  \\mu }^2 \\right ]   \n\\end{aligned}\n\\]\n위의 최대 모형에 대한 로그 가능도 함수 \\(\\ell_n( \\pmb  \\theta; \\pmb  y)\\)에서 \\(\\mu = \\pmb  X \\pmb  \\beta\\)에 대한 최대가능도 추정량과 모분산 \\(\\tau\\) 에 대한 추정량은 다음과 같아 주어진다(장 1 참조)\n\\[ \\hat \\mu = \\pmb  X \\hat {\\pmb  \\beta} = \\pmb  P \\pmb  y, \\quad \\hat \\tau_F = \\frac{1}{n} \\norm{ \\pmb  Q \\pmb  y}^2  = \\frac{1}{n} SSE(F)\\]\n따라서 최대 가능도 추정량 \\(\\hat {\\pmb  \\mu}\\) 와 \\(\\hat {\\tau}_F\\)를 로그 가능도 함수에 넣으면 다음과 같은 결과가 주어진다.\n\\[\n\\underset{\\pmb  \\mu, \\pmb  \\tau }{\\max} \\ell_n( \\pmb  \\theta_F; \\pmb  y)  = -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\hat \\tau_F - \\frac{n}{2}\n\\]\n위의 결과를 가능도 함수로 표시하면\n\\[\n\\underset{\\pmb  \\mu, \\pmb  \\tau }{\\max} L_n( \\pmb  \\theta_F; \\pmb  y)  =(2 \\pi e)^{-n/2} [SSE(F)/n]^{-n/2}\n\\tag{3.16}\\]\n축소모형 식 3.14 에 대해서도 같은 방법으로 최대 가능도 춛정량을 구하면 다음과 같이 주어지며\n\\[ \\hat \\mu_1 = \\pmb  X_1 \\hat {\\pmb  \\beta_1} = \\pmb  P_1 \\pmb  y, \\quad \\hat \\tau_R = \\frac{1}{n} \\norm{ \\pmb  Q_1 \\pmb  y}^2 = \\frac{1}{n} SSE(R)   \\]\n로그 가능도 함수에 넣어면 다음과 같은 결과가 주어진다.\n\\[\n\\underset{\\pmb  \\mu_1, \\pmb  \\tau }{\\max} \\ell_n( \\pmb  \\theta_R; \\pmb  y)  = -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\hat \\tau_R - \\frac{n}{2}\n\\]\n위의 결과를 가능도 함수로 표시하면 \\[\n\\underset{\\pmb  \\mu, \\pmb  \\tau }{\\max} L_n( \\pmb  \\theta_R; \\pmb  y)  =(2 \\pi e)^{-n/2} [SSE(R)/n]^{-n/2}\n\\tag{3.17}\\]\n최대 가능도 추정에서 만약 최대 모형 \\(F\\) 가 축소모형 \\(R\\) 을 포함하면 가설 식 3.15 에 대한 검정이 가능하다.\n위의 가설 검정에서 두 모형의 가능도 함수 식 3.16 와 식 3.17 의 비, 즉 가능도 비(Likelihood Ratio) \\(\\Lambda\\) 는 다음과 같이 주어진다.\n\\[\n\\Lambda= \\frac{ \\underset{\\pmb  \\mu_1, \\pmb  \\tau }{\\max} L_n( \\pmb  \\theta_R; \\pmb  y)}{\\underset{\\pmb  \\mu, \\pmb  \\tau }{\\max} L_n( \\pmb  \\theta_F; \\pmb  y) } =  \\left [ \\frac{SSE(F)}{SSE(R)} \\right ]^{n/2}\n\\tag{3.18}\\]\n위의 가능도 비 \\(\\Lambda\\)가 작으면 귀무가설 \\(H_0\\)을 기각한다.\n\\[ \\text{if } \\Lambda &lt; c', \\text{ then reject } H_0 \\]\n식 식 3.18 에 나타난 가능도 비를 다시 표현해 보자\n\\[ \\Lambda = \\left [ \\frac{SSE(F)}{SSE(R)} \\right ]^{n/2} = \\left [ 1 +\\frac{SSE(R) - SSE(F)}{SSE(F)} \\right ]^{-n/2} \\equiv (1 + F^*)^{-n/2} \\] 위의 식을 보면 \\(\\Lambda\\)가 \\(F^*\\)에 반비례하므로 다음과 같이 \\(F^*\\)가 크면 \\(H_0\\)를 기각할 수 있다. 이제 회귀분석에서 주로 쓰이는 부분 F-검정 통계량을 위의 식에서 나타난 \\(F^*\\)로 표시해보면 다음과 같이 쓸수 있다. \\[\n\\text{if } F = \\frac{n-(p+q+1)}{q} F^* = \\frac{[SSE(R) - SSE(F)]/q}{SSE(F)/[n-(p+q+1)]} &gt; c, \\text{ then reject } H_0  \n\\tag{3.19}\\]\n위의 식 3.19 에 있는 가설검정의 절차는 추가제곱합을 이용한 부분 F-검정(교과서 p.158-161)과 동일한 검정이다. 따라서 부분 F-검정은 가능도비 검정이다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>모형의 비교</span>"
    ]
  },
  {
    "objectID": "qmd/modeleval2.html",
    "href": "qmd/modeleval2.html",
    "title": "4  모형의 진단",
    "section": "",
    "text": "4.1 등분산성 가정의 위반\n일반적인 회귀분석모형에서 \\[ \\pmb y = \\pmb X \\pmb \\beta + \\pmb e \\]\n오차항이 다음과 같이 서로 독립이고 등분산성을 만족한다면 \\[ var(\\pmb e) =\\sigma^2 \\pmb I_n \\]\n최소제곱법에 의한 회귀계수 추정량 \\(\\hat  {\\pmb \\beta}\\) 다음과 같고\n\\[  \\hat {\\pmb \\beta}  = (\\pmb X^t\\pmb X)^{-1}\\pmb X^t\\pmb y \\]\n이는 최소분산선형추정량(BLUE)이다. 만약에 오차항에 대한 가정이 만족하지 않는다면 최소제곱법 추정량 \\(\\hat {\\pmb \\beta}\\)의 최적성이 유지되는가에 대한 질문이 생기게 된다.\n여기서 오차항의 분산에 대하여 좀더 일반적인 모형을 생각해보자. 가장 일반적인 모형은 다음과 같은 임의의 양정치행렬(positive definite matrix) \\(\\pmb V\\)가 오차항의 공분산 행렬인 경우이다.\n\\[ Var(\\pmb e) = \\pmb V \\] 가장 일반적인 경우를 고려하기 전에 전형적인 가정을 약간 벗어나면서 실제 문제에서 흔히 접하는 경우를 생각해 보자.\n일단 오차항이 서로 독립이지만 분산이 다른 경우이다.\n\\[ Var(\\pmb e) = \\text{diag} [\\sigma_1^2,\\sigma_2^2,\\dots, \\sigma_n^2 ]  \\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>모형의 진단</span>"
    ]
  },
  {
    "objectID": "qmd/modeleval2.html#등분산성-가정의-위반",
    "href": "qmd/modeleval2.html#등분산성-가정의-위반",
    "title": "4  모형의 진단",
    "section": "",
    "text": "4.1.1 가중 최소제곱법\n이러한 경우에 가중 최소제곱법(Weighted Leadt Square Estimator; WLSE)을 사용하면 최소제곱법 추정량의 최적성을 유지할 수 있다.\n오차항의 분산 \\(\\sigma_1^2,\\sigma_2^2,\\dots, \\sigma_n^2\\)을 안다고 가정하면 각 관측값 \\(y_i\\)를 해당 오차의 표분편차 \\(\\sigma_i\\)로 나누면 등분산성을 다시 얻을 수 있다.\n\\[ Var (y_i) = Var (\\pmb x_i^t \\pmb \\beta + e_i)=Var ( e_i)=\\sigma_i^2 \\]\n\\[ \\Rightarrow Var (y_i/\\sigma_i) = Var (\\pmb x_i^t \\pmb \\beta' + e_i/ \\sigma_i)=Var ( e_i/\\sigma_i) = 1 \\]\n이 떄 새로운 관측치 \\(y'_i = y_i/\\sigma_i\\)를 사용하여 최소제곱법을 적용하면\n\\[ \\min_{\\pmb \\beta'} \\sum_{i=1}^n (y'_i - \\pmb x_i^t \\pmb \\beta')^2 = \\min_{\\pmb \\beta}  \\sum_{i=1}^n \\left [\\frac{1}{\\sigma_i^2} \\right ](y_i - \\pmb x_i^t \\pmb \\beta)^2 \\equiv \\min_{\\pmb \\beta}  \\sum_{i=1}^n w_i (y_i - \\pmb x_i^t \\pmb \\beta)^2\\]\n여기서 \\(w_i=1/\\sigma^2_i\\)이다. 이러한 가중최소제곱법은 각 관측치에 대하여 서로 다른 가중치를 적용하여 최소제곱 추정량을 구하며 위의 경우에는 가중치가 반응값의 분산에 반비례한다. 따라서 분산이 큰 오차항을 가진 반응값의 가중치는 분산이 작은 반응값에 비해 상대적으로 작다. 이러한 가중치와 분산의 관계는 변이가 적은 반응값 근방에서 오차를 더욱 줄이려고 하는 직관적인 생각과 일치한다. 가중치를 적용한 최소제곱 추정량은 다음과 같이 나타낼 수 있다.\n\\[ \\hat  {\\pmb \\beta}= (\\pmb X^t \\pmb W \\pmb X)^{-1}\\pmb X^t \\pmb W \\pmb y \\] 여기서\n\\[ \\pmb W = \\text{diag} \\left [\\frac{1}{\\sigma_1^2},\\frac{1}{\\sigma_2^2},\\dots, \\frac{1}{\\sigma_n^2} \\right ] = [Var(\\pmb e)]^{-1} =\\pmb V^{-1} \\]\n위에서 본 가중최소제곱법을 오차항이 일반적인 공분산 행렬 \\(\\pmb V\\)를 가질 때 적용하면 다음과 같이 목적 함수를 나타낼 수 있으며\n\\[ \\min_{\\pmb \\beta} (\\pmb y - \\pmb X \\pmb \\beta)^t \\pmb V^{-1} (\\pmb y - \\pmb X \\pmb \\beta) \\]\n가중최소제곱추정량은 다음과 같이 나타낼 수 있다.\n\\[\n\\hat  {\\pmb \\beta}_* = (\\pmb X^t \\pmb V^{-1} \\pmb X)^{-1} \\pmb X^t \\pmb V^{-1} \\pmb y\n\\tag{4.1}\\]\n가중최소제곱추정량은 다음과 같은 성질은 만족한다.\n\n가중최소제곱추정량은 불편추정량이다: \\(E(\\hat  {\\pmb \\beta}_*) =\\pmb \\beta\\)\n가중최소제곱추정량 \\(\\hat  {\\pmb \\beta}_*\\)는 분산이 가장 작은 불편선형 추정량이다 (Gauss-Markov Theorem)\n가중최소제곱추정량 \\(\\hat  {\\pmb \\beta}_*\\)의 분산:\n\n\\[ Var(\\hat  {\\pmb \\beta}_*) =  (\\pmb X^t \\pmb V^{-1} \\pmb X)^{-1} \\]\n\n가중최소제곱추정량 \\(\\hat  {\\pmb \\beta}_*\\)은 \\(\\pmb y\\)가 평균이 \\(\\pmb X \\pmb \\beta\\)이고 분산이 \\(V\\)인 정규분포에서 \\(\\pmb \\beta\\)에 대한 최대우도추정량이다.\n일반최소제곱추정량도 불편추정량이다: \\(E(\\hat  {\\pmb \\beta}) =\\pmb \\beta\\)\n\n가중최소제곱법에서 유의할 점은 오차항의 공분산 행렬 \\(\\pmb V\\)에 대하여 모르는 경우 이를 추정해야 하며 가중최소제곱추정량에 공분산 행렬의 추정치\n\\(\\hat {\\pmb V}\\)을 사용할 경우 위에서 언급한 최적성은 더 이상 성립하지 않는다.\n\\[ \\hat {\\pmb \\beta}_* = (\\pmb X^t \\hat {\\pmb V}^{-1} \\pmb X)^{-1}\\pmb X^t \\hat {\\pmb V}^{-1} \\pmb y \\]\n더 나아가 공분산행렬 \\(\\pmb V\\)에 대한 추정은 어려운 문제이므로 그 추정 방법과 통계적 성질을 잘 고려하여 사용해야 한다. 정확한 추정을 위해서 또는 자료의 특성을 이용하여 공분산 행렬 \\(\\pmb V\\)에 대한 모형을 어느 정ㅇ도 단순화하는 것이 바람직하다. 예를 들어 공분산 행렬 \\(\\pmb V\\)를 다음과 같이 나타낼 수 있다면 유용할 것이다.\n\\[ \\pmb V = \\sigma^2 \\text{diag} [v_1, v_2,\\dots, v_n ] \\] 여기서 \\((v_1,v_2,\\dots,v_n)\\)은 알려진 값이고 \\(\\sigma^2\\)는 추정해야하는 모수이다.\n오차항의 등분산성에 대한 가정을 검토하기 위한 방법중 가장 유용하고 간단한 방법은 잔차그림을 이용하는 것이다. 잔차 \\(r_i\\)와 적합된 값 \\(\\hat y_i\\)에 대한 잔차그림을 그려서 잔차의 퍼진 정도가 적합된 값 \\(\\hat y_i\\)에 따라 변하면 등분산성에 대한 가정을 의심해봐야 한다. 실제 자료에서 반응값의 분산이 독립변수에 비례하여 나타나는 경우가 많다. 단순회귀모형을 고려하고\n\\[ y_i= \\beta_0 + \\beta_1 x_i + e_i \\] 오차항이 서로 독립이며 그 분산이 독립변수에 비례한다고 가정하자.\n\\[ Var (e_i) = x_i \\sigma^2 \\] 이러한 경우는 독립변수의 값이 양인 경우이며 독립변수의 값이 커지면 반응값의 분산도 커진다. 이러한 경우 독립변수와 종속변수의 관계, 회귀식, 잔차그림은 다음과 같이 나타난다.\n\n\n\n\n\n오차항의 등분산성이 위반된 경우",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>모형의 진단</span>"
    ]
  },
  {
    "objectID": "qmd/modeleval2.html#변수변환",
    "href": "qmd/modeleval2.html#변수변환",
    "title": "4  모형의 진단",
    "section": "4.2 변수변환",
    "text": "4.2 변수변환\n변수변환(Variable transformation)은 독립변수와 종속변수를 변환함으로서 회귀식의 적합도를 향상시켜 예측력을 높일 수 있을뿐 아니라 최소제곱법에서의 등분산성 가정에 대한 만족도를 높일 수 있는 유용한 방법이다 (variance stabilization). 이 절에서는 변수변환의 종류와 그 효과를 단순회귀식에서 살펴본다. 중회귀의 경우에는 변수변환의 적용을 복합적으로 고려해야 할 것이다.\n\n4.2.1 지수모형과 멱함수: 로그변환\n회귀식에 대한 모형이 지수함수 모형인 경우, 즉 독립변수와 종속변수가 다음과 같은 경우 \\[ y = \\beta_0 \\exp (\\beta_1 x) \\] 종속변수에 대한 로그 변환(log transformation)을 하면 선형관계에 매우 가깝게 된다 ( 아래 그림 참조). \\[ \\log(y_i) = \\beta,_0 + \\beta_1 x_i + e_i \\] 여기서 주의할 점은 원래의 지수모형에 오차항의 지수함수가 곱해지는 형태가 되어야 로그 변환후에 등분산성의 가정을 만족하게 된다. 즉 오차항 \\(e_i\\)를 서로 독립이고 평균이 0, 분산이 \\(\\sigma^2\\)이라고 하면 다음과 같은 관계가 성립된다. \\[ y_i = \\beta_0 \\exp (\\beta_1 x_i)\\exp(e_i) \\quad \\Rightarrow \\quad \\log(y_i) = \\beta'_0 + \\beta_1 x_i + e_i \\]\n\n\n\n\n\n지수모형과 로그변환\n\n\n\n\n회귀식에 대한 모형이 멱함수모형인 경우, 즉 독립변수와 종속변수가 다음과 같은 경우 \\[ y = \\beta_0  x^\\beta_1 \\] 독립변수와 종속변수에 대한 로그 변환을 하면 선형관계에 매우 가깝게 된다. \\[ \\log(y_i) = \\beta,_0 + \\beta_1 \\log(x_i) + e_i \\] 여기서 주의할 점도 원래의 멱함수모형에 오차항이 곱해지는 형태가 되어야 로그 변환후에 등분산성의 가정을 만족하게 된다. 즉 오차항 \\(e_i\\)를 서로 독립이고 평균이 0, 분산이 \\(\\sigma^2\\)이라고 하면 다음과 같은 관계가 성립된다. \\[ y_i = \\beta_0  x^\\beta_1 \\exp(e_i) \\quad \\Rightarrow \\quad \\log(y_i) = \\beta'_0 + \\beta_1 \\log(x_i) + e_i \\]\n회귀식에 대한 모형이 역지수함수 모형인 경우, 즉 독립변수와 종속변수가 다음과 같은 경우 \\[ y = \\beta_0 \\exp (\\beta_1/ x) \\] 종속변수에 대한 로그 변환과 독림변수에 대한 역변환을 하면 선형관계에 매우 가깝게 된다. \\[ \\log(y_i) = \\beta,_0 + \\beta_1 \\left ( \\frac{1}{x_i} \\right ) + e_i \\] 여기서 주의할 점은 원래의 역지수모형에 오차항의 지수함수가 곱해지는 형태가 되어야 로그 변환후에 등분산성의 가정을 만족하게 된다.\n\n\n\n\n\n역지수모형과 로그/역변환\n\n\n\n\n\n\n4.2.2 쌍곡선과 역변환\n생물학, 경제학등의 문야에서 독립변수와 종속변수의 관계가 쌍곡선(Hyperbola) 형태인 경우가 많다. 독립변수가 증가함에 따라 종속변수의 값이 수렴하는 경우에 이러한 관계가 매우 유용하다. \\[ y = \\frac{x}{\\beta_0 + \\beta_1 x} \\] 이러한 경우에 독립변수와 종속변수에 모두 역변환(Inverse transformation)을 취하면 선형관계에 매우 가깝게 된다 (아래 그림 참조). \\[ \\frac{1}{y_i} = \\beta_0 + \\beta_1 \\left ( \\frac{1}{x_i} \\right ) + e_i \\]\n\n\n\n\n\n쌍곡선과 역변환\n\n\n\n\n위에서 살펴본 회귀모형들에서 변수변환후에 등분산성에 대한 가정을 만족하려면 대부분 변환전의 함수관계식에 오차항의 지수함수가 곱해지는 형태가 되어야 한다 (multiplicative error model). 이렇게 오차항이 함수관계식에 곱해지는 형태가 아니 다른 형태라면, 예를 들어 오차항이 원래의 함수관계식에 더해지는 형태 (additive error model), 등분산성의 가정이 상당히 위배될 수 있음을 주의해야 한다. 예를 들어 회귀식에 대한 모형이 지수함수 모형인 경우 서로 독립이고 평균이 0, 분산이 \\(\\sigma^2\\)인 오차항 \\(e_i\\)를 함수 관계식에 더해졌다면 로그변환된 종속 변수와 독립변수는 선형관계를 보이지만 등분산성 가정은 만족하지 못하게된다.\n\\[ y_i = \\beta_0 \\exp (\\beta_1 x_i) +e_i \\quad \\Rightarrow \\quad \\log(y_i) \\cong \\beta'_0 + \\beta_1 x_i + e^*_i \\]\n\n\n\n\n\nAdditive error model 과 로그변환\n\n\n\n\n\n\n4.2.3 Box-Cox 변환\n앞 절에서 보았듯이 종속변수에 로그변환 등을 적용하면 여러 가지 유용한 점이 많다. 위에서 살펴본 종속변수에 변환은 여러 가지 비선형모형을 선형모형에 가깝게 만들어 주며 Multiplicative error model과 같이 반응값이 분산이 독립변수의 크기에 영향을 받는 모형을 등분산성을 가진 형태의 모형으로 버꾸어 준다 (Variance stabilization). 이렇게 종속변수에 대한 여러 가지 변환을 하나의 체계적인 형태로 결합한것을 Box-Cox 변환으로 부르며 다음과 같아 정의한다.\n\\[\ny^{(\\lambda)} =\n\\begin{cases}\n\\log(y) & \\text{ if } \\lambda=0 \\\\\n\\frac{y^\\lambda -1}{\\lambda} & \\text{ if } \\lambda \\ne 0\n\\end{cases}\n\\tag{4.2}\\]\nBox-Cox 변환은 로그변환과 멱변환을 모두 포함하고 있다. 또한 Box-Cox 변환된 \\(y^{(\\lambda)}\\)가 정규분포를 따른다는 가정 하에 자료가 주어졌을때 \\(\\lambda\\)에 대한 최대우도추정량을 구할 수 있다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>모형의 진단</span>"
    ]
  },
  {
    "objectID": "qmd/modeleval2.html#다중공선성",
    "href": "qmd/modeleval2.html#다중공선성",
    "title": "4  모형의 진단",
    "section": "4.3 다중공선성",
    "text": "4.3 다중공선성\n회귀분석에서 독립변수들간의 강한 선형 관계의 경향이 있을 때 이를 다중공선성(multicollinearity)라고 한다. 즉, \\(p\\)개의 독립변수 \\(x_1,x_2,\\dots,x_p\\)의 관계가 다음과 같은 선형관계에 가깝다면 다중공선성이 존재한다고 한다.\n\\[ c_1 x_1 + c_2 x_2 + \\dots + c_p x_p \\cong 0 \\]\n다중공선성에 의해 발생하는 여러 가지 문제점들을 기술적으로 독립변수들의 강한 선형관계때문에 행렬 \\(X^tX\\)가 ill-conditioned 행렬이 되어 그 역행렬이 불안정하게 구해지는 결과 때문에 생기게 된다. 여기서 회귀계수의 공분산 행렬은 다음과 같이 주어짐에 유의하자.\n\\[ Var(\\pmb b) = \\sigma^2 (X^t X)^{-1} \\]\n예를 들어서 두 개의 독립변수가 있는 회귀 모형을 생각해 보자.\n\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e \\]\n그림 4.1 에서 (a)의 경우는 두 개의 독립변수 \\(x_1\\)과 \\(x_2\\)의 관계가 독립적이어서 (linearly independent) 적합된 회귀식이(그림에서 2차원 평면) 안정적이다. 반면에 그림 4.1 에서 (b)의 경우는 두 개의 독립변수 \\(x_1\\)과 \\(x_2\\)가 완벽한 선형관계가 있기 떄문에 (linearly dependent)\n\\[ c_1 x_1 + c_2 x_2 = 0 \\] 적합된 회귀식이 여러가지 존재한다. 이러한 경우는 매우 드물지만 그림 4.1 에서 (c)의 경우는 두 개의 독립변수 \\(x_1\\)과 \\(x_2\\)가 선형관계에 매우 가깝기 때문에 적합된 회귀식이 불안정하다.\n\n\n\n\n\n\n\n\n그림 4.1: 회귀분석에서의 다중공선성의 정도(Fox 2008, p310)\n\n\n\n\n\n회귀분석에서 다중공선성의 정도를 측정할 수 있는 통계량은 다음과 같은 것들이 있다.\n\n4.3.1 독립변수간의 상관계수\n독립변수간의 상관계수를 보아 강한 상관관계를 가지는 변수들이 있다면 다중공선성의 가능성이 크다.\n\n\n4.3.2 \\(\\pmb X^t \\pmb X\\) 의 고유값(Eigenvalues)\n행렬 \\(\\pmb X^t \\pmb X\\)의 고유값를 구하여 큰 순서대로 나열했을 때 가장 작은 값이 0에 매우 가까우면 다중공선성의 가능성이 크다. 만약에 독립변수들간의 선형 관계가 있다면 행렬 \\(\\pmb X^t \\pmb X\\)은 최대 계수(full rank) 행렬이 아니므로 하나 이상의 고유값이 0이 되게 된다.\n이제 \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p\\)를 \\(\\pmb X^t \\pmb X\\)의 고유값라고 하자. 이런 가정 하에서 \\(1/\\lambda_i\\)는 \\((\\pmb X^t \\pmb X)^{-1}\\)의 고유값이다. \\(\\pmb X^t \\pmb X\\)의 고유값에 대한 고유벡터를 \\(\\pmb p_1, \\pmb p_2,\\dots,\\pmb p_p\\)라고 하고 행렬 \\(\\pmb P=[\\pmb p_1, \\pmb p_2,\\dots,\\pmb p_p]\\)로 정의하자. 이때 다음과 같이 스펙트렇 분해를 이용하여 \\(\\pmb X^t \\pmb X\\)를 나타낼 수 있다.\n\\[ \\pmb P^t (\\pmb X^t \\pmb X) \\pmb P = \\pmb \\Lambda = \\text{diag}(\\lambda_1 , \\lambda_2 , \\dots , \\lambda_p) \\] 또한\n\\[ (\\pmb X^t \\pmb X)^{-1} =  \\pmb P \\pmb \\Lambda^{-1}\\pmb P^t  \\] 따라서 가장 작은 고유값 \\(\\lambda_p\\)가 매우 0에 가까우면 \\((\\pmb X^t \\pmb X) \\pmb p_p =\\lambda_p \\pmb p_p \\approx 0\\)이 성립하고 이는 \\(\\pmb X \\pmb p_p \\approx 0\\)을 의미하여 독립변수간에 선형관계가 있다는 것을 암시한다.\n일반적으로 최소제곱추정량의 공분산은 다음과 같아 나타내어지고\n\\[ Var(\\hat {\\pmb \\beta}) = \\sigma^2 (\\pmb X^t \\pmb X)^{-1} = \\sigma^2 \\pmb P \\pmb \\Lambda^{-1}\\pmb P^t  =\n\\sigma^2  \\sum_{i=1}^p \\frac{1}{\\lambda_i} \\pmb p_i \\pmb p^t_i \\]\n최소제곱추정량의 분산의 합(total variance)은 다음과 같다.\n\\[ \\sum_{i=1}^p Var(\\hat \\beta_i) = tr[\\sigma^2 (\\pmb X^t \\pmb X)^{-1}]= \\sigma^2 tr[ (\\pmb X^t \\pmb X)^{-1}]=\n\\sigma^2 \\sum_{i=1}^p \\frac{1}{\\lambda_i} \\]\n따라서 고유값의 값이 0에 가까와 지면 추정량의 분산의 합은 매우 크게 된다.\n\n\n4.3.3 조건지수 (condition number)\n다중공선성의 판별을 위하여 행렬 \\(\\pmb X^t \\pmb X\\)의 고유값이 중요한 측도라고 했다. 고유값을 상대적으로 비교하면 다중공선성을 더 명확하게 알 수 있다. 조건지수는 가장 튼 고유값과 다른 고유값의 비율의 제곱근으로 나타내어진다.\n\\[ \\kappa_i = \\sqrt{\\frac {\\lambda_1}{\\lambda_i}}, \\quad i=2,3,\\cdots,p \\]\n중요한 역활을 하는 조건지수는 가장 작은 고유값에 의한 것이다.\n\\[ \\kappa_p =\\kappa(\\pmb X) = \\sqrt{\\frac {\\lambda_1}{\\lambda_p}} \\]\n\\(\\kappa(\\pmb X)\\)의 값이 크면 클수록 다중공선성의 가능성이 높다. 일반적으로 \\(\\kappa(\\pmb X)\\) 가 30 이상이면 다중공선성의 가능성이 크다고 본다.\n\n\n4.3.4 분산팽창계수 (Variance Inflation Factor ;VIF)\n하나의 독립변수 \\(x_i\\)를 나머지 다른 \\(p-1\\)개의 독립변수 \\(x_1,\\dots, x_{i-1}, x_{i+1},\\dots, x_p\\)를 이용하여 회귀식을 적합시킬 수 있다. 이때 다중공선성이 존재한다면 적합된 회귀식의 결정계수 \\(R^2_i\\)는 1에 매우 가까울 것이다\n\\[ x_i = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_{i-1} x_{i-1} +\\beta_{i+1} x_{i+1} + \\dots + \\beta_p x_p +e \\]\n이때 독립변수 \\(x_i\\)에 대한 VIF는 다음과 같이 정의되며 그 값이 5 또는 10보다 크면 다중공선성의 가능성이 크다.\n\\[ VIF_i = \\frac{1}{1-R_i^2} \\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>모형의 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html",
    "href": "qmd/residual.html",
    "title": "5  관측값에 대한 진단",
    "section": "",
    "text": "5.1 서론\n회귀분석을 포함한 통계적 자료분석에서 흔하게 접하는 문제는 자료 중의 일부가 통계적 모형에 의해 추정된 평균적인 경향에서 매우 벗어나 있는 점을 발견하게 되는 경우이다.\n이러한 경우 평균적인 경향에서 매우 벗어난 자료를 분석에서 제외시킬 것인지에 대한 논의도 필요할 수 있으며 이러한 자료들이 모형의 모수에 대한 추정에 어떤 영향을 미칠 것인자에 대한 검토도 필요할 수 있다.\n평균적인 경향에서 매우 벗어난 자료를 흔히 이상점(outlier)라고 부른다. 회귀분석에서 이러한 이상점은 회귀계수 추정과 그에 따른 여러 가지 통계적 추론에 많은 영향을 미친다. 따라서 이상점들이 회귀분석의 계수 추정에 어떤 영향을 얼만큼 끼치는가에 대한 검토는 매우 중요하다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#이상점의-유형",
    "href": "qmd/residual.html#이상점의-유형",
    "title": "5  관측값에 대한 진단",
    "section": "5.2 이상점의 유형",
    "text": "5.2 이상점의 유형\n일차원 자료에서는 평균적인 경향에서 매우 벗어난 자료의 식별이 단순하고 쉽다. 예를 들어 다음과 같이 일변량 자료 \\(\\pmb x\\)만 고려하면 이상점이 어떤 점인지는 쉽게 찾을 수 있다.\n\\[ \\pmb x^t = (1,2,3,4,5,10) \\]\n그러나 반응변수와 독립변수들을 고려해야 하는 회귀분석에서 이상점은 간단하게 파악하기 힘들고 상황에 따라 그 의미가 매우 다르다.\n회귀분석에서 이상점의 다양한 종류와 그 영향을 알아보기 위하여 단순회귀분석을 고려하고 다음 그림을 보자.\n\n\n\n\n\n여러가지 종류의 이상점과 그 영향\n\n\n\n\n위의 그림에서 실선은 검정색 점을 포함해서 적합한 회귀직선이고 점선은 검정색 점을 제외했을 때의 회귀직선이다\n그림의 (a)에서 검정색 점은 설명변수 \\(x\\)에 대해서는 이상점이 아니지만 \\(x\\)가 주어진 경우 반응변수 \\(y\\)에 대해서는 평균적인 경향에서 많이 벗어나 있기 때문에 이상점이다. 이러한 이상점을 회귀이상점(regression outlier)라고 부르기도 한다. (a)의 회귀이상점의 유무는 회귀계수의 추정에 크게 영향을 주지 않는다. 이렇게 어떤 관측점이 있고 없음에 따라 회귀계수의 값이 크게 변하지 않는다면 그 자료의 영향력(leverage)이 작다고 한다.\n(b)에서의 검은 점은 설명변수 \\(x\\)에 대하여 이상점이며 또한 회귀이상점이다. 더 나아가 이 이상점을 제외하고 적합한 회귀계수는 이상점을 포함했을 때 적합한 회귀계수와 매우 다르다. 이 경우 이 이상점은 큰 영향력을 가졌다고 말한다.\n(c)에서의 검은 점은 설명변수 \\(x\\)에 대해서 이상점이지만 회귀이상점은 아니다. 이러한 경우 이상점의 유무에 따라 회귀계수의 값이 크게 변하지 않으므로 이상점은 작은 영향력을 가졌다고 말한다. 하지만 (c)에서의 검은 점이 설명변수 \\(x\\)의 중심점으로부터 크게 멀어져있으므로 \\(y\\)의 값이 조그만 변해도 그림 (d)와 같이 큰 영향력을 가진다.\n관측치 \\(y_i\\) 를 제외할 때와 포함할 때의 회귀계수 추정치가 매우 다르면 그 관측치를 영향점(influential point)라고 하며 그 영향의 크기는 그 점이 가진 영향력(leverage)의 크기와 평균에서 떨어진 정도에 비례한다.\n\n자료가 선형모형의 계수 추정에 미치는 영향 \\(\\propto\\) 영향력의 크기 \\(\\times\\) 이상치의 특이한 정도\n\n위에서 살펴보았듯이 회귀분석에서 이상점의 종류와 그 영향은 매우 다양하며 복잡하다. 이러한 이상점의 종류와 회귀계수의 영향에 대하여 분석할 때 유용하게 쓰이는 통계량이 잔차(residual)이다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#지렛값",
    "href": "qmd/residual.html#지렛값",
    "title": "5  관측값에 대한 진단",
    "section": "5.3 지렛값",
    "text": "5.3 지렛값\n\\(y\\)가 반응변수이고 \\(p-1\\)개의 설명변수 \\(x_1,x_2,\\dots,x_{p-1}\\)가 있을 때 회귀식은 다음과 같이 표현된다.\n\\[ \\pmb y = \\pmb X \\pmb \\beta + \\pmb e \\]\n회귀계수 \\(\\pmb \\beta\\)의 최소제곱 추정치 \\(\\hat{ \\pmb \\beta}\\)는 다음과 같이 주어지며\n\\[ \\hat{ \\pmb \\beta} = (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y \\]\n관측값 \\(\\pmb y\\) 의 추정치 \\(\\hat {\\pmb y}\\)는 다음과 같다.\n\\[  \\hat {\\pmb y} = \\pmb X \\hat{ \\pmb \\beta} =  \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y \\equiv \\pmb H \\pmb y \\]\n여기서 \\(\\pmb H= \\pmb X (\\pmb X^t \\pmb X)^{-1} \\pmb X^t\\)를 사영행렬(hat matrix 또는 projection matrix)라고 부르며 사영행렬 \\(\\pmb H\\) 의 \\(i\\) 번째 대각원소를 \\(h_{ii}\\)라고 하며 이는 이상치 또는 영향치 분석에 중요한 역할을 한다.\n\\(i\\)번째 관측치의 설명변수 벡터를 다음과 같이 표시하면\n\\[\\pmb x_{i}^t=(1, x_{i1},x_{i2},\\dots, x_{i,p-1}) \\]\n\\(\\pmb H\\) 의 \\(i\\) 번째 대각원소를 \\(h_{ii}\\)는 다음과 같이 표현된다.\n\\[\nh_{ii} = \\pmb x_{i}^t (\\pmb X^t \\pmb X)^{-1}  \\pmb x_{i}\n\\tag{5.1}\\]\n\\(h_{ii}\\)는 \\(i\\) 번째 관측치의 설명변수 \\((x_{i1},x_{i2},\\dots, x_{i,p-1})\\) 들이 모든 관측치의 평균 \\((\\bar x_1, \\bar x_2,\\dots, \\bar x_{p-1})\\) 에서 얼마나 멀리 떨어져 있는가에 대한 상대적인 양을 나타낸다. 따라서 \\(h_{ii}\\) 를 지렛점(leverage point)라고 부른다.\n지렛점 값이 클수로 영향점일 가능성이 크며 큰 값을 높은 지렛값(high leverage point)라고 부른다.\n보통 \\(h_{ii}\\)값이 \\(p/n\\)보다 크면 영향력이 크다고 말한다. 참고로 단순 회귀식에서 \\(h_{ii}\\)는 다음과 같이 주어진다.\n\\[ h_{ii} = \\frac{1}{n} + \\frac{ ( x_i-\\bar x)^2 }{\\sum (x_i-\\bar x)^2} \\]\n또한 \\(h_{ii}\\)값을 모두 더하면 설명변수의 개수와 같다. \\[ tr(\\pmb H)=\\sum_{i=1}^k h_{ii} = p \\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#내-표준화-잔차",
    "href": "qmd/residual.html#내-표준화-잔차",
    "title": "5  관측값에 대한 진단",
    "section": "5.4 내 표준화 잔차",
    "text": "5.4 내 표준화 잔차\n잔차 \\(r_i\\)는 \\(y\\) 의 실제 관측값과 그 추정치의 차이이며\n\\[\nr_i = y_i -\\hat y_i  = y_i - {\\pmb x}_i^t \\hat {\\pmb \\beta}\n\\tag{5.2}\\]\n잔차벡터에 대한 식은 다음과 같다.\n\\[\n\\pmb r = \\pmb y - \\hat {\\pmb y} = \\pmb y - \\pmb H \\pmb y = (\\pmb I-\\pmb H)\\pmb y\n\\tag{5.3}\\]\n잔차의 공분산 행렬을 살펴보면 다음과 같이 주어진다. 따라서 \\(i\\) 번째 잔차의 분산은 \\(Var(r_i) = (1-h_{ii})\\sigma^2\\) 이다.\n\\[\nVar(\\pmb r) = \\sigma^2 (\\pmb I-\\pmb H)\n\\tag{5.4}\\]\n위에서 언급한 잔차 \\(r_i\\) 를 보통 잔차(ordinary residual)이라고 하며 그 크기가 단위에 따라 바뀌므로 잔차분석에서는 표준화 잔차(standardized residual)을 더 많이 사용한다.\n아래와 같이 잔차를 그 표준편차로 나눈 값을 내 표준화 잔차(internally studentized residual) 이라고 부른다.\n\\[\nr^s_i = \\frac{r_i}{s \\sqrt{1-h_{ii}}}\n\\tag{5.5}\\]\n위의 식에서 \\(s\\)는 오차항의 표준편차 \\(\\sigma\\)의 추정량이며 \\(h_{ii}\\)는 사영행렬 \\(\\pmb H\\) 의 \\(i\\) 번째 대각원소(즉 지렛값)이다.\n잔차분석에서는 척도(scale)에 영향이 없는 표준화 잔차를 이용하는 것이 좋다. 그 값이 클수로 이상치일 가능성이 크다. 보통 내표준화 잔차의 절대값이 2보다 크면 이상치일 가능성이 크다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#관측값의-영향-계수-추정",
    "href": "qmd/residual.html#관측값의-영향-계수-추정",
    "title": "5  관측값에 대한 진단",
    "section": "5.5 관측값의 영향: 계수 추정",
    "text": "5.5 관측값의 영향: 계수 추정\n회귀분석에서 하나의 관측치가 회귀계수의 추정에 영향을 미치는 정도를 알아볼 때 유용한 방법은 그 관측치를 제외했을 때의 최소제곱추정량과 포함했을 때의 추정량을 비교하는 것이다.\n\\(i\\)번째 관측치에 대한 반응값과 설명변수들이 다음과 같은 때\n\\[ y_i, \\quad \\pmb x_{i}^t=(1, x_{i1},x_{i2},\\dots, x_{i,p-1}) \\]\n\\(i\\)번째 관측치를 제외한 자료에서 반응변수와 설명변수의 벡터식을 다음과 같이 표시한다.\n\\[ \\pmb y_{-i}, \\quad \\pmb X_{-i} \\]\n\\(i\\)번째 관측치를 제외했을 때 회귀계수의 최소제곱추정량을 \\(\\hat{ \\pmb \\beta}_{-i}\\)라 하면 모든 관측치를 이용한 최소제곱추정량을 \\(\\hat{ \\pmb \\beta}\\)와의 관계는 다음과 같이 나타낼 수 있다. 아래 식 세번째 중의 결과는 우드베리 공식 식 A.3 을 이용하였다.\n\\[\n\\begin{aligned}\n\\hat{ \\pmb \\beta}_{-i} & =  (\\pmb X_{-i}^t \\pmb X_{-i})^{-1} \\pmb X_{-i}^t \\pmb y_{-i}\\\\\n& = (\\pmb X^t \\pmb X - \\pmb x_{i} \\pmb x_{i}^t)^{-1} (\\pmb X^t \\pmb y -  \\pmb x_{i} y_{i}) \\\\\n& = \\left [ (\\pmb X^t \\pmb X)^{-1} - \\frac { (\\pmb X^t \\pmb X)^{-1}  \\pmb x_{i} \\pmb x_{i}^t\n  (\\pmb X^t \\pmb X)^{-1}  }{ 1- \\pmb x_{i}^t (\\pmb X^t \\pmb X)^{-1}  \\pmb x_{i} } \\right ]\n(\\pmb X^t \\pmb y -  \\pmb x_{i} y_{i})  \\\\\n& = \\hat{ \\pmb \\beta} + \\frac{1}{1-h_{ii}} (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i \\left [ \\pmb x_i^t \\hat{ \\pmb \\beta} - (1-h_{ii}) y_i - h_{ii} y_i \\right ] \\\\\n& =  \\hat{ \\pmb \\beta} - \\frac{1}{1-h_{ii}} (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i ( y_i - \\pmb x_i^t \\hat{ \\pmb \\beta}) \\\\\n& =  \\hat{ \\pmb \\beta} - \\frac{r_i}{1-h_{ii}} (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i\n\\end{aligned}\n\\tag{5.6}\\]\n또한 \\(i\\)번째 관측치를 제외했을 때 오차항 분산의 추정량을 \\(s^2_{-i}\\)로 나타낸다.\n\\[\ns^2_{-i} = \\frac{1}{n-p-1} \\sum_{j \\ne i} (y_j - {\\pmb x}_j^t \\hat{ \\pmb \\beta}_{-i} )^2\n\\tag{5.7}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#외-표준화-잔차와-press-잔차",
    "href": "qmd/residual.html#외-표준화-잔차와-press-잔차",
    "title": "5  관측값에 대한 진단",
    "section": "5.6 외 표준화 잔차와 PRESS 잔차",
    "text": "5.6 외 표준화 잔차와 PRESS 잔차\n잔차를 표준화 할 떄 \\(i\\)번째 관측치를 제외했을 때 분산의 추정량을 \\(s^2_{-i}\\)을 이용하는 것이 합리적이다. 이는 반응값이 이상점인 경우 분산의 추정량이 커지게 된다. 식 식 5.5 에서 정의된 내 표준화 잔차에서는 이상점이 분산의 추정량에 영향을 주어 잔차의 크기가 작아지게 된다. 따라서 내 표본화 잔차는 이상점을 구별할 수 있는 능력이 떨어진다. 이러한 점을 보완하기 위하여 이상점의 영향을 약화시킬 수 있도록 \\(s^2_{-i}\\)를 이용하여 표준화 한 양이 아래와 같이 정의된 표준화 잔차이다.\n\\[\nr^*_i = \\frac{r_i}{s_{-i} \\sqrt{1-h_{ii}}}\n\\tag{5.8}\\]\n식 식 5.8 에서 정의된 차를 표준화 잔차(studentized residual) 또는 외 표준화 잔차(externally studentized residual)라고 부른다.\n외 표준화 잔차는 \\(i\\)번째 관측치가 회귀식 적합에 미치는 영향을 내 표분화 잔차보다 더 민감하게 탐색할 수 있다. 보통 외 표준화 잔차의 절대값이 2보다 크면 이상치일 가능성이 크다.\nPRESS 잔차 \\(r_{i,-i}\\)는 \\(i\\) 번쨰 관측값을 빼고 적합한 회귀식으로 부터 얻은 \\(E(y| \\pmb x_i)\\)의 추정치 \\(\\hat y_{i,-i}\\)를 이용하여 만든 잔차이다. PRESS 잔차는 다음과 같이 정의된다.\n\\[\nr_{i,-i}  =   y_i - \\hat y_{i,-i} = y_i - \\pmb x^t_i \\hat{ \\pmb \\beta}_{-i}\n\\tag{5.9}\\]\n실제 PRESS 잔차를 구할 경우 관측값을 제외하지 않고도 원래의 회귀식을 이용하여 아래와 같이 쉽게 구할 수 있다. 그 값이 클수로 이상치 또는 영향점일 가능성이 크다.\n\\[\n\\begin{aligned}\nr_{i,-i} & =   y_i - \\hat y_{i,-i} \\\\\n& =y_i - \\pmb x^t_i \\hat{ \\pmb \\beta}_{-i}  \\\\\n& = y_i - \\pmb x^t_i  \\left [ \\hat{ \\pmb \\beta} - \\frac{1}{1-h_{ii}} (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i r_i \\right ] \\\\\n&= (y_i - \\pmb x^t_i \\hat{ \\pmb \\beta}) +   r_i  \\frac{\\pmb x^t_i (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i}{1-h_{ii}} \\\\\n&= \\frac{r_i}{1-h_{ii}}\n\\end{aligned}\n\\tag{5.10}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#관측값의-영향-분산-추정",
    "href": "qmd/residual.html#관측값의-영향-분산-추정",
    "title": "5  관측값에 대한 진단",
    "section": "5.7 관측값의 영향: 분산 추정",
    "text": "5.7 관측값의 영향: 분산 추정\n참고로 식 식 5.7 에서 정의된 \\(s^2_{-i}\\) 과 \\(s^2 = SSE/(n-p)\\)의 관계를 살펴보자. 먼저 \\(SSE\\)의 정의와 식 식 5.6 과 식 5.10 를 이용하여 다음과 같은 분해가 가능하다.\n\\[\n\\pmb y - {\\pmb X} \\hat{ \\pmb \\beta}_{-i}  = ( \\pmb y - {\\pmb X} \\hat{ \\pmb \\beta}) +\n\\frac{r_i}{1-h_{ii}} \\pmb X (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i\n\\]\n따라서\n\\[\n\\begin{aligned}\n& \\sum_{j \\ne i} (y_j - {\\pmb x}_j^t \\hat{ \\pmb \\beta}_{-i} )^2   + (y_i - \\hat {y}_{i,-i} )^2 \\\\\n& = \\sum_{j \\ne i} (y_j - {\\pmb x}_j^t \\hat{ \\pmb \\beta}_{-i} )^2   + (y_i - {\\pmb x}_i^t \\hat{ \\pmb \\beta}_{-i} )^2  \\\\\n  & = ( \\pmb y - \\pmb X \\hat {\\pmb \\beta}_{-i})^t ( \\pmb y - \\pmb X \\hat {\\pmb \\beta}_{-i}) \\\\\n  & = ( \\pmb y - \\pmb X \\hat {\\pmb \\beta})^t ( \\pmb y - \\pmb X \\hat {\\pmb \\beta})\n   -2 \\frac{r_i}{1-h_{ii}} ( \\pmb y - \\pmb X \\hat {\\pmb \\beta})^t  \\pmb X (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i  \\\\\n   & \\quad + \\frac{r^2_i}{(1-h_{ii})^2} \\pmb x_i^t  (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb X (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i  \\\\\n   & = ( \\pmb y - \\pmb X \\hat {\\pmb \\beta})^t ( \\pmb y - \\pmb X \\hat {\\pmb \\beta})\n   -2 \\frac{r_i}{1-h_{ii}}  \\pmb y^t(\\pmb I - \\pmb H)  \\pmb X (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i  \\\\\n   & \\quad  + \\frac{r^2_i}{(1-h_{ii})^2} \\pmb x_i^t (\\pmb X^t \\pmb X)^{-1}  \\pmb x_i  \\\\\n    & = SSE + 0 + \\frac{r^2_i}{(1-h_{ii})^2} h_{ii}  \\\\\n    & = SSE  + \\frac{r^2_i h_{ii}}{(1-h_{ii})^2}   \n\\end{aligned}\n\\tag{5.11}\\]\n이제 위의 식의 결과와 식 식 5.10 를 이용하면 다음과 같은 결과를 얻는다.\n\\[\n\\begin{aligned}\n\\sum_{j \\ne i} (y_j - {\\pmb x}_j^t \\hat{ \\pmb \\beta}_{-i} )^2   \n& =  SSE  + \\frac{r^2_i h_{ii}}{(1-h_{ii})^2}   -  (y_i - \\hat {y}_{i,-i} )^2 \\\\\n& = SSE  + \\frac{r^2_i h_{ii}}{(1-h_{ii})^2}   -  (y_i - \\hat {y}_{i,-i} )^2 \\\\\n  & = SSE  + \\frac{r^2_i h_{ii}}{(1-h_{ii})^2}   -   \\frac{r^2_i}{(1-h_{ii})^2} \\\\\n  & = SSE  - \\frac{r^2_i }{1-h_{ii}}    \n\\end{aligned}\n\\tag{5.12}\\]\n따라서 다음 식을 이용하면 \\(s^2_{-i}\\)은 모든 관측값을 이용한 \\(s^2\\)으로부터 쉽게 유도할 수 있다.\n\\[\n(n-p-1) s^2_{-i} = (n-p)s^2 + - \\frac{r^2_i }{1-h_{ii}}    \n\\tag{5.13}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "qmd/residual.html#영향력의-측도",
    "href": "qmd/residual.html#영향력의-측도",
    "title": "5  관측값에 대한 진단",
    "section": "5.8 영향력의 측도",
    "text": "5.8 영향력의 측도\n하나의 관측값이 있는 경우 회귀계수 추정치와 없는 경우의 추정치의 차이가 크면 그 관측값이 큰 영향력을 가진다. 이러한 영향력을 측정할 수 있는 측조에 대하여 알아보자.\n쿡의 거리(COOK’s distance) \\(C_i\\)는 \\(i\\)번째 관측치가 회귀식 적합의 계수에 미치는 영향을 나타내는 양으로서 다음과 같이 정의된다.\n\\[\nC_i = \\frac{  (\\hat{ \\pmb \\beta} -\\hat{ \\pmb \\beta}_{-i})^t [ \\widehat {Cov}(\\hat {\\pmb \\beta}]^{-1}\n(\\hat{ \\pmb \\beta} -\\hat{ \\pmb \\beta}_{-i}) } {p} = \\frac{  (\\hat{ \\pmb \\beta} -\\hat{ \\pmb \\beta}_{-i})^t (\\pmb X^t \\pmb X) (\\hat{ \\pmb \\beta} -\\hat{ \\pmb \\beta}_{-i}) } {p s^2}\n\\tag{5.14}\\]\n여기서 \\(\\hat{ \\pmb \\beta}_{-i}\\)는 \\(i\\)번째 관측치를 제외하고 적합한 회귀식에 의한 회귀계수이며 \\(p\\)는 설명변수의 개수이다. 그 값이 클수로 영향점일 가능성이 크다.\n쿡의 거리 \\(C_i\\)과 내 표준화 잔차와의 관계는 다음과 같다.\n\\[ C_i = \\frac{ (r^s_i)^2}{p} \\left ( \\frac{h_{ii} }{1-h_{ii}} \\right ) \\]\nDFFITS는 \\(n\\)개의 모든 자료를 이용했을 때의 \\(i\\) 번째 관측값의 평균 \\(E(y|\\pmb x_i)\\)의 추정치 \\(\\hat y_i\\)와 \\(i\\) 번쨰 관측값을 빼고 적합한 회귀식에 의한 추정치 \\(\\hat y_{i,-i}\\)의 표준화된 차이을 말한다.\n즉, \\(\\hat y_{i,-i}\\)를 \\(i\\)번째 관측치를 제외하고 적합한 회귀식에 의한 예측치라고 한다면 두 예측치의 차이 \\(\\hat y_i - \\hat y_{i,-i}\\)를 표준화시키면 다음과 같다.\n\\[\nDFFITS_i = \\frac{\\hat y_i - \\hat y_{i,-i}}{s_{-i}\\sqrt{h_{ii}}}  \n\\tag{5.15}\\]\nDFFITS 는 그 값이 클수로 영향점일 가능성이 크다.\n여기서 식 식 5.6 를 이용하면 다음 식를 얻고\n\\[ {\\pmb x}_i^t \\hat{ \\pmb \\beta}_{-i} =\\pmb x_i^t \\hat{ \\pmb \\beta} -\\frac{r_i h_{ii}}{1-h_{ii}}\\]\nDFFITS과 잔차와의 관계를 알 수 있다.\n\\[\n\\begin{aligned}\nDFFITS_i & = \\frac{\\hat y_i - \\hat y_{i,-i}}{s_{-i}\\sqrt{h_{ii}}}  \\\\\n& =  \\frac{ [h_{ii}/(1-h_{ii})]r_i } {s_{-i}\\sqrt{h_{ii}}} \\\\\n& =  \\frac{ r_i } {s_{-i}\\sqrt{1-h_{ii}}} [h_{ii}/(1-h_{ii})]^{1/2} \\\\\n&= r^*_i \\left [\\frac{h_{ii}}{1-h_{ii}} \\right ]^{1/2}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>관측값에 대한 진단</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "강근석, and 유형조. 2016. R을 활용한 선형회귀분석.\n1st ed. 교우사. https://github.com/regbook/regbook.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html",
    "href": "qmd/math_mat_basic.html",
    "title": "부록 A — 행렬의 기초",
    "section": "",
    "text": "A.1 벡터와 행렬\n다음 \\(p\\)-차원 벡터(vector) 또는 열벡터(column vector) \\(\\pmb a\\) 는 \\(p\\)개의 원소 \\(a_1, a_2, \\dots, a_p\\) 를 하나의 열(column)에 배치한 형태를 가진 개체이다.\n\\[\n\\pmb a =\n\\begin{bmatrix}\na_1 \\\\\na_2 \\\\\n\\vdots \\\\\na_p\n\\end{bmatrix}\n\\tag{A.1}\\]\n차원이 \\(n \\times p\\) 인 행렬 \\(\\pmb A\\) 는 다음과 같이 \\(n\\)개의 행과 \\(p\\) 개의 열에 원소 \\(a_{ij}\\)를 다음과 같이 배치한 형태를 가진다.\n\\[\n\\pmb A =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{n1} & a_{n2} & \\dots & a_{np}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#두-행렬의-덧셈",
    "href": "qmd/math_mat_basic.html#두-행렬의-덧셈",
    "title": "부록 A — 행렬의 기초",
    "section": "A.2 두 행렬의 덧셈",
    "text": "A.2 두 행렬의 덧셈\n두 행렬 \\(\\pmb A\\) 와 \\(\\pmb B\\) 를 더하는 규칙은 다음과 같다.\n\n두 행렬 \\(\\pmb A\\) 와 \\(\\pmb B\\) 는 행과 열의 갯수가 같아야 한다.\n\\(\\pmb A + \\pmb B = \\pmb C\\) 라고 하면, 덧셈의 결과로 만들어진 행렬 \\(\\pmb C\\)는 두 행렬과 같은 수의 행과 열을 가지면 각 원소는 다음과 같다.\n\n\\[ \\pmb A + \\pmb B = \\pmb C \\quad \\rightarrow \\quad c_{ij} = a_{ij} + b_{ij} \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#스칼라곱",
    "href": "qmd/math_mat_basic.html#스칼라곱",
    "title": "부록 A — 행렬의 기초",
    "section": "A.3 스칼라곱",
    "text": "A.3 스칼라곱\n임의의 실수 \\(\\lambda\\) (스칼라)가 주어졌을 때, \\(\\lambda\\) 와 행렬 \\(\\pmb A\\)의 스칼라곱(scalar product) 는 행렬의 모든 원소에 \\(\\lambda\\) 를 곱해준 행렬로 정의된다.\n예를 들어 \\(\\lambda=2\\), \\(\\pmb A \\in \\RR^{2\\times 3}\\) 인 경우\n\\[\n\\lambda \\pmb A =\n2\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n-1 & 0 & 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 & 4 & 6 \\\\\n-2 & 0 & 4\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#벡터와-행렬의-곱셈",
    "href": "qmd/math_mat_basic.html#벡터와-행렬의-곱셈",
    "title": "부록 A — 행렬의 기초",
    "section": "A.4 벡터와 행렬의 곱셈",
    "text": "A.4 벡터와 행렬의 곱셈\n\\(n \\times p\\) 인 행렬 \\(\\pmb A\\) 와 \\(p\\)-차원 벡터(vector) \\(\\pmb b\\)는 다음과 같이 두 개의 서로 다른 형태로 나타낼 수 있다.\n\nA.4.1 행과 열의 내적\n먼저 행렬과 벡터의 곱셈은 행렬 \\(\\pmb A\\) 의 행벡터와 벡터 \\(\\pmb b\\) 의 내적(inner product)로 나타낼 수 있다.\n\\[\n\\begin{aligned}\n{\\pmb A} {\\pmb b} & =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{n1} & a_{n2} & \\dots & a_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n{\\pmb r}^t_1 \\\\\n{\\pmb r}^t_2 \\\\\n\\vdots \\\\\n{\\pmb r}^t_n\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix}\n\\quad\n\\text{ where }\n{\\pmb r}^t_i =\n\\begin{bmatrix}\na_{i1} & a_{i2} & \\dots & a_{ip}\n\\end{bmatrix}  \\\\\n& =\n\\begin{bmatrix}\n{\\pmb r}^t_1 {\\pmb b} \\\\\n{\\pmb r}^t_2 {\\pmb b} \\\\\n\\vdots \\\\\n{\\pmb r}^t_n {\\pmb b}\n\\end{bmatrix}  \n=\n\\begin{bmatrix}\n\\sum_{j=1}^p a_{1j} b_j \\\\\n\\sum_{j=1}^p a_{2j} b_j \\\\\n\\vdots \\\\\n\\sum_{j=1}^p a_{nj} b_j\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n&lt;\\pmb r_1, \\pmb b&gt;  \\\\\n&lt;\\pmb r_2, \\pmb b&gt; \\\\\n\\vdots \\\\\n&lt;\\pmb r_n, \\pmb b&gt;\n\\end{bmatrix}\n\\end{aligned}\n\\]\n위에서 \\(&lt; \\pmb a, \\pmb b&gt;\\) 는 다음과 같은 두 벡터의 내적(inner product)을 의미한다.\n\\[ &lt; \\pmb a, \\pmb b&gt; = {\\pmb a}^t {\\pmb b} = \\sum_{i=1}^p a_i b_i \\]\n\n\nA.4.2 열벡터의 선형조합\n이제 행렬과 벡터의 곱셈을 행렬을 구성하는 열벡터들의 선형조합(linear combination)으로 나타낼 수 있다.\n\\[\n\\begin{aligned}\n{\\pmb A} {\\pmb b} & =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{n1} & a_{n2} & \\dots & a_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n{\\pmb c}_1 & {\\pmb c}_2 & \\dots & {\\pmb c}_p\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix}\n\\quad\n\\text{ where }\n{\\pmb c}_j =\n\\begin{bmatrix}\na_{1j} \\\\\na_{2j} \\\\\n\\vdots \\\\\na_{nj}\n\\end{bmatrix} \\\\\n& =\nb_1\n\\begin{bmatrix}\na_{11} \\\\\na_{21} \\\\\n\\vdots \\\\\na_{n1}\n\\end{bmatrix}\n+\nb_2\n\\begin{bmatrix}\na_{12} \\\\\na_{22} \\\\\n\\vdots \\\\\na_{n2}\n\\end{bmatrix}\n+ \\cdots +\nb_p\n\\begin{bmatrix}\na_{1p} \\\\\na_{2p} \\\\\n\\vdots \\\\\na_{np}\n\\end{bmatrix}  \\\\\n& =\nb_1 {\\pmb c}_1 + b_2 {\\pmb c}_2 + \\cdots + b_p {\\pmb c}_p \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#행렬의-전치",
    "href": "qmd/math_mat_basic.html#행렬의-전치",
    "title": "부록 A — 행렬의 기초",
    "section": "A.5 행렬의 전치",
    "text": "A.5 행렬의 전치\n\\(\\pmb A^t\\)는 행렬의 전치(transpose)를 나타낸다. 행렬의 전치는 원소의 행과 열을 바꾸어 만든 행렬이다.\n\\[\n{\\pmb A}  =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{n1} & a_{n2} & \\dots & a_{np}\n\\end{bmatrix}\n= \\{ a_{ij} \\}_{ n \\times p}\n\\quad\n\\rightarrow\n\\quad\n{\\pmb A}^t  =\n\\begin{bmatrix}\na_{11} & a_{21} & \\dots & a_{n1} \\\\\na_{12} & a_{22} & \\dots & a_{n2} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{1p} & a_{2p} & \\dots & a_{np}\n\\end{bmatrix}\n= \\{ a_{ji} \\}_{ p \\times n}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#행렬의-곱셈",
    "href": "qmd/math_mat_basic.html#행렬의-곱셈",
    "title": "부록 A — 행렬의 기초",
    "section": "A.6 행렬의 곱셈",
    "text": "A.6 행렬의 곱셈\n먼저 두 행렬 \\(\\pmb A\\) 와 \\(\\pmb B\\) 의 곱셈\n\\[ \\pmb A \\times \\pmb B \\equiv \\pmb A \\pmb B \\]\n을 정의하려면 다음과 같은 조건이 만족되어야 한다.\n\n행렬 \\(\\pmb A\\) 의 열의 갯수와 행렬 \\(\\pmb B\\) 의 행의 갯수가 같아야 한다\n\n따라서 두 행렬의 곱셈은 순서를 바꾸면 정의 자체가 안될 수 있다.\n이제 두 행렬 \\(\\pmb A \\in \\RR^{m \\times n}\\) 와 \\(\\pmb B \\in \\RR^{n \\times k}\\)의 곱셈은 다음과 같이 정의된다.\n\\[ \\pmb A \\pmb B =  \\pmb C\\]\n행렬 \\(\\pmb C\\) 는 \\(m\\) 개의 행과 \\(k\\)개의 열로 구성된 행렬이며(\\(\\pmb C \\in \\RR^{m \\times k}\\)) 각 원소 \\(c_{ij}\\)는 다음과 같이 정의된다.\n\\[  c_{ij} = \\sum_{l=1}^n a_{il} b_{lk}, \\quad i=1,2,\\dots,m; j=1,2,\\dots,k \\]\n먼저 간단한 예제로 다음과 같은 두 개의 행렬의 곱을 생각해 보자.\n\\[\n\\pmb A \\pmb B =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 1 \\\\\n-1 & 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(1)(0) + (2)(-1) & (1)(1) + (2)(2) \\\\\n(3)(0) + (4)(-1) & (3)(1) + (4)(2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-2 & 5 \\\\\n-4 & 11\n\\end{bmatrix}\n\\]\n곱하는 순서를 바꾸어 계산해 보자.\n\\[\n\\pmb B \\pmb A =\n\\begin{bmatrix}\n0 & 1 \\\\\n-1 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(0)(1) + (1)(3) & (0)(2) + (1)(4) \\\\\n(-1)(1) + (2)(3) & (-1)(2) + (2)(4)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix}\n\\]\n위 두 결과를 보면 행렬의 곱셈에서는 교환법칙이 성립하지 않음을 알 수 있다.\n이제 차원이 다른 두 행렬의 곱셈을 살펴보자.\n\\[\n\\pmb A =\n\\begin{bmatrix}\n1 & 2 & 3\\\\\n3 & 2 & 1\n\\end{bmatrix},\n\\quad\n\\pmb B =\n\\begin{bmatrix}\n0 & 2 \\\\\n1 & -1 \\\\\n0 & 1\n\\end{bmatrix}\n\\]\n두 행렬의 곱셈은 다음과 같이 계산할 수 있다.\n\\[\n\\pmb A \\pmb B =\n\\begin{bmatrix}\n1 & 2 & 3\\\\\n3 & 2 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 2 \\\\\n1 & -1 \\\\\n0 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 & 3 \\\\\n2 & 5\n\\end{bmatrix}\n\\]\n두 행렬의 곱하는 순서를 바꾸면 차원이 전혀 다른 행렬이 얻어진다.\n\\[\n\\pmb B \\pmb A =\n\\begin{bmatrix}\n0 & 2 \\\\\n1 & -1 \\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 & 3\\\\\n3 & 2 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n6 & 4 & 2 \\\\\n-2 & 0 & 2 \\\\\n3 & 2 & 1\n\\end{bmatrix}\n\\]\n행렬의 곱셈은 교환법칙이 성립하지 않는다.\n\\[  \\pmb A \\pmb B \\ne  \\pmb B \\pmb A \\tag{A.2}\\]\n\n\n\n\n\n\n주의\n\n\n\n교환법칙이 성립하지 않는다는 의미는 식 A.2 이 언제나 성립한다는 의미는 아니다. 아래와 같이 특별한 경우 교환법칙이 성립하는 경우도 있다.\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 3\n\\end{bmatrix}\n\\]\n\n\n\n행렬의 곱셈은 결합법칙과 배분법칙은 성립한다.\n\n\\[ (\\pmb A \\pmb B) \\pmb C = \\pmb A (\\pmb B \\pmb C) \\]\n\\[ (\\pmb A + \\pmb B) \\pmb C = \\pmb A \\pmb C +  \\pmb B \\pmb C \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#단위벡터와-항등행렬",
    "href": "qmd/math_mat_basic.html#단위벡터와-항등행렬",
    "title": "부록 A — 행렬의 기초",
    "section": "A.7 단위벡터와 항등행렬",
    "text": "A.7 단위벡터와 항등행렬\n\\(i\\)번째 단위벡터 \\(\\pmb e_i\\)를 정의하자. 단위벡터 \\(\\pmb e_i\\)는 \\(n\\)- 차원 벡터로서 \\(i\\)번째 원소만 1이고 나머지는 0인 벡터이다.\n\\[ \\pmb e_i =\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\  \n1 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix}\n\\]\n즉 \\(n\\)-차원 항등행렬 \\(\\pmb I\\)는 n개의 단위벡터들을 모아놓은 것이다. 단위행렬은 대각원소가 1이고 나머지는 0인 정방행렬이다.\n\\[  \\pmb I = [ \\pmb e_1 ~~ \\pmb e_2 ~~ \\dots ~~ \\pmb e_n ] \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#대각합",
    "href": "qmd/math_mat_basic.html#대각합",
    "title": "부록 A — 행렬의 기초",
    "section": "A.8 대각합",
    "text": "A.8 대각합\n\\(\\pmb A = \\{ a_{ij} \\}\\)를 \\(n \\times n\\) 정방행렬(square matrix)인 경우, 행렬의 대각 원소(diagonal element)들의 합(trace)을 \\(tr(\\pmb A)\\)로 표시한다.\n\\[ tr(\\pmb A) = \\sum_{i=1}^n a_{ii} \\]\n두 행렬의 덧셈(뺄셈)에 대한 대각합에 대한 성질들은 다음과 같다.\n\\[ tr( {\\pmb A} \\pm {\\pmb B}) = tr({\\pmb A}) \\pm tr({\\pmb B}) \\]\n\n\n\n\n\n\n주의\n\n\n\n행렬의 곱셈은 일반적으로 교환법칙이 성립하지 않지만 대각합의 연산은 교환법칙이 성립한다.\n\\[  tr(\\pmb A \\pmb B)  = tr( \\pmb B \\pmb A) \\]\n\n\n대각합은 교환법칙이 성립히기 떄문에 다음과 같은 성질이 성립한다.\n\\[\n\\operatorname{tr}(\\pmb {A} \\pmb {K} \\pmb {L})=\\operatorname{tr}(\\pmb {K} \\pmb {L} \\pmb {A})\n\\]\n벡터의 연산에서도 대각합의 교환법칙이 성립되어 다음과 같은 유용한 식이 성립한다.\n\\[\n\\operatorname{tr}\\left(\\pmb {x} \\pmb {y}^t \\right)=\\operatorname{tr}\\left(\\pmb {y}^t  \\pmb {x}\\right)=\\pmb {y}^t  \\pmb {x} \\in \\mathbb{R} .\n\\]\n대각합의 교환법칙때문에 어떤 행렬의 앞에 특정 행렬을 곱하고, 뒤에 역행렬을 곱해도 대각합은 변하지 않는다.\n\\[\n\\operatorname{tr}\\left(\\pmb {S}^{-1} \\pmb {A} \\pmb {S}\\right) = \\operatorname{tr}\\left(\\pmb {A} \\pmb {S} \\pmb {S}^{-1}\\right)=\\operatorname{tr}(\\pmb {A})\n\\]\n대각합에 대한 그 밖의 성질들은 다음과 같다.\n\n\\(\\operatorname{tr}(\\alpha \\pmb {A})=\\alpha \\operatorname{tr}(\\pmb {A}), \\alpha \\in \\mathbb{R}\\) for \\(\\pmb {A} \\in \\mathbb{R}^{n \\times n}\\)\n\\(\\operatorname{tr}\\left(\\pmb {I}_n\\right)=n\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#행렬식",
    "href": "qmd/math_mat_basic.html#행렬식",
    "title": "부록 A — 행렬의 기초",
    "section": "A.9 행렬식",
    "text": "A.9 행렬식\n\\(\\pmb A\\)의 행렬식(determinant)을 \\(det(\\pmb A)=|\\pmb A|\\)로 표기한다.\n이차원 행렬 \\(\\pmb A\\) 의 행렬식은 다음과 같이 계산한다.\n\\[\n\\operatorname{det}( \\pmb {A})=\\left|\\begin{array}{ll}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{array}\\right|=a_{11} a_{22}-a_{12} a_{21} .\n\\]\n만약 행렬 \\(\\pmb A\\)가 대각행렬(diagonal matrix)이면 \\(|\\pmb A|\\)는 행렬의 대각원소의 곱이다 (\\(| \\pmb A| =\\prod a_{ii}\\)).\n두 행렬의 곱의 행렬식은 각 행렬의 행렬식의 곱이다.\n\\[ |\\pmb A \\pmb B | = | \\pmb A| |\\pmb B| \\]\n행렬식에 대한 유용한 공식들은 다음과 같다.\n\n\\(|{\\pmb A}^t| = |{\\pmb A}|\\)\n\\(|c {\\pmb A}| = c^n |{\\pmb A}|\\)\n\n만약 행렬 \\(\\pmb A\\)가 다음과 같은 분할행렬(partitioned matrix) 의 형태를 가지면\n\\[\n\\pmb A =\n\\begin{bmatrix}\n{\\pmb A}_{11} & {\\pmb A}_{12} \\\\\n{\\pmb 0} & {\\pmb A}_{22}\n\\end{bmatrix}\n\\]\n행렬 \\(\\pmb A\\)의 행렬식은 다음과 같이 주어진다.\n\\[ |{\\pmb A}| = |{\\pmb A}_{11}| |{\\pmb A}_{22} | \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#직교행렬",
    "href": "qmd/math_mat_basic.html#직교행렬",
    "title": "부록 A — 행렬의 기초",
    "section": "A.10 직교행렬",
    "text": "A.10 직교행렬\n만약 정방행렬 \\(\\pmb P\\)가 다음과 같은 조건을 만족하면 직교행렬(orthogonal matrix)라고 부른다.\n\\[  \\pmb P \\pmb P^t = \\pmb P^t \\pmb P = \\pmb I \\]\n직교행렬의 정의에서 주의할 점은 $P P^t = I $ 와 \\(\\pmb P^t \\pmb P = \\pmb I\\) 이 모두 성립하해야 한다는 점이다.\n행렬 \\(\\pmb P\\) 의 역행렬은 \\(\\pmb P^t\\) 이다.\n\\[ \\pmb P^{-1} = \\pmb P^t\\]\n만약 \\(\\pmb P\\)가 직교행렬이면 다음과 같은 성질을 가진다.\n\n\\(| \\pmb P | = \\pm 1\\) , 왜냐하면 \\[  | \\pmb P \\pmb P^t | = | \\pmb P | |\\pmb P^t |  = | \\pmb P|^2 = |\\pmb I| =1 \\]\n임의의 정방행렬 \\(\\pmb A\\)에 대하여 다음이 성립한다. \\[ tr(\\pmb P \\pmb A \\pmb P^t) = tr(\\pmb A \\pmb P^t \\pmb P) = tr(\\pmb A) \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#우드베리-공식",
    "href": "qmd/math_mat_basic.html#우드베리-공식",
    "title": "부록 A — 행렬의 기초",
    "section": "A.11 우드베리 공식",
    "text": "A.11 우드베리 공식\n다음은 우드베리공식(Woodbury formula) 과 파생된 유용한 공식들이다.\n\\[\n(\\pmb A+\\pmb U\\pmb C\\pmb V)^{-1} = \\pmb A^{-1}-\\pmb A^{-1} \\pmb U (\\pmb C^{-1} + \\pmb V \\pmb A^{-1}\\pmb U)^{-1} \\pmb V \\pmb A^{-1}\n\\]\n\\[\n(\\pmb I+\\pmb U \\pmb C\\pmb V)^{-1} = \\pmb I - \\pmb U (\\pmb C^{-1} + \\pmb V \\pmb U)^{-1} \\pmb V\n\\]\n\\[ (\\pmb A+\\pmb u\\pmb v^t)^{-1} = \\pmb A^{-1} - \\frac{ \\pmb A^{-1} \\pmb u\\pmb v^t \\pmb A^{-1}}{1+\\pmb v^t \\pmb A^{-1}\\pmb u}  \\tag{A.3}\\]\n\\[\n(a \\pmb I_n + b \\pmb 1_n \\pmb 1_n^t)^{-1} = \\frac{1}{a} \\left [ \\pmb I_n - \\frac{b}{a+nb} \\pmb 1 \\pmb 1^t \\right ]\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html",
    "href": "qmd/math_vector_space.html",
    "title": "부록 B — 벡터공간",
    "section": "",
    "text": "B.1 벡터공간의 정의와 의미\n먼저 지금까지 우리가 배운 벡터의 개념을 일반화하여 다루기 위해서 벡터공간의 일반적 개념을 정의하고자 한다.\n벡터는 숫자를 모아놓은 형태인 식 A.1 로 주로 나타내지만 이러한 벡터를 모아놓은 집합은 실벡터 공간(real vector space)이라고 한다. 즉, 식 A.1 의 벡터는 \\(p\\)-차원 실벡터(real vector)라고 한다.\n지금부터 논의할 추상적인 벡터 공간(abstract vector space)은 어떤 집합이든 원소에 대한 더하기와 스칼라곱이 정의되어 있는 공간을 말한다.\n이제부터 \\(\\RR\\) 을 실수 전체 집합이라고 하자. 또한 \\(\\RR^n\\) 을 \\(n\\)-차원 실벡터(real vector)의 집합이라고 하자. 또한 \\(\\RR^{n \\times p}\\) 을 \\(n \\times p\\)-차원 행렬의 집합이라고 하자.\n벡터공간(vector space) 은 어떤 집합 \\(S\\) 에 다음과 같은 두 개의 연산이 정의된 공간을 말한다.\n위에서 더하기 연산이 정의되어 있다는 의미는 다음에 주어진 규칙이 성립한다는 의미이다.\n\\[  s_1 + b \\in S \\quad \\forall s_1,b \\in S \\]\n\\[  (s_1 + s_2) + s_3 = s_1 + (s_2 +s_3)  \\quad \\forall s_1,s_2,s_3 \\in S \\]\n\\[  s + e = e + s = s \\quad \\exists e ~~\\forall s \\in S \\]\n\\[ s + i = i  + s = 0 \\quad \\exists i ~~\\ \\forall s \\in S \\]\n일반적으로 항등원(\\(e\\)) 는 \\(0\\) 으로 표시하며 역원(\\(i\\)) 는 \\(-s\\) 로 표시한다.\n\\[  s_1 + s_2 = s_2 + s_1  \\quad \\forall s_1,s_2 \\in S \\]\n또한 위에서 스칼라곱 연산이 정의되어 있다는 의미는 다음에 주어진 규칙이 성립한다는 의미이다.\n\\[  r_1(s_1+s_2) = r_1 s_1 + r_2 s_2,~~~ (r_1+r_2)s = r_1 s + r_2 s  \\quad \\forall s_1,s_2 \\in S, ~~ \\forall r_1,r_2 in \\RR \\]\n\\[  r_1(r_2s) = (r_1 r_2) s  \\quad \\forall s \\in S, ~~ \\forall r_1,r_2 in \\RR \\]\n\\[  1 \\cdot s  = s \\quad \\forall s \\in S \\]\n이 강의에서는 스칼라로 실수만 사용하고 벡터공간은 실벡터공간(real vector space)만 고려할 것이다. 하지만 벡터공간은 실벡터가 아닌 다른 일반적인 집합에 대해서도 정의할 수 있음을 유의하자. 예를 들어 \\(n\\)-차원 다힝식들을 모두 모아 놓은 집합은 벡터공간이다. 또한 연속인 함수들을 모아 놓은 집합도 벡터공간이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#벡터공간의-정의와-의미",
    "href": "qmd/math_vector_space.html#벡터공간의-정의와-의미",
    "title": "부록 B — 벡터공간",
    "section": "",
    "text": "두 개의 원소에 대한 더하기(addition, \\(+\\)) 연산의 정의되어 있다.\n\\[+ ~ ~ : S + S \\rightarrow  S \\tag{B.1}\\]\n하나의 실수와 한 개의 원소에 대한 스칼라곱(scalar product, \\(\\cdot\\)) 연산이 정의되어 있다.\n\\[\\cdot ~ ~ : \\RR \\cdot S \\rightarrow  S \\tag{B.2}\\]\n\n\n\n집합 \\(S\\) 가 연산에 대하여 닫혀있다 (closure).\n\n\n\n결합법칙이 성립한다 (Associativity).\n\n\n\n항등원이 존재한다 (Neutral element).\n\n\n\n역원이 존재한다 (Inverse element).\n\n\n\n\n교환법칙이 성립한다 (Commutativity).\n\n\n\n\n스칼라곱 연산의 분배법칙이 성립한다 (Distributivity).\n\n\n\n스칼라곱 연산의 결합법칙이 성립한다\n\n\n\n스칼라곱 연산의 항등원이 존재한다 (Neutral element).\n\n\n\n\n\n\n\n\n주의\n\n\n\n벡터 공간에서 주의할 점은 두 벡터의 곱하기 가 정의되어 있다는 것이 아니라 하나의 스칼라와 하나의 벡터에 대한 스칼라 곱하기가 정의되어 있다는 것이다.\n\\[\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n=?\n\\quad {but} \\quad\n3 \\cdot\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n3 \\\\\n6\n\\end{bmatrix}\n\\]\n두 벡터의 곱하기 는 내적(inner product) 란 이름으로 따로 정의한다. 또한 두 벡터의 곱셈이 유일하게 정의되지 않는다는 점에 유의하자. 예를 들어 벡터의 곱셈은 외적(cross product) 이라는 이름으로 정의된다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#벡터의-선형독립",
    "href": "qmd/math_vector_space.html#벡터의-선형독립",
    "title": "부록 B — 벡터공간",
    "section": "B.2 벡터의 선형독립",
    "text": "B.2 벡터의 선형독립\n벡터공간에 속한 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 의 선형결합(또는 선형결합, linear combination)이란 각 벡터에 스칼라를 곱하여 더한 것들이다.\n즉 다음과 같은 형태의 식을 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\)의 선형결합(linear combination)이라고 한다:\n\\[ r_1 \\pmb v_1 + r_2 \\pmb v_2 + \\cdots + r_n \\pmb v_n, \\quad r_1,r_2,\\dots, r_n \\in \\RR  \\tag{B.3}\\]\n\n정의 B.1 (벡터의 선형독립과 선형종속) 벡터공간에 속한 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 가 있다고 하자. 만약 다음 식이 만약 모두 \\(0\\)인 \\(n\\)개의 스칼라 \\(x_1,x_2,\\dots,x_n\\)에 대해서만 성립하면 \\(n\\)개 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 들은 선형독립(linearly independent)라고 한다.\n\\[\nx_1 \\pmb v_1 + x_2 \\pmb v_2 + \\dots + x_n \\pmb v_n = \\pmb 0 \\quad \\Longleftrightarrow\nx_1 = x_2 = \\dots = x_n =0\n\\tag{B.4}\\]\n또한 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 가 선형독립이 아니면 선형종속(linear dependent)라고 한다. 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 가 선형종속이면 모두 0이 아닌 \\(x_1,x_2,\\dots,x_n\\) 이 존재하여 다음이 성립한다는 것이다.\n\\[\n\\exists~ x_1,x_2,\\dots,x_n \\in \\RR \\text{ s.t. } (x_1,x_2,\\dots,x_n) \\ne \\pmb 0,\\quad  \\pmb v_1 + x_2 \\pmb v_2 + \\dots + x_n \\pmb v_n = \\pmb 0\n\\tag{B.5}\\]\n\\(\\blacksquare\\)\n\n예를 들어 다음과 같이 주어진 3개의 3-차원 벡터들은 선형종속이다.\n\\[\n\\pmb v_1 =\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\n\\end{bmatrix},\n\\quad\n\\pmb v_2 =\n\\begin{bmatrix}\n1\\\\\n0\\\\\n1\n\\end{bmatrix},\n\\quad\n\\pmb v_3 =\n\\begin{bmatrix}\n3\\\\\n2\\\\\n5\n\\end{bmatrix}\n\\tag{B.6}\\]\n왜냐하면 다음과 같이 모두 0이 아닌 스칼라에 의해서 다음 식이 성립하기 떄문이다. 즉 벡터 \\(\\pmb v_3\\)는 \\(\\pmb v_2\\) 에 2를 곱하여 \\(\\pmb v_1\\)에 더한 값과 같다.\n\\[\n\\pmb v_3 = \\pmb v_1 + 2 \\pmb v_2 \\quad \\Longleftrightarrow \\quad    \\pmb v_1 + 2 \\pmb v_2 -\\pmb v_3 = 0\n\\]\n이제 다음과 같이 주어진 3개의 3-차원 벡터들은 선형독립이다. 즉 3개 벡터의 선형 조합이 0이 될 수 있도록 만드는 스칼라는 모두 0인 경우 밖에 없다.\n\\[\n\\pmb v_1 =\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\n\\end{bmatrix},\n\\quad\n\\pmb v_2 =\n\\begin{bmatrix}\n1\\\\\n0\\\\\n1\n\\end{bmatrix},\n\\quad\n\\pmb v_3 =\n\\begin{bmatrix}\n3\\\\\n2\\\\\n4\n\\end{bmatrix}\n\\tag{B.7}\\]\n이제 다음과 같이 주어진 4개의 3-차원 벡터들은 선형종속이다.\n\\[\n\\pmb v_1 =\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\n\\end{bmatrix},\n\\quad\n\\pmb v_2 =\n\\begin{bmatrix}\n1\\\\\n0\\\\\n1\n\\end{bmatrix},\n\\quad\n\\pmb v_3 =\n\\begin{bmatrix}\n3\\\\\n2\\\\\n4\n\\end{bmatrix}\n\\quad\n\\pmb v_4 =\n\\begin{bmatrix}\n0\\\\\n0\\\\\n1\n\\end{bmatrix}\n\\tag{B.8}\\]\n\\(\\pmb v_3\\) 가 다음과 같이 다른 벡터의 선형결합으로 나타난는 것을 보여준다.\n\\[ \\pmb v_3 = \\begin{bmatrix}\n3\\\\\n2\\\\\n4\n\\end{bmatrix}\n= (1)\\pmb v_1 +  (2)\\pmb v_2 +  (-1)\\pmb v_4\n= (1)\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\n\\end{bmatrix}\n+\n(2)\n\\begin{bmatrix}\n1\\\\\n0\\\\\n1\n\\end{bmatrix}\n+\n(-1)\\begin{bmatrix}\n0\\\\\n0\\\\\n1\n\\end{bmatrix}\n\\]\n식 B.8 와 같이 3차원 벡터가 4개인 경우 벡터의 값에 관계없이 선형종속으로 나타난다. 이러한 사실은 \\(\\RR^n\\)의 \\(n+1\\) 개의 벡터는 항상 선형종속이라는 정리의 결과이다.\n즉, \\(\\RR^n\\)에서 \\(n\\)개보다 더 많은 벡터들은 항상 선형종속이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#역행렬",
    "href": "qmd/math_vector_space.html#역행렬",
    "title": "부록 B — 벡터공간",
    "section": "B.3 역행렬",
    "text": "B.3 역행렬\n정방행렬 \\(\\pmb A\\) 의 역행렬(inverse matrix) \\(\\pmb A^{-1}\\)는 다음과 같은 성질을 만족하는 행렬이다.\n\\[ \\pmb A \\pmb A^{-1} = \\pmb A^{-1} \\pmb A = \\pmb I \\]\n역행렬은 언제나 존재하는 것은 아니다. 만약 행렬 \\(\\pmb A\\)가 역행렬을 가지면 이를 \\(\\pmb A^{-1}\\)로 표시한다. 또한 역행렬이 존재하면 정칙행렬(non-singular matrix)이라고 한다.\n역행렬은 존재하는 조건은 행렬식(determinant)이 0이 아니어야 한다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#행렬의-계수",
    "href": "qmd/math_vector_space.html#행렬의-계수",
    "title": "부록 B — 벡터공간",
    "section": "B.4 행렬의 계수",
    "text": "B.4 행렬의 계수\n행렬의 계수(rank)란 일차 독립인 열들의 최대 수 또는 일차 독립인 행들의 최대 수로 정의된다\n\\[ rank(\\pmb A) = rk(\\pmb A) = dim(Col(\\pmb A)) = dim(Row(\\pmb A)) \\] 꼭 기억해야 할 것은 행렬의 계수는 열들을 이용하여 구한 계수와 행들을 이용하여 구한 계수가 같다는 것이다. 즉, 행렬의 계수는 열의 계수와 행의 계수 중 하나만 구해도 된다는 것이다.\n예를 들어 식 B.6 에 주어진 3 개의 벡터를 열로 하는 행렬의 계수는 2이다. 왜냐하면 선형종속인 벡터가 하나 있기 때문이다.\n\\[\n\\pmb A =\n\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 0 & 2\\\\\n3 & 1 & 5\n\\end{bmatrix} \\quad \\rightarrow \\quad rank(\\pmb A) = 2\n\\]\n위에 주어진 행렬 \\(\\pmb A\\)의 행들을 고려하면 첫 번째 행과 두 번째 행의 합이 세 번째 행으로 나타난다. 즉, 서로 독립인 행의 최대 개수는 2 이며 이는 서로 독립인 열의 최대 개수와 같다. 따라서 행렬 \\(\\pmb A\\)의 계수는 2이다.\n다음으로 식 B.7 에 주어진 3 개의 벡터를 열로 하는 행렬의 계수는 3이다. 3개의 열벡터와 3개의 행벡터들은 모두 선형독립이다.\n\\[\n\\pmb A =\n\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 0 & 2\\\\\n3 & 1 & 4\n\\end{bmatrix} \\quad \\rightarrow \\quad rank(\\pmb A) = 3\n\\] 이제 식 B.8 주어진 4개의 벡터로 이루어진 행렬의 계수는 3이다. 왜냐하면 4개의 열벡터 중 3개의 열벡터는 선형독립이지만 4번째 열벡터는 선형종속이기 때문이다.\n\\[\n\\pmb A =\n\\begin{bmatrix}\n1 & 1 & 3 & 0\\\\\n2 & 0 & 2 & 0\\\\\n3 & 1 & 4 & 1\n\\end{bmatrix} \\quad \\rightarrow \\quad rank(\\pmb A) = 3\n\\]\n행렬의 열과 행의 개수가 다를 때 행렬의 계수는 열의 개수와 행의 개수 중 작은 값보다 같거나 작다 예를 들어 \\(n \\times p\\) 행렬의 계수는 \\(min(n,p)\\) 와 같거나 작다.\n\\[ A \\in \\RR^{n \\times p} \\quad \\rightarrow \\quad rank(\\pmb A) \\le min(n,p) \\]\n다음은 행렬의 계수에 관련된 주요 공식이다.\n\n\\(rank(\\pmb A ) = rank(\\pmb A^t)\\)\n행렬 \\(\\pmb A\\) 가 정방행렬이고 계수가 \\(n\\) 이면 역행렬이 존재한다(정칙행렬).\n또한 더 나아가 \\(\\pmb A\\) 가 정칙행렬이라는 사실은 아래 나열된 조건들과 동치(equivalance)이다.\n\\(\\quad \\Leftrightarrow\\) \\(\\pmb A\\) 의 열들이 일차독립이다.\n\\(\\quad \\Leftrightarrow\\) \\(\\pmb A\\) 의 행들이 일차독립이다.\n\\(\\quad \\Leftrightarrow\\) \\(\\pmb A\\) 의 계수가 \\(n\\) 이다.\n\\(\\quad \\Leftrightarrow\\) \\(\\pmb A\\) 의 행렬식이 0이 아니다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#생성집합과-기저",
    "href": "qmd/math_vector_space.html#생성집합과-기저",
    "title": "부록 B — 벡터공간",
    "section": "B.5 생성집합과 기저",
    "text": "B.5 생성집합과 기저\n벡터공간 \\(V\\) 의 벡터 \\(\\pmb v_1,\\pmb v_n, \\dots, \\pmb v_m\\) 의 선형결합을 모두 모은 집합\n\\[ W = span\\{\\pmb v_1,\\pmb v_2, \\dots, \\pmb v_m \\} = \\{r_1\\pmb v_1 + r_2 \\pmb v_2 + \\cdots+ r_m \\pmb v_m:\nr_1,r_2,\\dots,r_m \\in \\RR \\}\\]\n을 벡터 \\(\\pmb v_1,\\pmb v_n, \\dots, \\pmb v_m\\) 의 생성(span)이라고 하며 \\(W\\) 의 생성집합(generating set, spanning set) 이라고 한다.\n또한 어떤 벡터공간의 생성집합에 속한 벡터들이 선형독립일 때 이 생성집합을 기저 (basis)라고 한다.\n만약 주어진 벡터 공간의 부분집합이 다시 벡터공간의 정의를 만족한다면 이를 부분공간(subspace)이라고 한다. 위에서 정의한 생성집합 \\(W\\)는 벡터공간 \\(V\\)의 부분공간이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#벡터공간의-차원",
    "href": "qmd/math_vector_space.html#벡터공간의-차원",
    "title": "부록 B — 벡터공간",
    "section": "B.6 벡터공간의 차원",
    "text": "B.6 벡터공간의 차원\n\n\\(\\RR^n\\) 의 모든 기저는 \\(n\\)개의 원소를 갖는다.\n임의의 벡터공간 \\(V\\)에 대해서 \\(V\\)의 부분집합 \\(B = \\{\\pmb b_1,\\dots,\\pmb b_n\\}\\) 가 \\(V\\)의 기저라고 하면 다음을 보일 수 있다.\n\n\\(V\\) 의 모든 벡터들은 \\(\\pmb b_1,\\dots,\\pmb b_n\\) 의 선형결합으로 나타낼 수 있으며 유일하다.\n\\(V\\) 의 부분집합이 \\(n\\) 개보다 많은 벡터를 포함하면 이 부분집합의 벡터들은 선형종속이다.\n\\(V\\) 의 또 다른 기저 \\(C=\\{\\pmb c_1,\\dots,\\pmb c_m \\}\\) 가있다면 \\(m=n\\) 이다.\n\n벡터공간 \\(V\\)의 차원(dimension) 은 기저의 개수로 정의되며 \\(dim(V)\\)로 표시한다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#행렬의-열공간과-행공간",
    "href": "qmd/math_vector_space.html#행렬의-열공간과-행공간",
    "title": "부록 B — 벡터공간",
    "section": "B.7 행렬의 열공간과 행공간",
    "text": "B.7 행렬의 열공간과 행공간\n\\(n \\times p\\) 행렬 \\(\\pmb A\\) 에 의하여 생성되는 열공간(column space) \\(C(\\pmb A)\\)는 행렬 \\(\\pmb A\\)를 구성하는 열벡터의 선형조합으로 나타낼 수 있는 모든 벡터들의 집합을 말한다.\n\\[ C(\\pmb A) = \\{ {\\pmb y } | {\\pmb y} = {\\pmb A} {\\pmb x}, {\\pmb x} \\in \\RR^p \\}  \\subset \\RR^n\\]\n\\(n \\times p\\) 행렬 \\(\\pmb A\\) 에 의하여 생성되는 영공간(null space) \\(N(\\pmb A)\\) 는 다음과 같이 정의되는 벡터들의 집합을 말한다.\n\\[ N(\\pmb A) = \\{ {\\pmb x} | {\\pmb A} {\\pmb x} = {\\pmb 0} \\text{ for } {\\pmb x} \\in \\RR^p \\} \\subset \\RR^n\\]\n벡터공간과 영공간은 다음과 같은 성질을 가진다.\n\n\\(rank(\\pmb A) = \\text{ dimension of } C(\\pmb A) = dim[C(\\pmb A)]\\)\n\\(dim[C(\\pmb A)] + dim[N(\\pmb A)] = n\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#두-벡터의-사영",
    "href": "qmd/math_vector_space.html#두-벡터의-사영",
    "title": "부록 B — 벡터공간",
    "section": "B.8 두 벡터의 사영",
    "text": "B.8 두 벡터의 사영\n선형독립인 두 벡터 \\(\\pmb a_1\\)과 \\(\\pmb a_2\\) 가 있다고 하자. 벡터 \\(\\pmb a_1\\)과 같은 방향을 가지면서 벡터 \\(\\pmb a_2\\)에 가장 가까운 벡터를 \\(proj_{\\pmb a_1} (\\pmb a_2)\\) 라고 정의하고 이를 벡터 \\(\\pmb a_1\\) 방향으로 벡터 \\(\\pmb a_2\\) 의 사영(projection)이라고 부른다.\n그러면 이러한 사영은 어떻게 구할 수 있나? 벡터 \\(\\pmb a_2\\) 의 사영은 벡터 \\(\\pmb a_1\\) 방향에 있으므로 어떤 실수 \\(c\\) 가 있어서 다음과 같이 표시할 수 있다.\n\\[ proj_{\\pmb a_1} (\\pmb a_2) =  c \\pmb a_1 \\]\n이제 사영 \\(c \\pmb a_1\\)과 벡터 \\(\\pmb a_2\\)의 거리 \\(d(c)\\) 를 생각하면 다음과 같다.\n\\[\n\\begin{aligned}\nd^2(c) & = \\norm{\\pmb a_2 - c \\pmb a_1}^2 \\\\\n   & = (\\pmb a_2 - c \\pmb a_1)^t(\\pmb a_2 - c \\pmb a_1) \\\\\n   & = \\pmb a^t_2 \\pmb a_2 -2 c \\pmb a_2^t \\pmb a_1 + c^2 \\pmb a^t_1 \\pmb a_1\n\\end{aligned}\n\\]\n위에서 \\(\\norm{\\pmb a}\\) 는 벡터 \\(\\pmb a\\)의 길이를 나타낸다.\n\\[ d(\\pmb a) = \\norm{\\pmb a} = \\sqrt{\\pmb a^t \\pmb a} \\]\n상수 \\(c\\) 는 거리 \\(d(c)\\)를 최소로 만드는 수이다. \\(d^2(c)\\)은 \\(c\\) 에 대하여 미분 가능한 2차 함수이며 아래로 볼록한 함수이므로 이를 미분하여 \\(c\\) 를 구할 수 있다.\n\\[ \\pardiff{d^2(c)}{c} = - 2\\pmb a_2^t \\pmb a_1 + 2c \\pmb a^t_1 \\pmb a_1 =0 \\]\n위의 방적식으로 부터 \\(c\\)를 얻고 \\[ c= \\frac{\\pmb a_2^t \\pmb a_1  }{\\pmb a^t_1 \\pmb a_1} \\]\n다음과 같이 벡터 \\(\\pmb a_1\\) 방향으로 벡터 \\(\\pmb a_2\\) 의 사영을 나타낼 수 있다.\n\\[\nproj_{\\pmb a_1} (\\pmb a_2) = \\frac{ \\pmb a_2^t \\pmb a_1} {\\pmb a_1^t \\pmb a_1} \\pmb a_1\n\\tag{B.9}\\]\n이제 위의 두 벡터의 사영을 이용하면 벡터 \\(\\pmb a_1\\) 과 직교하는 벡터 \\(\\tilde {\\pmb q}_2\\)를 다음과 같이 찾을 수 있다.\n\\[\n\\tilde {\\pmb q}_2 = \\pmb a_2 - proj_{\\pmb a_1} (\\pmb a_2) = \\pmb a_2 -  \\frac{\\pmb a_2^t \\pmb a_1} {\\pmb a_1^t \\pmb a_1} \\pmb a_1\n\\]\n\n\n\n\n\n벡터의 사영\n\n\n\n\n두 벡터 \\(\\pmb a_1\\)와 \\(\\tilde {\\pmb q}_2\\)의 직교성은 다음과 같이 보일 수 있다.\n\\[\n\\begin{aligned}\n\\pmb a_1^t  \\tilde {\\pmb q}_2 & =\n\\pmb a_1^t  \\left ( \\pmb a_2 -  \\frac{ \\pmb a_2^t \\pmb a_1} {\\pmb a_1^t \\pmb a_1} \\pmb a_1 \\right ) \\notag \\\\\n& = \\pmb a_1^t \\pmb a_2 - \\frac{ \\pmb a_2^t \\pmb a_1} {\\pmb a_1^t \\pmb a_1}  \\pmb a_1^t \\pmb a_1 \\notag \\\\\n& = \\pmb a_1^t \\pmb a_2 - \\pmb a_2^t \\pmb a_1 \\notag \\\\\n& = 0\n\\end{aligned}\n\\tag{B.10}\\]\n이제 두 벡터 \\(\\pmb q_1\\)과 \\(\\pmb q_2\\) 를 다음과 같이 정규직교벡터로 만들 수 있다.\n\\[\n\\begin{aligned}\n\\pmb q_1 & =  \\pmb a_1 / \\norm{\\pmb a_1 } \\\\\n\\pmb q_2 & =  \\tilde {\\pmb q}_2 / \\norm{\\tilde {\\pmb q}_2}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_vector_space.html#최소제곱법과-사영",
    "href": "qmd/math_vector_space.html#최소제곱법과-사영",
    "title": "부록 B — 벡터공간",
    "section": "B.9 최소제곱법과 사영",
    "text": "B.9 최소제곱법과 사영\n회귀계수벡터의 값을 구하는 최소제곱법의 기준을 다시 살펴보자.\n\\[   \n\\min_{\\pmb \\beta } ( \\pmb y -  \\pmb X \\pmb \\beta )^t( \\pmb y -  \\pmb X \\pmb \\beta )  \n\\]\n위에서 \\(\\pmb X \\pmb \\beta\\)는 행렬 \\(\\pmb X\\)의 열벡터 \\(\\pmb x_1, \\pmb x_2, \\dots, \\pmb x_p\\) 로 이루어진 선형조합이다.\n\\[ \\pmb X \\pmb \\beta = [\\pmb x_1~ \\cdots \\pmb x_p]\\pmb \\beta\n=  \\beta_1 \\pmb x_1 + \\cdots + \\beta_p \\pmb x_p \\]\n행렬 \\(\\pmb X\\)의 열벡터로 생성돤 공간을 \\(C(\\pmb X)\\) 라고 하자\n\\[ C(\\pmb X) = span \\{ \\pmb x_1, \\dots, \\pmb x_p \\}  = \\{ \\pmb X \\pmb \\beta ~|~ \\pmb \\beta \\in \\RR^p \\}\\]\n따라서 최소제곱법으로 구한 회귀계수 벡터 \\(\\hat {\\pmb \\beta}\\)는 반응값 벡터 \\(\\pmb y\\) 와 \\(\\pmb X {\\pmb \\beta}\\)의 거리가 최소가 되도록 만들어 준다.\n\\[\n\\min_{\\pmb \\beta } ( \\pmb y -  \\pmb X \\pmb \\beta )^t( \\pmb y -  \\pmb X \\pmb \\beta )  =\n( \\pmb y -  \\pmb X  \\hat {\\pmb \\beta} )^t( \\pmb y -  \\pmb X  \\hat {\\pmb \\beta} )  \n\\] \\[\n\\hat {\\pmb \\beta} = (\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y\n\\]\n따라서 예측값 벡터 \\(\\hat {\\pmb y}\\) 는 행렬 \\(\\pmb X\\)의 열벡터로 생성한 열공간 \\(C(\\pmb X)\\) 방향으로 반응값 벡터 \\(\\pmb y\\)를 사영한 벡터이다.\n\\[ \\hat {\\pmb y} = \\pmb X \\hat {\\pmb \\beta} = \\pmb  X(\\pmb X^t \\pmb X)^{-1} \\pmb X^t \\pmb y \\]\n위에서 행렬 \\(\\pmb X(\\pmb X^t \\pmb X)^{-1} \\pmb X^t\\)를 열공간 \\(C(\\pmb X)\\)의 사영행렬(projection matrix)라고 부른다. 사영행렬의 정의는 부록 F 에서 공부한다.\n\\[\n\\pmb H = \\pmb X(\\pmb X^t \\pmb X)^{-1} \\pmb X^t\n\\tag{B.11}\\]\n\n\n\n\n\n최소제곱법을 설명한 그림",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>벡터공간</span>"
    ]
  },
  {
    "objectID": "qmd/math_eigen_value.html",
    "href": "qmd/math_eigen_value.html",
    "title": "부록 C — 고유값과 고유벡터",
    "section": "",
    "text": "C.1 특성다항식\n특성다항식(Characteristic polynomial)은 다음과 같이 정의된다\n실수 \\(\\lambda \\in \\mathbb{R}\\) 와 정방행렬(square matrix) \\(\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}\\) 에 대하여\n\\[\n\\begin{aligned}\np_{\\boldsymbol{A}}(\\lambda) & :=\\operatorname{det}(\\boldsymbol{A}-\\lambda \\boldsymbol{I}) \\\\\n& =c_0+c_1 \\lambda+c_2 \\lambda^2+\\cdots+c_{n-1} \\lambda^{n-1}+(-1)^n \\lambda^n,\n\\end{aligned}\n\\tag{C.1}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>고유값과 고유벡터</span>"
    ]
  },
  {
    "objectID": "qmd/math_eigen_value.html#고유값과-고유벡터",
    "href": "qmd/math_eigen_value.html#고유값과-고유벡터",
    "title": "부록 C — 고유값과 고유벡터",
    "section": "C.2 고유값과 고유벡터",
    "text": "C.2 고유값과 고유벡터\n\nC.2.1 정의\n\\(n\\)-차원 정방행렬 \\(\\pmb A\\) 이 있을 때, 다음 식을 만족하는 \\(\\lambda\\) 와 벡터 \\(\\pmb x\\)가 존재하면 \\(\\lambda\\) 를 행렬 \\(\\pmb A\\) 의 고유값(eigenvalue), \\(\\pmb x\\) 를 행렬 \\(\\pmb A\\) 의 고유벡터(eigenvector)라고 한다 (부교재 definition 4.6)\n\\[ \\pmb A \\pmb x = \\lambda \\pmb x \\]\n\n고유벡터는 유일하지 않다. 즉, 벡터 \\(\\pmb x\\) 가 고유벡터이면 \\(c \\pmb x\\) 도 고유벡터이다.\n\n\\[ \\pmb A (c \\pmb x) = c \\pmb A \\pmb x = c \\lambda \\pmb x = \\lambda (c \\pmb x) \\]\n\n\nC.2.2 계산\n다음 3개의 문장은 동치이다\n\n\\(\\lambda\\) 는 행렬 \\(\\pmb A\\) 의 고유값이다.\n방정식 \\((\\pmb A - \\lambda \\pmb I)\\pmb x = \\pmb 0\\) 은 영벡터이외의 해를 가진다(nontrivial solution)\n\\(\\lambda\\) 는 행렬 \\(\\pmb A - \\lambda \\pmb I\\) 의 행렬식이 0이다.\n\n\\[ \\operatorname{det}(\\pmb A - \\lambda \\pmb I) = 0  \\tag{C.2}\\]\n\n\\(\\lambda\\) 는 행렬 \\(\\pmb A - \\lambda \\pmb I\\) 의 rank가 \\(n\\) 보다 작다.\nTheorem 4.8 에 의하면 위에서 행렬식이 0 인 방정식 식 C.2 을 푸는 것은 식 C.1 의 이 0 을 푸는 것과 동일하다는 것이다.\n\n\n\nC.2.3 중복도와 고유공간\n\n대수적 중복도(algebraic multiplicity) 는 특성다항식 식 C.1 이 0인 방정식을 푸는 경우 다항식에서 고유값이 중근(multiple root)의 해로 나타나는 차수를 의미한다.\n기하적 중복도(geometric multiplicity) 는 고유값에 대응하는 고유벡터들 중 선형독립인 고유벡터들의 최대 개수를 의미한다.\n고유 공간(eigenspace)은 고유값에 대응하는 고유벡터들이 생성하는 벡터공간을 의미한다.\n\n\n\n예제 C.1 3차원 행렬 \\(\\pmb A\\) 가 다음과 같을 때\n\\[\\pmb A=\\left[\\begin{array}{ccc}0 & 0 & -2 \\\\ 1 & 2 & 1 \\\\ 1 & 0 & 3\\end{array}\\right]\\]\n행렬 \\(\\pmb A\\)의 특성다항식은 다음과 같다.\n\\[\n\\operatorname{det}(\\lambda \\pmb I -\\pmb A)=\n\\left|\\begin{array}{ccc}\n\\lambda & 0 & 2 \\\\\n-1 & \\lambda-2 & -1 \\\\\n-1 & 0 & \\lambda-3\n\\end{array}\\right|=(\\lambda-1)(\\lambda-2)^2\n\\] 참고로 특성방정식을 푸는 경우, 방정식 \\(\\operatorname{det}(\\pmb A - \\lambda \\pmb I)=0\\) 이나 \\(\\operatorname{det}(\\lambda \\pmb I -\\pmb A)= 0\\) 중 어느 것을 사용해도 상관없다.\n첫번째 고유값은 \\(\\lambda_1=1\\) 이다. 고유벡터를 구하기 위해서는 다음과 같은 방정식을 풀면 된다.\n\\[ (\\lambda_1 \\pmb I -\\pmb A )\\pmb x = \\pmb 0  \\]\n위의 방정식을 풀면\n\\[\n(\\lambda_1 \\pmb I -\\pmb A )\\pmb x= (\\pmb I -\\pmb A )\\pmb x\n=\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n-1 & -1 & -1 \\\\\n-1 & 0 & -2\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\n아래와 같이 간단히 할 수 있으며\n\\[ x_1 = -2x_3, \\quad x_2 = x_3 \\] 다음과 같은 고유값과 고유벡터를 얻을 수 있다.\n\\[ \\lambda_1=1 \\quad \\rightarrow \\quad  {\\pmb x}_1=\\begin{bmatrix}-2 \\\\ 1 \\\\ 1\\end{bmatrix} \\]\n첫번째 고유값은 \\(\\lambda_1=1\\) 이며 대수적 중복도는 1이고 기하적 중복도도 1이다. 이 경우 고유공간 \\(E_1\\) 은 한 개의 고유벡터 \\(\\pmb x_1\\) 이 생성하는 부분공간을 의미한다.\n\\[\nE_1 = \\text{span}\\left\\{\\begin{bmatrix}-2 \\\\ 1 \\\\ 1\\end{bmatrix} \\right\\}\n\\]\n다음으로 두번째 고유값에 대한 방정식 \\((\\lambda_2 \\pmb I -\\pmb A )\\pmb x = \\pmb 0\\) 을 풀면 다음과 같다.\n\\[\n(\\lambda_2 \\pmb I -\\pmb A )\\pmb x= (2\\pmb I -\\pmb A )\\pmb x =\n\\begin{bmatrix}\n2 & 0 & 2 \\\\\n-1 & 0 & -1 \\\\\n-1 & 0 & -1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\n이 방정식은 아래와 같이 간단히 할 수 있으며\n\\[ x_1 = -x_3 \\] 다음과 같은 두 개의 고유벡터를 얻을 수 있다.\n\\[\n\\lambda_2=2\\quad \\rightarrow \\quad  {\\pmb x}_2=\\begin{bmatrix}-1 \\\\ 0 \\\\ 1\\end{bmatrix}\n\\quad {\\pmb x}_3=\\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix}\n\\]\n위에서 두번째 고유값은 \\(\\lambda_2=2\\) 이며 대수적 중복도는 2이다. 또한 선형독립인 2개의 고유벡터를 구할 수 있으므로 기하적 중복도는 2이다.\n이 경우 \\(E_2\\) 는 두 개의 고유벡터 \\(\\pmb x_2, \\pmb x_3\\) 가 생성하는 부분공간을 의미한다.\n\\[\nE_2 = \\text{span}\\left\\{\\begin{bmatrix}-1 \\\\ 0 \\\\ 1\\end{bmatrix}, \\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix}\\right\\}\n\\]\n\\(\\blacksquare\\)\n\n\n이제 대수적 중복도와 기하적 중복도가 다른 경우에 대한 예제를 들어보자.\n\n\n예제 C.2 3차원 행렬 \\(\\pmb A\\) 가 다음과 같을 때\n\\[\\pmb A=\\left[\\begin{array}{ccc}1 & 0 & 2 \\\\ -1 & 1 & 3 \\\\ 0 & 0 & 2\\end{array}\\right]\\]\n행렬 \\(\\pmb A\\)의 특성다항식은 다음과 같다.\n\\[\n\\operatorname{det}(\\lambda \\pmb I -\\pmb A)=\n\\left|\\begin{array}{ccc}\n\\lambda-1 & 0 & -2 \\\\\n1 & \\lambda-1 & -3  \\\\\n0 & 0 & \\lambda-2\n\\end{array}\\right|=(\\lambda-1)^2(\\lambda-2)\n\\] 첫번째 고유값은 \\(\\lambda_1=1\\) 이다. 고유벡터를 구하기 위해서는 다음과 같은 방정식을 풀면 된다.\n\\[ (\\lambda_1 \\pmb I -\\pmb A )\\pmb x = \\pmb 0  \\]\n위의 방정식을 풀면\n\\[\n(\\lambda_1 \\pmb I -\\pmb A )\\pmb x= (\\pmb I -\\pmb A )\\pmb x =\n\\begin{bmatrix}\n0 & 0 & -2 \\\\\n1 & 0 & -3 \\\\\n0 & 0 & -1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\n아래와 같이 간단히 할 수 있으며\n\\[ \\quad x_1 = x_3 =0 \\] 다음과 같은 하나의 고유벡터를 얻을 수 있다.\n\\[ \\lambda_1=1 \\quad \\rightarrow \\quad  x_1=\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\]\n첫번째 고유값은 \\(\\lambda_1=1\\) 이며 대수적 중복도는 2이지만 기하적 중복도는 1이다. 이 경우 고유공간 \\(E_1\\) 은 한 개의 고유벡터 \\(\\pmb x_1\\) 이 생성하는 부분공간을 의미한다.\n\\[\nE_1 = \\text{span}\\left\\{\\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix} \\right\\}\n\\]\n다음으로 두번째 고유값에 대한 방정식 \\((\\lambda_2 \\pmb I -\\pmb A )\\pmb x = \\pmb 0\\) 을 풀면 다음과 같다.\n\\[\n(\\lambda_2 \\pmb I -\\pmb A )\\pmb x= (2\\pmb I -\\pmb A )\\pmb x =\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n1 & 1 & -3 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\n이 방정식은 아래와 같이 간단히 할 수 있으며\n\\[ x_1 = -2x_3, \\quad x_2=5x_3  \\] 다음과 같은 한 개의 고유벡터를 얻을 수 있다.\n\\[\n\\lambda_2=2\\quad \\rightarrow \\quad  x_2=\\begin{bmatrix}-2 \\\\ 5 \\\\ 1\\end{bmatrix}\n\\]\n위에서 두번째 고유값은 \\(\\lambda_2=2\\) 이며 대수적 중복도는 1이다. 또한 선형독립인 1개의 고유벡터를 구할 수 있으므로 기하적 중복도는 1이다.\n이 경우 \\(E_2\\) 는 한 개의 고유벡터 \\(\\pmb x_2\\) 가 생성하는 부분공간을 의미한다.\n\\[\nE_2 = \\text{span}\\left\\{\\begin{bmatrix}-2\\\\ 5 \\\\ 1\\end{bmatrix}\\right\\}\n\\]\n\\(\\blacksquare\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>고유값과 고유벡터</span>"
    ]
  },
  {
    "objectID": "qmd/math_eigen_value.html#대칭행렬의-대각화",
    "href": "qmd/math_eigen_value.html#대칭행렬의-대각화",
    "title": "부록 C — 고유값과 고유벡터",
    "section": "C.3 대칭행렬의 대각화",
    "text": "C.3 대칭행렬의 대각화\n\\(n\\)차원 대칭행렬 \\(\\pmb  A\\) 에 대하여 직교행렬 \\(\\pmb  P\\)가 존재하여 다음과 같은 분해가 가능하다.\n\\[\n\\pmb  P^t \\pmb  A \\pmb  P = \\pmb  \\Lambda = diag(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)\n\\tag{C.3}\\]\n식 F.2 의 분해에서 \\(\\lambda_i\\)는 행렬 \\(\\pmb  A\\)의 고유치이며 행렬 \\(\\pmb  P\\)의 \\(i\\) 번째 열은 대응하는 고유벡터 \\(\\pmb  p_i\\) 로 구성되어 있다.\n\\[\n\\pmb  P = [ \\pmb  p_1~~ \\pmb  p_2 ~~ \\dots ~~ \\pmb  p_n ]\n\\] 이제 위의 분해를 증명해 보자. 고유치 \\(\\lambda_i\\) 와 대응하는 고유벡터 \\(\\pmb  p_i\\)의 정의에 따라서 다음과 같은 \\(n\\)개의 식을 얻을 수 있고\n\\[ \\pmb  A \\pmb  p_i = \\lambda_i \\pmb  p_i , \\quad i=1,2,3\\dots, n \\]\n위의 식을을 합쳐서 표기하면 다음과 같은 식을 얻으며 이는 식 F.2 를 의미한다.\n\\[ \\pmb  A \\pmb  P = \\pmb  P \\pmb  \\Lambda \\]\n식 F.2 를 다시 쓰면 다음과 같은 스펙트럴 분해(spectral decomposition)를 얻는다.\n\\[\n\\pmb  A  = \\pmb  P \\pmb  \\Lambda \\pmb  P^t  = \\sum_{i=1}^n \\lambda_i \\pmb  p_i \\pmb  {p}_i^t\n\\tag{C.4}\\]\n참고로 다음의 유용한 두 식을 기억하자.\n\\[ tr(\\pmb  A) = \\sum_i \\lambda_i ,\\quad |\\pmb  A| = \\prod_i \\lambda_i \\]\n대칭행렬의 분해 (ref?)(eq:symmdecomp1)를 이용하면 다음과 같은 이차형식의 분해를 얻을 수 있다.\n\\[\\begin{equation}\nQ(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x = \\pmb  x^t \\pmb  P \\pmb  \\Lambda \\pmb  P^t \\pmb  x = \\pmb  y^t \\pmb  \\Lambda \\pmb  y= \\sum_{i=1}^n \\lambda_i y_i^2\n(\\#eq:quaddecomp)\n\\end{equation}\\]\n이차형식의 분해식 @ref(eq:quaddecomp) 를 보면 행렬 \\(\\pmb  A\\)의 모든 고유치가 0보다 크면 양정치 임을 알 수 있다. 또한 모든 고유치가 0보다 크거나 같으면 양반정치 임을 알 수 있다.\n또한 \\(rank(\\pmb  A) = rank(\\pmb  \\Lambda)\\)이며 이는 0이 아닌 고유치의 개수가 행렬 \\(\\pmb  A\\)의 계수(rank)임을 알 수 있다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>고유값과 고유벡터</span>"
    ]
  },
  {
    "objectID": "qmd/math_vec_cal.html",
    "href": "qmd/math_vec_cal.html",
    "title": "부록 D — 벡터 미분",
    "section": "",
    "text": "D.1 용어",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>벡터 미분</span>"
    ]
  },
  {
    "objectID": "qmd/math_vec_cal.html#용어",
    "href": "qmd/math_vec_cal.html#용어",
    "title": "부록 D — 벡터 미분",
    "section": "",
    "text": "vector differential: 벡터 미분\npartial derivative: 편미분\ngradient: 그레디언트\nJacobian: 야코비안, 자코비안",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>벡터 미분</span>"
    ]
  },
  {
    "objectID": "qmd/math_vec_cal.html#벡터-미분의-표기법",
    "href": "qmd/math_vec_cal.html#벡터-미분의-표기법",
    "title": "부록 D — 벡터 미분",
    "section": "D.2 벡터 미분의 표기법",
    "text": "D.2 벡터 미분의 표기법\n이제 다변량함수(multivariate function), \\(f: \\RR^n \\rightarrow \\RR^m\\)에 대한 미분을 생각해보자.\n먼저 간단한 예제를 고려해 보자. 두 열벡터\n\\[\n\\pmb x=\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}\n\\in \\RR_2, \\quad\n\\pmb y=\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3\n\\end{bmatrix} \\in \\RR^3\n\\]\n를 고려하고 다음과 같은 함수로 두 벡터의 관계가 정의된다고 하자.\n\\[\ny_1 = x_1^2 + x_2, \\quad y_2= \\exp (x_1) + 3 x_2, \\quad y_3 = \\sin(x_1) + x_2^3\n\\tag{D.1}\\]\n위의 관계를 함수 관계 \\(\\pmb f: \\RR^2 \\rightarrow \\RR^3\\) 로 나타내보면\n\\[\n\\pmb f(\\pmb x) =\n\\begin{bmatrix} f_1(\\pmb x) \\\\ f_2 (\\pmb x) \\\\ f_3(\\pmb x) \\end{bmatrix} =\n\\begin{bmatrix} x_1^2 + x_2 \\\\ \\exp (x_1) + 3 x_2 \\\\ \\sin(x_1) + x_2^3 \\end{bmatrix} =\n\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} = \\pmb y\n\\]\n이러한 경우 다변량 함수 \\(\\pmb f\\)를 벡터 \\(\\pmb x\\)로 미분하려면, 즉 미분 표기법을 이용하려면 편미분을 한 결과를 행렬의 형태를 정해야한다.\n\\[  \\pardifftwo{ \\pmb f}{\\pmb x} = (n \\times m)-\\text{matrix} \\quad \\text{ or }  \\quad (m \\times n)-\\text{matrix}? \\]\n일단 각각의 편미분 \\(\\pardifftwo{f_i}{x_j}\\)를 구해야 하며 이는 scalar 미분으로 쉽게 구해진다.\n\\[\n\\begin{aligned}\n\\pardifftwo{  f_1}{ x_1} & = 2x_1, & \\quad \\pardifftwo{  f_2}{ x_1} & = \\exp(x_1), & \\quad\n\\pardifftwo{  f_3}{ x_1} & = \\cos(x_1) \\\\\n\\pardifftwo{  f_1}{ x_2} & = 1,    & \\quad \\pardifftwo{  f_2}{ x_1} & = 3,         & \\quad\n\\pardifftwo{  f_3}{ x_1} & = 3 x_2^2 \\\\\n\\end{aligned}\n\\tag{D.2}\\]\n이제 이제 편미분값들을 행렬의 형태로 정리해보자. 편미분을 행렬에 배치할 떄 다음과 같은 규칙을 사용할 것이다.\n\n행렬의 행은 \\(\\pmb x\\)의 차원 \\(n\\) 과 같다.\n행렬의 열은 \\(\\pmb f\\)의 차원 \\(m\\) 과 같다.\n\n위와 같이 편미분을 배치하는 벡타 미분 표기법을 분모 표기법 (denominator layout)이라고 한다.\n\n\n\n\n\n\n분모 표기법\n\n\n\n\\[\n\\pmb J = \\nabla_x \\pmb x =\\pardifftwo{ \\pmb f}{\\pmb x}  \\equiv \\pardifftwo{ \\pmb f^t}{\\pmb x}\n\\underset{def}{\\equiv} \\begin{bmatrix}\n\\pardifftwo{  f_1}{ x_1} &  \\pardifftwo{  f_2}{ x_1} &  \\pardifftwo{  f_3}{ x_1}  \\\\\n\\pardifftwo{  f_1}{ x_2} &  \\pardifftwo{  f_2}{ x_2}  &  \\pardifftwo{  f_3}{ x_2}\n\\end{bmatrix}\n=  \\begin{bmatrix}\n2x_1 &  \\exp(x_1)  &  \\cos(x_1)   \\\\\n1 &  3  &  3x_2^2\n\\end{bmatrix}\n\\] \\(\\pmb J\\)는 야코비안 행렬(Jacobian matrix)이라고 부른다.\n\n\n이제 이러한 분자표기법의 특별한 결과를 알아보자\n\n\\(f: \\RR^n \\rightarrow \\RR^1\\) 인 경우\n\\(f: \\RR^n \\rightarrow \\RR^1\\) 인 경우 벡터미분 결과를 그레디언트(gradient)라고 부르며 다음과 같이 표기된다.\n\n\\[\n\\nabla_x f = \\pardifftwo{ f}{\\pmb x} =\n\\begin{bmatrix}\n\\pardifftwo{ f}{x_1} \\\\\n\\pardifftwo{ f}{x_2} \\\\\n\\vdots \\\\\n\\pardifftwo{ f}{x_n}\n\\end{bmatrix}\n\\]\n\n\\(f: \\RR^1 \\rightarrow \\RR^m\\) 인 경우\n\n\\[\n\\pardifftwo{\\pmb f}{x} =\n\\begin{bmatrix}\n\\pardifftwo{ f_1}{x} \\\\\n\\pardifftwo{ f_2}{x} \\\\\n\\vdots \\\\\n\\pardifftwo{ f_m}{x}\n\\end{bmatrix}\n\\]\n참고로 식 D.1 에서 정의한 함수 관계를 두 벡터 \\(\\pmb x\\) 와 \\(\\pmb y\\) 의 사상관계로 보면\n\\[ \\pmb f : \\pmb x \\mapsto \\pmb y \\] 다음과 같이 그레디언트 벡터를 표기할 수 있다.\n\\[  \\pardifftwo{ \\pmb f}{\\pmb x} = \\pardifftwo{ \\pmb y}{\\pmb x}\n=\n\\begin{bmatrix}\n\\pardifftwo{  y_1}{ x_1} &  \\pardifftwo{  y_2}{ x_1} &  \\pardifftwo{  y_3}{ x_1}  \\\\\n\\pardifftwo{  y_1}{ x_2} &  \\pardifftwo{  y_2}{ x_2}  &  \\pardifftwo{  y_3}{ x_2}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>벡터 미분</span>"
    ]
  },
  {
    "objectID": "qmd/math_vec_cal.html#함성함수의-미분법",
    "href": "qmd/math_vec_cal.html#함성함수의-미분법",
    "title": "부록 D — 벡터 미분",
    "section": "D.3 함성함수의 미분법",
    "text": "D.3 함성함수의 미분법\n이제 합성함수의 미분법(chain rule)에 대하여 알아보자.\n두 개의 함수\n\\[\n\\pmb g :\\RR^n \\mapsto \\RR^m, \\quad \\pmb f :\\RR^m \\mapsto \\RR^p\n\\] 가 있을 때, \\(\\pmb f\\)와 \\(\\pmb g\\)의 합성함수 \\(\\pmb h\\)는 다음과 같이 정의된다.\n\\[ \\pmb h( \\pmb x) = \\pmb f( \\pmb g( \\pmb x)) = \\pmb f \\circ \\pmb g\\] 즉,\n\\[\n\\pmb h : \\RR^n \\mapsto \\RR^m \\mapsto \\RR^p\n\\]\n이러한 합성함수의 미분은 다음과 같이 계산된다.\n\\[\n\\pardifftwo{ \\pmb h}{\\pmb x} = \\pardifftwo{ \\pmb f \\circ \\pmb g}{\\pmb x} = \\pardifftwo{ \\pmb g}{\\pmb x} \\pardifftwo{ \\pmb f}{\\pmb g}\n\\tag{D.3}\\]\n식 D.3 에서 \\(\\pardifftwo{ \\pmb f}{\\pmb g}\\)는 \\(m \\times p\\) Jacovian 벡터이고\n\\[\n\\pardifftwo{ \\pmb f}{\\pmb g}\n\\begin{bmatrix}\n\\pardifftwo{  f_1}{ g_1} &  \\pardifftwo{  f_2}{ g_1} & \\cdots &  \\pardifftwo{  f_p}{ g_1} \\\\\n\\pardifftwo{  f_1}{ g_2} &  \\pardifftwo{  f_2}{ g_2} & \\cdots &  \\pardifftwo{  f_p}{ g_2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\pardifftwo{  f_1}{ g_m} &  \\pardifftwo{  f_2}{ g_m} & \\cdots &  \\pardifftwo{  f_p}{ g_m} \\\\\n\\end{bmatrix}\n=(m \\times p)\n\\]\n\\(\\pardifftwo{ \\pmb g}{\\pmb x}\\)는 \\(n \\times m\\) Jacovian 벡터이다\n\\[\n\\pardifftwo{ \\pmb g}{\\pmb x} =\n\\begin{bmatrix}\n\\pardifftwo{  g_1}{ x_1} &  \\pardifftwo{  g_2}{ x_1} & \\cdots &  \\pardifftwo{  g_m}{ x_1} \\\\\n\\pardifftwo{  g_1}{ x_2} &  \\pardifftwo{  g_2}{ x_2} & \\cdots &  \\pardifftwo{  g_m}{ x_2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\pardifftwo{  g_1}{ x_n} &  \\pardifftwo{  g_2}{ x_n} & \\cdots &  \\pardifftwo{  g_m}{ x_n} \\\\\n\\end{bmatrix}\n= (n \\times m)\n\\]\n함성함수의 미분 공식을 차원으로 나타내면 다음과 같다.\n\\[\n\\underset{ n \\times p} {\\pardifftwo{ \\pmb h}{\\pmb x}} = \\underset{ n \\times m} {\\pardifftwo{ \\pmb g}{\\pmb x}} \\underset{ m \\times p} {\\pardifftwo{ \\pmb f}{\\pmb g}}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>벡터 미분</span>"
    ]
  },
  {
    "objectID": "qmd/math_vec_cal.html#두-벡터-내적의-미분",
    "href": "qmd/math_vec_cal.html#두-벡터-내적의-미분",
    "title": "부록 D — 벡터 미분",
    "section": "D.4 두 벡터 내적의 미분",
    "text": "D.4 두 벡터 내적의 미분\n\nD.4.1 상수벡터와 변수벡터의 내적\n먼저 상수 벡터 \\(\\pmb a\\)와 변수 벡터 \\(\\pmb x\\)의 내적의 미분을 생각해 보자.\n참고로 다음과 같이 두 벡터의 내적은 스칼라이다.\n\\[ \\pmb a^t \\pmb x = \\pmb x^t \\pmb a = a_1 x_1 + a_2x_2 + \\dots + a_n x_n \\]\n따라서 그레이디언트를 구하는 방법과 같이 결과는 열벡터로 표기된다.\n\\[\n\\pardifftwo{ \\pmb a^t \\pmb x}{\\pmb x} = \\pardifftwo{ \\pmb x^t \\pmb a}{\\pmb x} = \\pmb a =\n\\begin{bmatrix}\na_1 \\\\\na_2\\\\\n\\vdots \\\\\na_n\n\\end{bmatrix}\n\\]\n위의 식에서 상수벡터 \\(\\pmb a\\)는 가 전치로 앞에 나타나는 표현 \\(\\pmb x^t \\pmb a\\) 를 사용하면 결과 벡터 \\(\\pmb a\\)가 열벡터로 그대로 나타나지므로 내적의 미분 표기로 사용할 것이다.\n\\[\n\\pardifftwo{ \\pmb x^t \\pmb a}{\\pmb x} =  \\pardifftwo{  \\pmb x^t}{\\pmb x} \\pmb a =\\pmb I  \\pmb a = \\pmb a\n\\tag{D.4}\\]\n\n\nD.4.2 상수벡터와 함수벡터의 내적\n더 나아가서 상수 벡터 \\(\\pmb a\\)와 함수 벡터 \\(\\pmb f\\)의 내적의 미분도 식 D.4 을 표시하는 동일한 논리로 다음과 같이 표기할 수 있다.\n\\[\n\\pardifftwo{ \\pmb f^t \\pmb a}{\\pmb x} =  \\pardifftwo{  \\pmb f^t}{\\pmb x} \\pmb a\n\\tag{D.5}\\]\n참고로 식 D.5 에서 \\(\\pardifftwo{  \\pmb f}{\\pmb x}\\)는 행벡터가 아닌 행렬로 나타날 수 있다.\n\n\nD.4.3 함수벡터와 함수벡터의 내적\n이제 다음과 같이 같은 공간으로 사상되는 두 함수 \\(\\pmb f\\) 와 \\(\\pmb g\\) 의 내적을 생각해 보자.\n\\[ \\pmb f : \\RR^n \\mapsto \\RR^m, \\quad \\pmb g : \\RR^n \\mapsto \\RR^m\\]\n두 함수의 내적을 미분하는 경우 곱셉 법칙을 적용하여야 하는데 행렬의 곱셉에서는 교환법칙이 성립되지 않으므로 순서에 주의해야 한다.\n내적 \\(\\pmb f^t \\pmb g\\) 를 각각 따로 미분해야 하는데 각 벡터에 대해 따로 미분을 실행해 보자\n\n\\(\\pmb f\\) 를 미분하는 경우 \\(\\pmb g\\) 는 상수 벡터 \\(\\pmb a\\) 로 취급한다. 그리고 식 D.5 를 적용한다.\n\n\\[\n\\pardifftwo{ \\pmb f^t \\pmb g}{\\pmb x} = \\pardifftwo{ \\pmb f^t \\pmb a}{\\pmb x} =\n=  \\pardifftwo{ \\pmb f^t}{\\pmb x} \\pmb a=\n\\pardifftwo{ \\pmb f}{\\pmb x} \\pmb g\n\\tag{D.6}\\]\n\n\\(\\pmb g\\) 를 미분하는 경우 \\(\\pmb f\\) 는 상수 벡터 \\(\\pmb a\\) 로 취급한다. 그리고 식 D.5 를 적용한다.\n\n\\[\n\\pardifftwo{ \\pmb f^t \\pmb g}{\\pmb x} = \\pardifftwo{ \\pmb a^t \\pmb g}{\\pmb x} =\n\\pardifftwo{ \\pmb g^t \\pmb a}{\\pmb x} =\n\\pardifftwo{ \\pmb g^t}{\\pmb x}  \\pmb a =\n\\pardifftwo{ \\pmb g}{\\pmb x} \\pmb f\n\\tag{D.7}\\]\n이제 위의 두 결과 식 D.6 과 식 D.7 를 합치면 다음과 같은 최종적인 결과를 얻을 수 있다.\n\\[\n\\pardifftwo{ \\pmb f^t \\pmb g}{\\pmb x} =  \\pardifftwo{ \\pmb f}{\\pmb x} \\pmb g +  \\pardifftwo{ \\pmb g}{\\pmb x} \\pmb f\n\\tag{D.8}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>벡터 미분</span>"
    ]
  },
  {
    "objectID": "qmd/math_vec_cal.html#벡터-미분의-응용",
    "href": "qmd/math_vec_cal.html#벡터-미분의-응용",
    "title": "부록 D — 벡터 미분",
    "section": "D.5 벡터 미분의 응용",
    "text": "D.5 벡터 미분의 응용\n\nD.5.1 선형사상의 미분상\n이제 앞에서 배운 벡터의 미분을 이용하여 유용한 응용 공식을 유도해보자.\n먼저 선형변환 \\(\\pmb y = \\pmb A \\pmb x\\) 를 생각해 보자. 이때 (\\(M \\times N\\))-\\(\\pmb A\\)는 상수 행렬이다. 이때 \\(\\pmb y\\)를 \\(\\pmb x\\)로 미분하면 다음과 같다.\n먼저 행렬 \\(\\pmb A\\)의 \\(i\\) 번째 행을 \\(\\pmb a_i^t\\)라고 하면\n\\[\n\\pmb A = \\begin{bmatrix}\nA_{11} & A_{12} & \\dots & A_{1N} \\\\\nA_{21} & A_{22} & \\dots & A_{2N} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{M1} & A_{M2} & \\dots & A_{MN} \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\pmb a_1^t  \\\\\n\\pmb a_2^t  \\\\\n\\vdots \\\\\n\\pmb a_M^t  \\\\\n\\end{bmatrix}\n\\]\n선형변환 \\(\\pmb f(\\pmb x) = \\pmb A \\pmb x\\) 로 정의하면 다음과 같이 나타낼 수 있다.\n\\[\n\\pmb A \\pmb x=\n\\begin{bmatrix}\n\\pmb a_1^t \\pmb x \\\\\n\\pmb a_2^t \\pmb x \\\\\n\\vdots \\\\\n\\pmb a_M^t \\pmb x \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nf_1(\\pmb x) \\\\\nf_2(\\pmb x) \\\\\n\\vdots \\\\\nf_M(\\pmb x) \\\\\n\\end{bmatrix}\n= \\pmb f(\\pmb x)\n\\]\n따라서\n\\[\n\\pardifftwo{\\pmb A \\pmb x}{\\pmb x}   = \\pardifftwo{\\pmb f (\\pmb x)}{\\pmb x} =\n\\begin{bmatrix}\n\\pardifftwo{f_1}{x_1} & \\pardifftwo{f_2}{x_1} & \\dots & \\pardifftwo{f_M}{x_1} \\\\\n\\pardifftwo{f_1}{x_2} & \\pardifftwo{f_2}{x_2} & \\dots & \\pardifftwo{f_M}{x_2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\pardifftwo{f_1}{x_N} & \\pardifftwo{f_2}{x_N} & \\dots & \\pardifftwo{f_M}{x_N} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nA_{11} & A_{21} & \\dots & A_{M1} \\\\\nA_{12} & A_{22} & \\dots & A_{M2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{1N} & A_{2N} & \\dots & A_{MN} \\\\\n\\end{bmatrix} = \\pmb A^t\n\\] 따라서 선형사상의 미분은 선형변환 행렬의 전치이다.\n\\[ \\pardifftwo{\\pmb A \\pmb x}{\\pmb x}  = \\pmb A^t  \\tag{D.9}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>벡터 미분</span>"
    ]
  },
  {
    "objectID": "qmd/multivar.html",
    "href": "qmd/multivar.html",
    "title": "부록 E — 다변량 확률변수의 성질",
    "section": "",
    "text": "E.1 일변량분포\n일변량 확률변수 \\(X\\)가 확률밀도함수 \\(f(x)\\)를 가지는 분포를 따를때 기대값과 분산은 다음과 같이 정의된다.\n\\[ E(X) = \\int x f(x)  dx = \\mu, \\quad V(X) = E[ X-E(X)]^2=\\int (x-\\mu)^2 f(x) dx =\\sigma^2 \\]\n새로운 확률변수 \\(Y\\)가 확률변수 \\(X\\)의 선형변환으로 표시된다면 (\\(a\\)와 \\(b\\)는 실수)\n\\[ Y = aX+b\\]\n그 기대값(평균)과 분산은 다음과 같이 계산된다.\n\\[\n\\begin{aligned}\nE(Y) &= E(aX+b) \\\\\n&= \\int (ax+b) f(x) dx \\\\\n&= a \\int x f(x) dx + b \\\\\n&= a E(X) + b\\\\\n&= a \\mu + b \\\\\nV(Y) &= Var(aX+b) \\\\\n&= E[aX+b -E(aX+b)]^2 \\\\\n&= E[a(X-\\mu)]^2 \\\\\n&= a^2 E(X-\\mu)^2\\\\\n&= a^2 \\sigma^2\n\\end{aligned}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>다변량 확률변수의 성질</span>"
    ]
  },
  {
    "objectID": "qmd/multivar.html#확률벡터와-분포",
    "href": "qmd/multivar.html#확률벡터와-분포",
    "title": "부록 E — 다변량 확률변수의 성질",
    "section": "E.2 확률벡터와 분포",
    "text": "E.2 확률벡터와 분포\n확률벡터 \\(\\pmb X\\)가 \\(p\\) 차원의 다변량분포를 따른다고 하고 결합확률밀도함수 \\(f(\\pmb x) =f(x_1,x_2,\\dots,x_p)\\)를 를 가진다고 하자.\n\\[\n\\pmb X =\n  \\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n..  \\\\\nX_p\n\\end{bmatrix}\n\\]\n다변량 확률벡터의 기대값(평균벡터)과 공분산(행렬)은 다음과 같이 계산된다.\n\\[\n\\pmb E(\\pmb X) =\n  \\begin{bmatrix}\nE(X_1) \\\\\nE(X_2) \\\\\nE(X_3) \\\\\n..  \\\\\nE(X_p)\n\\end{bmatrix}\n=\n  \\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n..  \\\\\n\\mu_p\n\\end{bmatrix}\n=\\pmb \\mu\n\\]\n\\[\nV(\\pmb X) =Cov(\\pmb X) = E (\\pmb X-\\pmb \\mu) (\\pmb X-\\pmb \\mu)^t\n=\n  \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p} \\\\\n\\sigma_{12} & \\sigma_{22} & \\dots & \\sigma_{2p} \\\\\n& \\dots & \\dots & \\\\\n\\sigma_{1p} & \\sigma_{2p} & \\dots & \\sigma_{pp} \\\\\n\\end{bmatrix}\n= \\pmb \\Sigma\n\\]\n여기서 \\(\\sigma_{ii}=V(X_i)\\), \\(\\sigma_{ij} = Cov(X_i, X_j)=Cov(X_j, X_i)\\)이다. 따라서 공분산 행렬 \\(\\pmb \\Sigma\\)는 대칭행렬(symmetric matrix)이다. 다음 공식은 유용한 공식이다.\n\\[ \\pmb \\Sigma = E (\\pmb X-\\pmb \\mu) (\\pmb X-\\pmb \\mu)^t  = E(\\pmb X \\pmb X^t)-\\pmb \\mu \\pmb \\mu^t \\]\n두 확률변수의 상관계수 \\(\\rho_{ij}\\)는 다음과 같이 정의된다.\n\\[ \\rho_{ij} = \\frac{Cov(X_i, X_j)}{ \\sqrt{V(X_i) V(X_j)}} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\n  \\sigma_{jj}}} \\]\n새로운 확률벡터 \\(\\pmb Y\\)가 확률벡터 \\(\\pmb X\\) 의 선형변환라고 하자.\n\\[ \\pmb Y = \\pmb A  \\pmb X + \\pmb b \\]\n단 여기서 \\(\\pmb A = \\{ a_{ij} \\}\\)는 \\(p \\times p\\) 실수 행렬이고 \\(\\pmb b =(b_1 b_2 \\dots b_p)^t\\)는 \\(p \\times 1\\) 실수 벡터이다.\n확률벡터 \\(\\pmb Y\\)의 기대값(평균벡터)과 공분산은 다음과 같이 계산된다.\n\\[\n\\begin{aligned}\nE(\\pmb Y ) &= E(\\pmb A \\pmb X+ \\pmb b) \\\\\n&= \\pmb A E(\\pmb X)+ \\pmb b \\\\\n&= \\pmb A \\pmb \\mu+ \\pmb b \\\\\nV(\\pmb Y) &= Var(\\pmb A \\pmb X+ \\pmb b) \\\\\n&= E[\\pmb A \\pmb X+ \\pmb b -E(\\pmb A \\pmb X+ \\pmb b)] [\\pmb A \\pmb X+ \\pmb b -E(\\pmb A \\pmb X+ \\pmb b)]^t \\\\\n&= E[\\pmb A \\pmb X -  \\pmb A \\pmb \\mu] [\\pmb A \\pmb X -  \\pmb A \\pmb \\mu]^t \\\\\n&= E[\\pmb A (\\pmb X - \\pmb \\mu)] [\\pmb A (\\pmb X - \\pmb \\mu)]^t \\\\\n&= \\pmb A E [(\\pmb X - \\pmb \\mu) (\\pmb X - \\pmb \\mu)^t] \\pmb A^t \\\\\n&= \\pmb A \\pmb \\Sigma \\pmb A^t\n\\end{aligned}\n\\]\n만약 표본 \\(\\pmb X_i, \\pmb X_2, \\dots, \\pmb X_n\\) 이 독립적으로 평균이 \\(\\pmb \\mu\\) 이고 공분산이 \\(\\pmb \\Sigma\\) 인 분포에서 추출되었다면 표본의 평균벡터 \\(\\bar {\\pmb  X}\\) 는 평균이 \\(\\pmb \\mu\\) 이고 공분산이 \\(\\frac{1}{n}\\pmb \\Sigma\\) 인 분포를 따른다.\n\\[\n\\bar {\\pmb X} =\n  \\begin{bmatrix}\n\\sum_{i=1}^n X_{i1} / n  \\\\\n\\sum_{i=1}^n X_{i2} / n \\\\\n\\sum_{i=1}^n X_{i3} / n \\\\\n..  \\\\\n\\sum_{i=1}^n X_{ip} / n\n\\end{bmatrix}\n\\]\n여기서 \\(X_{ij}\\) 는 \\(i\\)번째 표본벡터 \\(\\pmb X_i =(X_{i1} X_{i2} \\dots X_{ip})^t\\)의 \\(j\\)번째 확률변수이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>다변량 확률변수의 성질</span>"
    ]
  },
  {
    "objectID": "qmd/multivar.html#다변량-정규분포",
    "href": "qmd/multivar.html#다변량-정규분포",
    "title": "부록 E — 다변량 확률변수의 성질",
    "section": "E.3 다변량 정규분포",
    "text": "E.3 다변량 정규분포\n일변량 확률변수 \\(X\\)가 평균이 \\(\\mu\\) 이고 분산이 \\(\\sigma^2\\)인 정규분포를 따른다면 다음과 같이 나타내고 \\[ X \\sim N(\\mu, \\sigma^2 ) \\] 확률밀도함수 \\(f(x)\\) 는 다음과 갇이 주어진다.\n\\[ f(x) = (2 \\pi \\sigma^2)^{-1/2} \\exp \\left ( - \\frac{(x-\\mu)^2}{2} \\right ) \\]\n\\(p\\)-차원 확률벡터 \\(\\pmb X\\)가 평균이 \\(\\pmb \\mu\\) 이고 공분산이 \\(\\pmb \\Sigma\\)인 다변량 정규분포를 따른다면 다음과 같이 나타내고 \\[ \\pmb X \\sim N_p(\\pmb \\mu, \\pmb \\Sigma ) \\] 확률밀도함수 \\(f(\\pmb x)\\) 는 다음과 갇이 주어진다.\n\\[ f(\\pmb x) = (2 \\pi)^{-p/2} | \\pmb \\Sigma|^{-1/2}\n   \\exp \\left ( - \\frac{(\\pmb x-\\pmb \\mu) \\pmb \\Sigma^{-1}(\\pmb x-\\pmb \\mu)^t}{2} \\right ) \\]\n다변량 정규분포 \\(N(\\pmb \\mu, \\pmb \\Sigma)\\)를 따르는 확률벡터 \\(\\pmb X\\)를 다음과 같이 두 부분으로 나누면\n\\[\n  \\pmb X =\n    \\begin{bmatrix}\n  \\pmb X_1 \\\\\n  \\pmb X_2\n  \\end{bmatrix}, \\quad\n  \\pmb X_1 =\n    \\begin{bmatrix}\n  \\pmb X_{11} \\\\\n  \\pmb X_{12} \\\\\n  \\pmb \\vdots \\\\\n  \\pmb X_{1p}\n  \\end{bmatrix}, \\quad\n  \\pmb X_2=\n    \\begin{bmatrix}\n  \\pmb X_{21} \\\\\n  \\pmb X_{22} \\\\\n  \\pmb \\vdots \\\\\n  \\pmb X_{2q}\n  \\end{bmatrix}\n  \\]\n각각 다변량 정규분포를 따르고 다음과 같이 나타낼 수 있다.\n\\[\n  \\begin{bmatrix}\n  E(\\pmb X_1) \\\\\n  E(\\pmb X_2)\n  \\end{bmatrix}\n  =\n    \\begin{bmatrix}\n  \\pmb \\mu_1 \\\\\n  \\pmb \\mu_2\n  \\end{bmatrix}\n  , \\quad\n  \\begin{bmatrix}\n  V(\\pmb X_1) & Cov(\\pmb X_1, X_2) \\\\\n  Cov(\\pmb X_2 X_1) & V(\\pmb X_2)\n  \\end{bmatrix}\n  =\n    \\begin{bmatrix}\n  \\pmb \\Sigma_{11} & \\pmb \\Sigma_{12} \\\\\n  \\pmb \\Sigma^t_{12} & \\pmb \\Sigma_{22}\n  \\end{bmatrix}\n  \\]\n\\[  \\pmb X =\n    \\begin{bmatrix}\n  \\pmb X_1 \\\\\n  \\pmb X_2\n  \\end{bmatrix}\n  \\sim\n  N_{p+q} \\left (\n    \\begin{bmatrix}\n    \\pmb \\mu_1 \\\\\n    \\pmb \\mu_2\n    \\end{bmatrix}\n    ,\\begin{bmatrix}\n    \\pmb \\Sigma_{11} & \\Sigma_{12} \\\\\n    \\pmb \\Sigma^t_{12} & \\Sigma_{22}\n    \\end{bmatrix}\n    \\right )\n  \\]\n확률벡터 \\(\\pmb X_2 = \\pmb x_2\\)가 주어진 경우 \\(\\pmb X_1\\)의 조건부 분포는 \\(p\\)-차원 다변량 정규분포를 따르고 평균과 공분산은 다음과 같다.\n\\[\n  E(\\pmb X_1 | \\pmb X_2 = \\pmb x_2 ) = \\pmb \\mu_1 + \\pmb \\Sigma_{12} \\pmb \\Sigma^{-1}_{22} (\\pmb \\mu_2 - \\pmb x_2), \\quad\n  V(\\pmb X_1 | \\pmb X_2 = \\pmb x_2 )  = \\pmb \\Sigma_{11} -\\pmb \\Sigma_{12} \\pmb \\Sigma^{-1}_{22} \\pmb \\Sigma^t_{12}\n  \\]\n예를 들어 \\(2\\)-차원 확률벡터 \\(\\pmb X=(X_1, X_2)^t\\)가 평균이 \\(\\pmb \\mu=(\\mu_1,\\mu_2)^t\\) 이고 공분산 \\(\\pmb \\Sigma\\)가 다음과 같이 주어진\n\\[\n\\pmb \\Sigma =\n  \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_{22}\n\\end{bmatrix}\n\\]\n이변량 정규분포를 따른다면 확률밀도함수 \\(f(\\pmb x)\\)에서 \\(\\exp\\)함수의 인자는 다음과 같이 주어진다. \\[\n\\begin{aligned}\n&(\\pmb x-\\pmb \\mu) \\pmb \\Sigma^{-1}(\\pmb x-\\pmb \\mu)^t\n= \\\\\n&-\\frac{1}{2 (1-\\rho^2)}\n\\left [\n  \\left ( \\frac{(x_1-\\mu_1)^2}{\\sigma_{11}} \\right )\n  +\\left ( \\frac{(x_2-\\mu_2)^2}{\\sigma_{22}} \\right )\n  -2 \\rho \\left ( \\frac{(x_1-\\mu_1)}{\\sqrt{\\sigma_{11}}} \\right )\n  \\left ( \\frac{(x_2-\\mu_2)}{\\sqrt{\\sigma_{22}}} \\right )\n  \\right ]\n\\end{aligned}\n\\]\n그리고 \\(p=2\\)인 경우 확률밀도함수의 상수부분은 다음과 같이 주어진다.\n\\[ (2 \\pi)^{-p/2} | \\pmb \\Sigma|^{-1/2} = \\frac{1}{ 2 \\pi \\sqrt{\\sigma_{11} \\sigma_{22} (1-\\rho^2)}} \\]\n여기서 \\(\\rho = \\sigma_{12} / \\sqrt{\\sigma_{11} \\sigma_{22}}\\)\n만약 \\(X_2 = x_2\\)가 주어졌을 때 \\(X_1\\)의 조건부 분포는 정규분포이고 평균과 분산은 다음과 같이 주어진다.\n\\[\n  E( X_1 |  X_2 =  x_2 ) =  \\mu_1 +  \\frac{\\sigma_{12}}{\\sigma_{22}} ( \\mu_2 -  x_2)  = \\mu_1 +  \\rho \\frac{\\sqrt{\\sigma_{11}}}{\\sqrt{\\sigma_{22}}} ( \\mu_2 -  x_2) \\]\n\\[\n  V( X_1 |  X_2 =  x_2 )  =  \\sigma_{11} - \\frac{\\sigma^2_{12}}{\\sigma_{22}}  = \\sigma_{11}(1-\\rho^2)\n\\]\n다변량 정규분포에서 공분산이 0인 두 확률 변수는 독립이다. \\[ \\sigma_{ij} = 0 \\leftrightarrow X_i \\text{ and } X_j \\text{ are independent} \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>다변량 확률변수의 성질</span>"
    ]
  },
  {
    "objectID": "qmd/multivar.html#표준정규분포로의-변환",
    "href": "qmd/multivar.html#표준정규분포로의-변환",
    "title": "부록 E — 다변량 확률변수의 성질",
    "section": "E.4 표준정규분포로의 변환",
    "text": "E.4 표준정규분포로의 변환\n일변량 확률변수 \\(X\\)가 평균이 \\(\\mu\\) 이고 분산이 \\(\\sigma^2\\)인 경우 다음과 같은 선형변환을 고려하면.\n\\[ Z = \\frac{X - \\mu}{\\sigma} = (\\sigma^2)^{-1/2} (X-\\mu) \\] 확률변수 \\(Z\\) 는 평균이 \\(0\\) 이고 분산이 \\(1\\)인 분포를 따른다.\n\\(p\\)차원 확률벡터 \\(\\pmb X\\) 가 평균이 \\(\\pmb \\mu\\) 이고 공분산이 \\(\\pmb \\Sigma\\)인 분포를 가진다고 가정하자. 공분산 행렬 \\(\\pmb \\Sigma\\)는 양정치 행렬(positive definite matrix)이며 다음과 같은 행렬의 분해가 가능하다.\n\\[ \\Sigma = \\pmb C \\pmb C^t \\]\n여기서 \\(\\pmb C\\) 는 정칙행렬이며 역행렬 \\(\\pmb C^{-1}\\)가 존재한다. 위와 같은 행렬의 분해는 스펙트럴 분해(spectral decomposition)을 이용하여 구할 수 있다. 공분산 행렬 \\(\\pmb \\Sigma\\)는 양정치 행렬이므로 고유치(eigen value) \\((\\lambda_1, \\lambda_2,\\dots, \\lambda_p)\\)가 모두 양수이고 정규직교 고유벡터(orthonormal eigen vector)의 행렬 \\(\\pmb P\\)을 이용하여 다음과 같은 분해가 가능하다.\n\\[ \\Sigma = \\pmb P \\pmb \\Lambda \\pmb P^t = \\pmb P \\pmb \\Lambda^{1/2} \\Lambda^{1/2} \\pmb P^t \\]\n여기서 \\(\\pmb \\Lambda\\)는 고유치 \\((\\lambda_1, \\lambda_2,\\dots, \\lambda_p)\\)를 대각원소로 가지는 대각행렬이며 \\(\\pmb \\Lambda^{1/2}\\)는 고유치의 제곱근을 대각원소로 가지는 대각행렬이다. 따라서 \\(\\pmb C = \\pmb P \\pmb \\Lambda^{1/2}\\)로 하면 위와 같은 행렬의 분해가 가능하다. 정규직교 고유벡터(orthonormal eigen vector)의 행렬 \\(\\pmb P\\)는 직교행렬이므로\n\\[ \\pmb C^{-1} =  (\\pmb P \\pmb \\Lambda^{1/2})^{-1} = \\pmb \\Lambda^{-1/2} \\pmb P^t \\]\n\\(p\\)차원 확률벡터 \\(\\pmb X\\)의 다음과 같은 선형변환을 고려하면. \\[ \\pmb Z = \\pmb C^{-1} ( \\pmb X- \\pmb \\mu) = \\pmb \\Lambda^{-1/2} \\pmb P^t ( \\pmb X- \\pmb \\mu)  \\] 확률벡터 \\(\\pmb Z\\) 는 평균이 \\(\\pmb 0\\) 이고 공분산이 \\(\\pmb I\\)인 분포를 따른다 (why?).\n확률벡터 \\(\\pmb X\\)가 정규분포를 따른다면 선형변환한 확률벡터 \\(\\pmb Z\\)도 정규분포를 따른다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>다변량 확률변수의 성질</span>"
    ]
  },
  {
    "objectID": "qmd/multivar.html#예제",
    "href": "qmd/multivar.html#예제",
    "title": "부록 E — 다변량 확률변수의 성질",
    "section": "E.5 예제",
    "text": "E.5 예제\n예를 들어 이변량확률벡터 \\(\\pmb X\\)가 다음과 같은 평균벡터와 공분산을 가진 정규분포를 따른다고 하자\n\\[\n\\pmb \\mu =\n  \\begin{bmatrix}\n1\\\\\n2\n\\end{bmatrix}\n\\quad\n\\pmb \\Sigma =\n  \\begin{bmatrix}\n2 & 1\\\\\n1 & 2\n\\end{bmatrix}\n\\]\n공분산행렬 \\(\\pmb \\sigma\\)의 고유치는 \\(|\\pmb \\sigma -\\lambda \\pmb I|=0\\)의 방정식을 풀어 구할 수 있다.\n\\[\n|\\pmb \\sigma -\\lambda \\pmb I|  =\n  \\begin{bmatrix}\n2-\\lambda & 1\\\\\n1 & 2-\\lambda\n\\end{bmatrix}\n= \\lambda^2 -4 \\lambda +3=0\n\\]\n방정식을 풀면 고유치는 \\((\\lambda_1, \\lambda_2) = (3,1)\\)이다. 각 고유치에 대한 고유벡터 \\(\\pmb p=(p_1, p_2)^t\\)는 \\(\\pmb \\Sigma \\pmb p = \\lambda \\pmb p\\) 으로 구할 수 있다. 각 고유치에 대하여 방정식을 구하면 다음 두 개의 방정식을 얻을 수 있다.\n\\[\np_1 - p_2 = 1 \\text{ and } p_1 + p_2 = 0\n\\]\n정규직교 벡터의 조건을 만족 시키기 위해서 \\(p^2_1 + p^2_2=1\\)의 조건을 적용하면 다음과 같은 정규직교 고유행렬을 얻을 수 있다.\n\\[\n\\pmb P =\n  \\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\\\\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\]\n또한\n\\[\n\\pmb \\Lambda =\n  \\begin{bmatrix}\n3 & 0\\\\\n0 & 1\n\\end{bmatrix}\n\\quad\n\\pmb \\Lambda^{1/2} =\n  \\begin{bmatrix}\n\\sqrt{3} & 0\\\\\n0 & 1\n\\end{bmatrix}\n\\]\n따라서 \\(C^{-1} =  \\Lambda^{-1/2} \\pmb P^t\\) 이며\n\\[\n\\pmb C^{-1} =\n  \\pmb \\Lambda^{-1/2} \\pmb P^t =\n  \\begin{bmatrix}\n\\frac{1}{\\sqrt{3}} & 0\\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n=\n  \\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{6}}\\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>다변량 확률변수의 성질</span>"
    ]
  },
  {
    "objectID": "qmd/quadratic.html",
    "href": "qmd/quadratic.html",
    "title": "부록 F — 이차형식과 제곱합의 분포",
    "section": "",
    "text": "F.1 이차형식\n\\(n\\)-차원 벡터 \\({\\pmb  x}^t=[x_1,x_2,\\dots,x_n]\\)과 대칭행렬 \\(\\pmb  A\\)에 대하여 이차형식(quadratic form)은 다음과 같이 정의된다.\n\\[\nQ_A(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x =\\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j\n\\tag{F.1}\\]\n이차형식의 정의에서 반드시 행렬 \\(\\pmb  A\\)를 대칭행렬로 정의하지 않아도 되지만 임의의 행렬에 대하여 이차형식의 값이 동일한 대칭행렬이 존재하기 때문에 정의에서 이차형식으로 국한하는 것이 일반적이다.\n정칙행렬 \\(\\pmb  B\\)에 대하여 다음과 같은 선형변환을 고려하자.\n\\[   \\pmb  x = \\pmb  B \\pmb  y \\quad \\text{ or } \\quad \\pmb  y = \\pmb  {B}^{-1} \\pmb  x \\]\n벡터 \\(\\pmb  x\\)로 정의된 이차형식은 벡터 \\(\\pmb  y\\)의 형태로 다음과 같이 변환할 수 있다.\n\\[\nQ(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x = \\pmb  y^t \\pmb  B^t \\pmb  A \\pmb  B \\pmb  y =Q^*(\\pmb  y)\n\\]\n이차형식의 성질은 정칙 선형변환에서 유지된다. 즉 행렬 \\(\\pmb  A\\)가 양(반)정치 행렬이고 행렬 \\(\\pmb  B\\)가 정칙행렬이면 행렬 \\(\\pmb  B^t \\pmb  A \\pmb  B\\)도 양(반)정치 행렬이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>이차형식과 제곱합의 분포</span>"
    ]
  },
  {
    "objectID": "qmd/quadratic.html#이차형식",
    "href": "qmd/quadratic.html#이차형식",
    "title": "부록 F — 이차형식과 제곱합의 분포",
    "section": "",
    "text": "정의 F.1 (양정치 행렬) 이차형식 \\(Q_A(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x\\)가 영벡터가 아닌 모든 벡터 \\(\\pmb  x\\)에 대하여 0 보다 크면, 즉\n\\[ \\pmb  x^t \\pmb  A \\pmb  x  &gt;0  \\quad \\text{ for all } \\quad \\pmb x \\in \\RR^n\\]\n\\(\\pmb  A\\)를 양정치(positive definite)라고 부른다.\n만약 이차형식 \\(Q_A(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x\\)가 영벡터가 아닌 모든 벡터 \\(\\pmb  x\\)에 대하여 0 보다 크거나 같다면\n\\[ \\pmb  x^t \\pmb  A \\pmb  x  \\ge 0 \\quad \\text{ for all } \\quad \\pmb x \\in \\RR^n\\]\n\\(\\pmb  A\\)를 양반정치(positive semi-definite)라고 부른다.\n\\(\\blacksquare\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>이차형식과 제곱합의 분포</span>"
    ]
  },
  {
    "objectID": "qmd/quadratic.html#대칭행렬의-대각화",
    "href": "qmd/quadratic.html#대칭행렬의-대각화",
    "title": "부록 F — 이차형식과 제곱합의 분포",
    "section": "F.2 대칭행렬의 대각화",
    "text": "F.2 대칭행렬의 대각화\n\\(n\\)차원 대칭행렬 \\(\\pmb  A\\) 에 대하여 직교행렬 \\(\\pmb  P\\)가 존재하여 다음과 같은 분해가 가능하다.\n\\[\n\\pmb  P^t \\pmb  A \\pmb  P = \\pmb  \\Lambda = diag(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)\n\\tag{F.2}\\]\n식 F.2 의 분해에서 \\(\\lambda_i\\)는 행렬 \\(\\pmb  A\\)의 고유치이며 행렬 \\(\\pmb  P\\)의 \\(i\\) 번째 열은 대응하는 고유벡터 \\(\\pmb  p_i\\) 로 구성되어 있다.\n\\[\n\\pmb  P = [ \\pmb  p_1~~ \\pmb  p_2 ~~ \\dots ~~ \\pmb  p_n ]\n\\] 이제 위의 분해를 증명해 보자. 고유치 \\(\\lambda_i\\) 와 대응하는 고유벡터 \\(\\pmb  p_i\\)의 정의에 따라서 다음과 같은 \\(n\\)개의 식을 얻을 수 있고\n\\[ \\pmb  A \\pmb  p_i = \\lambda_i \\pmb  p_i , \\quad i=1,2,3\\dots, n \\]\n위의 식을을 합쳐서 표기하면 다음과 같은 식을 얻으며 이는 식 F.2 를 의미한다.\n\\[ \\pmb  A \\pmb  P = \\pmb  P \\pmb  \\Lambda \\]\n식 F.2 를 다시 쓰면 다음과 같은 스펙트럴 분해(spectral decomposition)를 얻는다.\n\\[\n\\pmb  A  = \\pmb  P \\pmb  \\Lambda \\pmb  P^t  = \\sum_{i=1}^n \\lambda_i \\pmb  p_i \\pmb  {p}_i^t\n\\tag{F.3}\\]\n참고로 대각합과 행렬식에 대한 고유치위 관계를 나타내는 다음의 유용한 두 식을 반드시 기억하자.\n\\[ tr(\\pmb  A) = \\sum_i \\lambda_i ,\\quad |\\pmb  A| = \\prod_i \\lambda_i \\]\n대칭행렬의 분해 식 F.2 를 이용하면 다음과 같은 이차형식의 분해를 얻을 수 있다.\n\\[\nQ(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x = \\pmb  x^t \\pmb  P \\pmb  \\Lambda \\pmb  P^t \\pmb  x = \\pmb  y^t \\pmb  \\Lambda \\pmb  y= \\sum_{i=1}^n \\lambda_i y_i^2\n\\tag{F.4}\\]\n위의 식에서\n\\[ \\pmb  y = \\pmb  P^t \\pmb  x \\]\n이차형식의 분해식 식 F.4 를 보면 행렬 \\(\\pmb  A\\)의 모든 고유치가 0보다 크면 양정치 임을 알 수 있다. 또한 모든 고유치가 0보다 크거나 같으면 양반정치 임을 알 수 있다.\n또한 \\(rank(\\pmb  A) = rank(\\pmb  \\Lambda)\\)이며 이는 0이 아닌 고유치의 개수가 행렬 \\(\\pmb  A\\)의 계수(rank)임을 알 수 있다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>이차형식과 제곱합의 분포</span>"
    ]
  },
  {
    "objectID": "qmd/quadratic.html#멱등행렬",
    "href": "qmd/quadratic.html#멱등행렬",
    "title": "부록 F — 이차형식과 제곱합의 분포",
    "section": "F.3 멱등행렬",
    "text": "F.3 멱등행렬\n\\(n\\)-차원 행렬 \\(\\pmb  A\\) 가 다음과 같은 성질을 가지면 멱등행렬(idenpotent matrix)라고 부른다.\n\\[ \\pmb  A^2 = \\pmb  A \\pmb  A = \\pmb  A \\]\n멱등행렬은 다음과 같은 성질을 가지고 있다.\n\n멱등행렬의 고유치는 0 또는 1이다.\n멱등행렬은 대각합이 계수와 같다.\n\n\\[ tr(\\pmb  A) =rank(\\pmb  A) \\]\n\n멱등행렬은 양반정치 행렬이다.\n\\(\\pmb  A\\) 멱등행렬이면 \\(\\pmb  I - \\pmb  A\\)도 멱등행렬이다.\n\n특별히 대칭인 멱등행렬을 사영행렬(또는 투영행렬, projection matrix)라고 부른다.\n최소제곱법에서 식 B.11 에서 나타난 행렬 \\(\\pmb  H = \\pmb  X (\\pmb  X^t \\pmb  X)^{-1} \\pmb  X^t\\)는 멱등행렬이며 따라서 사영행렬이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>이차형식과 제곱합의 분포</span>"
    ]
  },
  {
    "objectID": "qmd/quadratic.html#이차형식의-분포",
    "href": "qmd/quadratic.html#이차형식의-분포",
    "title": "부록 F — 이차형식과 제곱합의 분포",
    "section": "F.4 이차형식의 분포",
    "text": "F.4 이차형식의 분포\n\nF.4.1 카이제곱 분포\n만약 확률변수 \\(x\\) 이 표준 정규분포 \\(N(0,1)\\) 을 따른다면 \\(y=x^2\\) 은 자유도가 1인 카이제곱 분포 \\(\\chi^2_1\\)를 따른다. 더 나아가 \\(n\\) 개의 확률변수 \\(x_1,x_2,\\dots,x_n\\) 이 서로 독립이고 표준 정규분포 \\(N(0,1)\\) 을 따른다면 제곱합 \\(v=x_1^2 +x_2^2 +\\cdots + x_n^2\\)은 자유도가 n인 카이제곱 분포 \\(\\chi^2_n\\)를 따른다.\n이렇게 카이제곱 분포는 표준 정규분포를 따르는 서로 독립인 확률변수들의 제곱값에 대한 분포이다.\n\n\nF.4.2 비중심 카이제곱 분포\n만약 확률변수 \\(x\\)가 \\(N(\\mu, 1)\\)을 따른다면 \\(v=x^2\\)은 자유도가 1인 비중심 카이제곱 분포, \\(\\chi^2_1(\\lambda^2)\\) 를 따른다. 여기서 비중심 카이제곱 분포의 자유도는 1이고 비중심모수 \\(\\lambda^2 = \\mu^2\\)으로 주어진다.\n이제 \\(n\\)개의 서로 독립인 확률 변수 \\(x_1,x_2,\\cdots,x_n\\)이 각각 \\(N(\\mu_i, 1)\\)을 따른다면 \\(v=x_1^2+\\dots+x_n^2\\)은 자유도가 \\(n\\)이고 비중심 모수가 \\(\\lambda^2 = \\sum_{i=1}^n \\mu_i^2\\)인 비중심 카이제곱 분포, \\(\\chi^2_n(\\lambda^2)\\) 를 따른다.\n참고로 확률변수 \\(x\\)가 \\(N(0, 1)\\)을 따른다면 \\(v=x^2\\)은 중심 카이제곱 분포, \\(\\chi^2_1\\) 를 따르며 이때는 비중심모수가 \\(\\lambda^2=0\\)이다. 즉, 비중심모수가 0인 비중심 카이제곱 분포(non-central chi square distribution)를 중심 카이제곱 분포(central chi square distribution)라고 한다. 또한 중심 카이제곱 분포는 중심을 빼고 카이제곱 분포라고 부른다.\n\n\nF.4.3 이차형식의 분포\n\\(n\\)개의 서로 독립인 확률 변수 \\(x_1,x_2,\\cdots,x_n\\)이 각각 \\(N(\\mu_i, \\sigma^2)\\)를 따른다면 \\(n\\)-차원의 확률벡터 \\(\\pmb x\\)는 다음과 같은 다변량 정규분포를 따른다고 할 수 있다.\n\\[ \\pmb x \\sim N(\\pmb \\mu, \\sigma^2 \\pmb I) \\]\n위에서 \\(\\pmb \\mu^t =(\\mu_1, \\mu_2, \\dots, \\mu_n)\\)\n이제 이차형식의 분포에 대하여 논의하자.\n\n정리 F.1 (이차형식의 분포) \\(n\\)-차원의 확률벡터 \\(\\pmb x\\)가 \\(N(\\pmb \\mu, \\sigma^2 \\pmb I)\\)를 따른다면 이차형식 \\(Q=\\pmb x^t \\pmb A \\pmb x\\)의 분포는 다음과 같다.\n\\[\nV = \\frac{Q}{\\sigma^2} = \\frac {\\pmb x^t \\pmb A \\pmb x}{\\sigma^2} ~~ \\equiv_d ~~ \\sum_{i=1}^n \\lambda_i \\frac{u^2_i}{\\sigma^2}\n\\tag{F.5}\\]\n위의 식에서 \\(x \\equiv_d y\\) 는 확률변수 \\(x\\)와 \\(y\\)가 동일한 분포를 가진다는 것을 의미한다.\n식 F.5 에서 행렬 \\(\\pmb A\\)의 스펙트럴 분해는 \\(\\pmb A = \\pmb P \\pmb \\Lambda \\pmb P^t\\)이며 \\(\\lambda_i\\)는 행렬 \\(\\pmb A\\)의 고유치, 즉 행렬 \\(\\pmb \\Lambda\\)의 대각원소이다. 또한 확률변수 \\(u_i\\)들은 서로 독립이며 정규분포 \\(N(\\eta_i, \\sigma^2)\\)를 따른다. 여기서 \\(\\eta_1, \\eta_2,\\dots, \\eta_n\\)는 다음과 같이 정의된다.\n\\[\n\\pmb \\eta =\n\\begin{bmatrix}\n\\eta_1 \\\\\n\\eta_2 \\\\\n\\vdots \\\\\n\\eta_n\n\\end{bmatrix} = \\pmb P^t \\pmb \\mu\n\\]\n즉, 식 F.5 에서 \\(u_1^2/\\sigma^2, u_2^2/\\sigma^2, \\dots, u_n^2/\\sigma^2\\)는 서로 독립이며 각각 자유도가 1 이고 비중심 모수가 \\(\\eta_1^2/\\sigma^2, \\eta_2^2/\\sigma^2, \\dots,  \\eta_n^2/\\sigma^2\\)인 비중심 카이제곱-분포를 따른다.\n\\(\\blacksquare\\)\n\n정리 F.1 의 식 F.5 에서 나타난 이차형식의 분포는 비중심 카이제곱 분포를 따르는 서로 독립인 확률 변수들의 가중 평균과 같다는 것이다.\n이는 이차형식의 분포가 비중심 카이제곱 분포를 따른다는 것이 아님을 주의해야 한다. 그러면 어느 경우에 이차형식의 분포가 비중심 카이제곱 분포를 따르는가 생각해 보자.\n가장 쉽게 생각할 수 있는 경우가 식 F.5 에서 \\(\\lambda_i\\) 들의 값들이 0 또는 1인 경우이다. 이러한 경우는 행렬\n\\(\\pmb A\\) 가 멱등행렬인 경우이다. 실제로 다음 정리는 이차형식의 분포가 비중심 카이제곱 분포를 따르는 필요충분 조건이 행렬\n\\(\\pmb A\\) 가 멱등행렬이라는 것을 말해준다.\n\n따름정리 F.1 \\(n\\)-차원의 확률벡터 \\(\\pmb x\\)가 \\(N(\\pmb \\mu, \\sigma^2 \\pmb I)\\)를 따른다면 이차형식 \\(Q=\\pmb x^t \\pmb A \\pmb x\\)의 분포가 자유도가 \\(r\\) 이며 다음과 같은 비중심 모수 \\(\\lambda^2\\)을 가지는 비중심 카이제곱 분포를 따르는 필요충분 조건은 \\(\\pmb A\\)가 멱등행렬이고 \\(rank(\\pmb A)=r\\) 인 경우이다.\n\\[\n\\lambda^2 = \\frac{\\pmb \\mu^t \\pmb  A \\pmb  \\mu}{\\sigma^2}\n\\]\n더 나아가 \\(\\pmb A \\pmb \\mu = \\pmb 0\\) 이면 이차형식의 분포는 자유도가 \\(r\\)인 (중심)카이제곱 분포를 따른다.\n\\(\\blacksquare\\)\n\n\n\nF.4.4 이차형식의 독립\n두 개의 이차형식이 독립일 조건은 다음 정리와 같다.\n\n정리 F.2 (이차형식의 독립) \\(n\\)-차원의 확률벡터 \\(\\pmb {x}\\) 가 \\(N\\left(\\pmb {\\mu}, \\sigma^2 \\pmb {I}\\right)\\) 를 따른다고 하자. 두 이차형식 \\(Q_1=\\pmb {x}^t \\pmb {A} \\pmb {x}\\) 과 \\(Q_2=\\pmb {x}^t \\pmb {B} \\pmb {x}\\) 가 서로 독립일 필요충분 조건은 \\(\\pmb {A B}=\\pmb {0}\\) 이다.\n\\(\\blacksquare\\)\n\n\n\nF.4.5 이차형식의 차이\n만약 3 개의 이차형식 \\(Q, Q_1, Q_2\\) 가 있어서 다음과 같은 관계가 있다고 하자.\n\\[\nQ=\\pmb {x}^t \\pmb {A} \\pmb {x}=Q_1+Q_2=\\pmb {x}^t \\pmb {A}_1 \\pmb {x}+\\pmb {x}^t \\pmb {A}_2 \\pmb {x}\n\\]\n이러한 경우 두 이차형식 \\(Q\\) 과 \\(Q_1\\) 이 각각 카이제곱 분포를 따를 때 \\(Q_2=Q-Q_1\\) 이 카이제곱 분포를 따르는 조건이 중요하다. 다음 정리는 그 조건을 행렬 \\(\\pmb {A}_2\\) 가 양반정치인 경우라는 것을 말해준다.\n\n정리 F.3 (이차형식의 차이) \\(n\\)-차원의 확률벡터 \\(\\pmb {x}\\) 가 \\(N\\left(\\pmb {\\mu}, \\sigma^2 \\pmb {I}\\right)\\) 를 따른다고 하자. 세 개의 이차형식 \\(Q=\\pmb {x}^t \\pmb {A} \\pmb {x}, Q_1=\\pmb {x}^t \\pmb {A}_1 \\pmb {x}\\), \\(Q_2=\\pmb {x}^t \\pmb {A}_2 \\pmb {x}\\) 가 있다고 하고 \\(Q=Q_1+Q_2\\) 인 관계를 가진다고 가정하자.\n만약 \\(Q / \\sigma^2\\) 이 \\(\\chi_r^2\\left(\\lambda^2\\right)\\) 을 따르고 \\(Q_1 / \\sigma^2\\) 이 \\(\\chi_{r_1}^2\\left(\\lambda_1^2\\right)\\) 을 따르며 행렬 \\(\\pmb {A}_2\\) 가 양반정치 행렬이면 다음을 만족한다. 두 이차형식 \\(Q_1\\) 과 \\(Q_2\\) 는 서로 독립이다. 또한 이차형식 \\(Q_2\\) 는 자유도가 \\(r_2=r-r_1\\) 이고 비중심 모수가 \\(\\lambda_2^2=\\lambda^2-\\lambda_1^2\\) 인 비중심 카이제곱분포를 따른다.\n\\(\\blacksquare\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>이차형식과 제곱합의 분포</span>"
    ]
  },
  {
    "objectID": "qmd/quadratic.html#코크란의-정리",
    "href": "qmd/quadratic.html#코크란의-정리",
    "title": "부록 F — 이차형식과 제곱합의 분포",
    "section": "F.5 코크란의 정리",
    "text": "F.5 코크란의 정리\n선형모형에서 자주 등장하는 제곱합들의 분해, 즉 이차형식의 분해를 생각할 때 각 제곱합들의 분포를 아는 것이 매우 중요하다. 다음에 제시된 코크란의 정리(Cochran’s Theorem)는 총 제곱합을 분해했을 때 각 제곱합의 분포가 카이제곱 분포를 따를 조건을 말해준다.\n\n정리 F.4 (COCHRAN’S THEOREM) \\(n\\)-차원의 확률벡터 \\(\\pmb {x}\\) 가 \\(N\\left(\\pmb {\\mu}, \\sigma^2 \\pmb {I}\\right)\\) 를 따른다고 하자. \\(k\\) 개의 이차형식 \\(Q_j=\\pmb {x}^t \\pmb {A}_j \\pmb {x}, j=1,2, \\ldots, k\\) 를 생각하고 다음과 같은 관계를 가진다고 하자.\n\\[\n\\pmb {x}^t \\pmb {x}=\\sum_{i=1}^n x_i^2=\\sum_{j=1}^k Q_j\n\\]\n즉, \\(\\sum_{j=1}^k \\pmb {A}_j=\\pmb {I}\\) 이다. 또한 \\(r_j=\\operatorname{rank}\\left(\\pmb {A}_j\\right)\\) 이고 \\(\\lambda_j^2=\\pmb {\\mu}^t \\pmb {A}_j \\pmb {\\mu}\\) 라고 하자.\n\\(k\\) 개의 이차형식 \\(Q_1, Q_2, \\ldots, Q_k\\) 들이 모두 독립이고 각 이차형식 \\(Q_j / \\sigma^2\\) 가 비중심 카이제곱 분포 \\(\\chi_{r_j}^2\\left(\\lambda_j^2\\right)\\) 를 따를 필요충분 조건은 다음과 같다.\n\\[\nr_1+r_2+\\cdots+r_k=n\n\\] \\(\\blacksquare\\)\n\n이제 제곱합의 분포들에 대하여 지금까지 학습한 내용을 정리해보자. 만약 \\(n\\)-차원의 확률벡터 \\(\\pmb {x}\\) 가 \\(N\\left(\\pmb {\\mu}, \\sigma^2 \\pmb {I}\\right)\\) 를 따른다고 하고 위의 코크란의 정리와 같이 제곱합의 분해를 고려하자. 다음에 제시된 모든 문장은 서로 동치(equivalent)이다.\n\n이차형식 \\(Q_1, Q_2, \\ldots, Q_k\\) 들이 모두 독립이다.\n모든 \\(j=1,2, \\ldots, k\\) 에 대하여 이차형식 \\(Q_j / \\sigma^2\\) 가 비중심 카이제곱 분포 \\(\\chi_{r_j}^2\\left(\\lambda_j^2\\right)\\) 를 따른다.\n\\(\\pmb {A}_1, \\pmb {A}_2, \\ldots, \\pmb {A}_k\\) 가 모두 멱등행렬이다.\n모든 \\(j \\neq k\\) 에 대하여 \\(\\pmb {A}_j \\pmb {A}_k=\\pmb {0}\\) 이다.\n\\(r_1+r_2+\\cdots+r_k=n\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>이차형식과 제곱합의 분포</span>"
    ]
  },
  {
    "objectID": "qmd/practice-01.html",
    "href": "qmd/practice-01.html",
    "title": "부록 G — R-실습: 중회귀 모형 적합",
    "section": "",
    "text": "G.1 예제 3.3 자료\n예제 3.3에 나온 중고차 가격자료를 이용한 R 실습입니다.\nhead(usedcars)\n\n  price year mileage   cc automatic\n1   790   78  133462 1998         1\n2  1380   39   33000 2000         1\n3   270  109  120000 1800         0\n4  1190   20   69727 1999         1\n5   590   70  112000 2000         0\n6  1120   58   39106 1998         1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>R-실습: 중회귀 모형 적합</span>"
    ]
  },
  {
    "objectID": "qmd/practice-01.html#예제-3.3-자료",
    "href": "qmd/practice-01.html#예제-3.3-자료",
    "title": "부록 G — R-실습: 중회귀 모형 적합",
    "section": "",
    "text": "G.1.1 산점도 행렬\n\npairs(usedcars)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>R-실습: 중회귀 모형 적합</span>"
    ]
  },
  {
    "objectID": "qmd/practice-01.html#중회귀-모형의-적합",
    "href": "qmd/practice-01.html#중회귀-모형의-적합",
    "title": "부록 G — R-실습: 중회귀 모형 적합",
    "section": "G.2 중회귀 모형의 적합",
    "text": "G.2 중회귀 모형의 적합\n\nfit0 &lt;- lm(price ~ year + mileage + cc + automatic, usedcars)\n\n계획행렬은 다음과 같이 구할 수 있다.\n\nmodel.matrix(fit0)\n\n   (Intercept) year mileage   cc automatic\n1            1   78  133462 1998         1\n2            1   39   33000 2000         1\n3            1  109  120000 1800         0\n4            1   20   69727 1999         1\n5            1   70  112000 2000         0\n6            1   58   39106 1998         1\n7            1   53   95935 1800         1\n8            1   68  120000 1800         0\n9            1   15   20215 1798         1\n10           1   96  140000 1800         0\n11           1   63   68924 1998         1\n12           1   82   90000 2000         0\n13           1   76   81279 1998         0\n14           1   17   24070 1798         1\n15           1   38   40000 2000         0\n16           1   46   56887 1832         1\n17           1   95   91216 1997         1\n18           1   37   48680 1998         1\n19           1   68    8000 2000         0\n20           1   41   60634 1835         1\n21           1   69  114131 1998         1\n22           1   71   75000 1800         0\n23           1   99  124417 1998         1\n24           1  129  130000 1800         0\n25           1   57   77559 1997         1\n26           1  107   75216 1838         1\n27           1   45   52000 2000         0\n28           1   80   58000 2000         1\n29           1  113  134500 1800         0\n30           1   41   80000 2000         0\nattr(,\"assign\")\n[1] 0 1 2 3 4\n\n\nfit0 에 저장된 결과를 다음과 같이 함수 str을 이용하여 볼 수 있다.\n\nstr(fit0)\n\nList of 12\n $ coefficients : Named num [1:5] 525.28696 -5.79964 -0.00226 0.38879 165.31263\n  ..- attr(*, \"names\")= chr [1:5] \"(Intercept)\" \"year\" \"mileage\" \"cc\" ...\n $ residuals    : Named num [1:30] 76.98 212.69 -51.4 -4.01 -53.45 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:30] -4407 -1434 -369 -229 419 ...\n  ..- attr(*, \"names\")= chr [1:30] \"(Intercept)\" \"year\" \"mileage\" \"cc\" ...\n $ rank         : int 5\n $ fitted.values: Named num [1:30] 713 1167 321 1194 643 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:5] 0 1 2 3 4\n $ qr           :List of 5\n  ..$ qr   : num [1:30, 1:5] -5.477 0.183 0.183 0.183 0.183 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:5] \"(Intercept)\" \"year\" \"mileage\" \"cc\" ...\n  .. ..- attr(*, \"assign\")= int [1:5] 0 1 2 3 4\n  ..$ qraux: num [1:5] 1.18 1.18 1.08 1.03 1.26\n  ..$ pivot: int [1:5] 1 2 3 4 5\n  ..$ tol  : num 1e-07\n  ..$ rank : int 5\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 25\n $ xlevels      : Named list()\n $ call         : language lm(formula = price ~ year + mileage + cc + automatic, data = usedcars)\n $ terms        :Classes 'terms', 'formula'  language price ~ year + mileage + cc + automatic\n  .. ..- attr(*, \"variables\")= language list(price, year, mileage, cc, automatic)\n  .. ..- attr(*, \"factors\")= int [1:5, 1:4] 0 1 0 0 0 0 0 1 0 0 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:5] \"price\" \"year\" \"mileage\" \"cc\" ...\n  .. .. .. ..$ : chr [1:4] \"year\" \"mileage\" \"cc\" \"automatic\"\n  .. ..- attr(*, \"term.labels\")= chr [1:4] \"year\" \"mileage\" \"cc\" \"automatic\"\n  .. ..- attr(*, \"order\")= int [1:4] 1 1 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(price, year, mileage, cc, automatic)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:5] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. ..- attr(*, \"names\")= chr [1:5] \"price\" \"year\" \"mileage\" \"cc\" ...\n $ model        :'data.frame':  30 obs. of  5 variables:\n  ..$ price    : int [1:30] 790 1380 270 1190 590 1120 815 450 1290 420 ...\n  ..$ year     : int [1:30] 78 39 109 20 70 58 53 68 15 96 ...\n  ..$ mileage  : int [1:30] 133462 33000 120000 69727 112000 39106 95935 120000 20215 140000 ...\n  ..$ cc       : int [1:30] 1998 2000 1800 1999 2000 1998 1800 1800 1798 1800 ...\n  ..$ automatic: int [1:30] 1 1 0 1 0 1 1 0 1 0 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language price ~ year + mileage + cc + automatic\n  .. .. ..- attr(*, \"variables\")= language list(price, year, mileage, cc, automatic)\n  .. .. ..- attr(*, \"factors\")= int [1:5, 1:4] 0 1 0 0 0 0 0 1 0 0 ...\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:5] \"price\" \"year\" \"mileage\" \"cc\" ...\n  .. .. .. .. ..$ : chr [1:4] \"year\" \"mileage\" \"cc\" \"automatic\"\n  .. .. ..- attr(*, \"term.labels\")= chr [1:4] \"year\" \"mileage\" \"cc\" \"automatic\"\n  .. .. ..- attr(*, \"order\")= int [1:4] 1 1 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(price, year, mileage, cc, automatic)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:5] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. .. ..- attr(*, \"names\")= chr [1:5] \"price\" \"year\" \"mileage\" \"cc\" ...\n - attr(*, \"class\")= chr \"lm\"\n\n\n\nG.2.1 회귀계수의 추정과 결정계수\n함수 summary 는 각 계수의 추정값과 가설 \\(H_0: \\beta_i=0\\)에 대한 t-검정 결과를 보여준다. 또한 결정계수 \\(R^2\\)도 구해준다.\n\nsummary(fit0)\n\n\nCall:\nlm(formula = price ~ year + mileage + cc + automatic, data = usedcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-177.35  -63.91   -0.99   70.34  212.69 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.253e+02  3.998e+02   1.314 0.200823    \nyear        -5.800e+00  9.283e-01  -6.247 1.55e-06 ***\nmileage     -2.263e-03  7.211e-04  -3.138 0.004324 ** \ncc           3.888e-01  2.022e-01   1.923 0.065958 .  \nautomatic    1.653e+02  3.986e+01   4.147 0.000339 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 101.1 on 25 degrees of freedom\nMultiple R-squared:  0.9045,    Adjusted R-squared:  0.8892 \nF-statistic: 59.21 on 4 and 25 DF,  p-value: 2.184e-12\n\n\n각 회귀 계수에 대한 신뢰구간은 함수 confint로 구할 수 있다.\n\nconfint(fit0)\n\n                    2.5 %        97.5 %\n(Intercept) -2.981256e+02  1.348699e+03\nyear        -7.711605e+00 -3.887669e+00\nmileage     -3.748021e-03 -7.776672e-04\ncc          -2.763072e-02  8.052054e-01\nautomatic    8.322275e+01  2.474025e+02\n\n\n\n\nG.2.2 분산분석\n\nanova(fit0)\n\nAnalysis of Variance Table\n\nResponse: price\n          Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nyear       1 2056608 2056608 201.2036 1.841e-13 ***\nmileage    1  135864  135864  13.2919 0.0012228 ** \ncc         1   52409   52409   5.1273 0.0324794 *  \nautomatic  1  175828  175828  17.2018 0.0003389 ***\nResiduals 25  255538   10222                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nG.2.3 예측값\n반응변수에 대한 예측값 \\(\\hat {\\pmb y} = \\pmb X \\hat {\\pmb \\beta}\\)는 함수 predict를 이용한다.\n\npredict(fit0)\n\n        1         2         3         4         5         6         7         8 \n 713.0214 1167.3146  321.4025 1194.0114  643.4485 1042.5270  865.9501  559.1876 \n        9        10        11        12        13        14        15        16 \n1256.9013  351.5409  946.0553  623.6355  677.3900 1236.5788  991.9617 1007.3483 \n       17        18        19        20        21        22        23        24 \n 709.6348 1142.6549  890.3836 1029.0340  808.9611  643.6167  611.6964  182.7813 \n       25        26        27        28        29        30 \n 960.9247  614.4275  924.2101  872.9584  265.3927  884.0490 \n\n\n새로운 자료에 대한 예측값 \\(\\widehat { E(y|x)}\\)은 다음과 같이 데이터프레임을 만들고 예측한다.\n\nnw &lt;- data.frame(year=60, mileage=10000, cc=200, automatic=1)\nnw\n\n  year mileage  cc automatic\n1   60   10000 200         1\n\npredict(fit0, newdata=nw, interval=\"confidence\")\n\n       fit       lwr      upr\n1 397.7504 -342.6272 1138.128\n\n\n새로운 관측값에 대항 예측은 다음과 같이 한다.\n\npredict(fit0, newdata=nw, interval=\"prediction\")\n\n       fit       lwr      upr\n1 397.7504 -371.3501 1166.851",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>R-실습: 중회귀 모형 적합</span>"
    ]
  },
  {
    "objectID": "qmd/practice-01.html#잔차-분석",
    "href": "qmd/practice-01.html#잔차-분석",
    "title": "부록 G — R-실습: 중회귀 모형 적합",
    "section": "G.3 잔차 분석",
    "text": "G.3 잔차 분석\n\nplot(fit0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG.3.1 제곱합의 종류\n\nG.3.1.1 순차제곱합\n순차제곱합은 모형에 들어가는 변수의 순서에 따라서 제곱합이 틀려진다.\n다음의 예를 보면 두 모형이 같은 변수들로 적합되지만 순서가 달라지면 순차제곱합이 다르다.\n\nmodel1 &lt;- price ~ year + mileage + cc + automatic\nmodel2 &lt;- price ~ mileage + automatic + cc + year\nfit1 &lt;- lm(model1, usedcars)\nfit2 &lt;- lm(model2, usedcars)\nanova(fit1)\n\nAnalysis of Variance Table\n\nResponse: price\n          Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nyear       1 2056608 2056608 201.2036 1.841e-13 ***\nmileage    1  135864  135864  13.2919 0.0012228 ** \ncc         1   52409   52409   5.1273 0.0324794 *  \nautomatic  1  175828  175828  17.2018 0.0003389 ***\nResiduals 25  255538   10222                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(fit2)\n\nAnalysis of Variance Table\n\nResponse: price\n          Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nmileage    1 1637355 1637355 160.1870 2.274e-12 ***\nautomatic  1  341741  341741  33.4335 5.006e-06 ***\ncc         1   42683   42683   4.1758   0.05168 .  \nyear       1  398929  398929  39.0283 1.552e-06 ***\nResiduals 25  255538   10222                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n하지만 회귀계수의 추정량은 동일하다.\n\nsummary(fit1)\n\n\nCall:\nlm(formula = model1, data = usedcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-177.35  -63.91   -0.99   70.34  212.69 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.253e+02  3.998e+02   1.314 0.200823    \nyear        -5.800e+00  9.283e-01  -6.247 1.55e-06 ***\nmileage     -2.263e-03  7.211e-04  -3.138 0.004324 ** \ncc           3.888e-01  2.022e-01   1.923 0.065958 .  \nautomatic    1.653e+02  3.986e+01   4.147 0.000339 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 101.1 on 25 degrees of freedom\nMultiple R-squared:  0.9045,    Adjusted R-squared:  0.8892 \nF-statistic: 59.21 on 4 and 25 DF,  p-value: 2.184e-12\n\nsummary(fit2)\n\n\nCall:\nlm(formula = model2, data = usedcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-177.35  -63.91   -0.99   70.34  212.69 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.253e+02  3.998e+02   1.314 0.200823    \nmileage     -2.263e-03  7.211e-04  -3.138 0.004324 ** \nautomatic    1.653e+02  3.986e+01   4.147 0.000339 ***\ncc           3.888e-01  2.022e-01   1.923 0.065958 .  \nyear        -5.800e+00  9.283e-01  -6.247 1.55e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 101.1 on 25 degrees of freedom\nMultiple R-squared:  0.9045,    Adjusted R-squared:  0.8892 \nF-statistic: 59.21 on 4 and 25 DF,  p-value: 2.184e-12\n\n\n\n\nG.3.1.2 편제곱합\n편제곱합은 다른 변수들로 보정된 제곱합으로 순서에 관계없이 일정하다.패키지 car 에 있는 함수 Anova 를 사용하면 편제곱합을 구할 수 있다.\n\nAnova(fit1, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: price\n            Sum Sq Df F value    Pr(&gt;F)    \n(Intercept)  17645  1  1.7262 0.2008228    \nyear        398929  1 39.0283 1.552e-06 ***\nmileage     100649  1  9.8467 0.0043244 ** \ncc           37794  1  3.6975 0.0659577 .  \nautomatic   175828  1 17.2018 0.0003389 ***\nResiduals   255538 25                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(fit2, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: price\n            Sum Sq Df F value    Pr(&gt;F)    \n(Intercept)  17645  1  1.7262 0.2008228    \nmileage     100649  1  9.8467 0.0043244 ** \nautomatic   175828  1 17.2018 0.0003389 ***\ncc           37794  1  3.6975 0.0659577 .  \nyear        398929  1 39.0283 1.552e-06 ***\nResiduals   255538 25                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>R-실습: 중회귀 모형 적합</span>"
    ]
  },
  {
    "objectID": "qmd/practice-01.html#부분-f-검정",
    "href": "qmd/practice-01.html#부분-f-검정",
    "title": "부록 G — R-실습: 중회귀 모형 적합",
    "section": "G.4 부분 F 검정",
    "text": "G.4 부분 F 검정\n배기량(cc)에 대항 계수가 0인지 검정해보자.\n\\[ H_0: ~ \\beta_k =0 \\]\n하나의 계수에 대한 검정은 분산분석 표의 t-검정으로도 가능하며 결과는 동일하다.\n\nfullmodel &lt;- price ~ year + mileage + cc + automatic\nreducemodel1 &lt;- price ~ year + mileage + automatic\nfitfull &lt;- lm(fullmodel, data=usedcars)\nfitreduce1 &lt;- lm(reducemodel1, data=usedcars)\nanova(fitreduce1, fitfull)\n\nAnalysis of Variance Table\n\nModel 1: price ~ year + mileage + automatic\nModel 2: price ~ year + mileage + cc + automatic\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     26 293332                              \n2     25 255538  1     37794 3.6975 0.06596 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n이제 두 개 이상 의 변수에 대하여 부분 F 검정을 해보자. 설명변수 cc 와 automatic에 대한 계수가 0인지 검정해보자.\n\\[ H_0: ~ \\beta_k = \\beta_l = 0 \\]\n\nreducemodel2 &lt;- price ~ year + mileage\nfitreduce2 &lt;- lm(reducemodel2, data=usedcars)\nanova(fitreduce2, fitfull)\n\nAnalysis of Variance Table\n\nModel 1: price ~ year + mileage\nModel 2: price ~ year + mileage + cc + automatic\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     27 483775                                  \n2     25 255538  2    228237 11.165 0.0003429 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>R-실습: 중회귀 모형 적합</span>"
    ]
  },
  {
    "objectID": "qmd/practice-01.html#선형-가설에-대한-검정",
    "href": "qmd/practice-01.html#선형-가설에-대한-검정",
    "title": "부록 G — R-실습: 중회귀 모형 적합",
    "section": "G.5 선형 가설에 대한 검정",
    "text": "G.5 선형 가설에 대한 검정\n다음과 같은 선형 가설을 생각자.\n\\[ H_0: \\pmb L \\pmb \\beta= \\pmb 0\\]\n교과서 예제 4.4 에서 다음과 같은 가설을 고려한다.\n\\[ H_0: \\beta_2=0, \\beta_3= 2.5 \\beta_4 \\]\n\nmodreduce &lt;- lm(suneung ~ kor + I(2.5*math + sci), data=suneung)\nmodfull &lt;- lm(suneung ~ kor + eng + math + sci, data=suneung)\nanova(modreduce, modfull)\n\nAnalysis of Variance Table\n\nModel 1: suneung ~ kor + I(2.5 * math + sci)\nModel 2: suneung ~ kor + eng + math + sci\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     22 3136.4                           \n2     20 3023.5  2    112.95 0.3736  0.693\n\n\n위의 검정은 다음과 과 같이 선형행렬 \\(L\\)을 정의하고 함수 car::linearHypothesis를 이용한 결과와 같다.\n\\[\nH_0:\n\\begin{bmatrix}\n0 & 0  & 1 & 0 & 0  \\\\\n0 & 0  & 0 & 1 &-2.5\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\beta_3 \\\\\n\\beta_4\n\\end{bmatrix}\n=\\pmb 0\n\\]\n\nL &lt;- matrix(c(0,0,1,0,0,0,0,0,1,-2.5),2,5, byrow=TRUE)\nL\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    1    0  0.0\n[2,]    0    0    0    1 -2.5\n\nlinearHypothesis(modfull, hypothesis.matrix=L)\n\nLinear hypothesis test\n\nHypothesis:\neng = 0\nmath - 2.5 sci = 0\n\nModel 1: restricted model\nModel 2: suneung ~ kor + eng + math + sci\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     22 3136.4                           \n2     20 3023.5  2    112.95 0.3736  0.693",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>R-실습: 중회귀 모형 적합</span>"
    ]
  },
  {
    "objectID": "qmd/practice-02.html",
    "href": "qmd/practice-02.html",
    "title": "부록 H — R-실습: 중회귀 모형 진단",
    "section": "",
    "text": "H.1 변수변환",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>R-실습: 중회귀 모형 진단</span>"
    ]
  },
  {
    "objectID": "qmd/practice-02.html#변수변환",
    "href": "qmd/practice-02.html#변수변환",
    "title": "부록 H — R-실습: 중회귀 모형 진단",
    "section": "",
    "text": "H.1.1 예제 4.8\n여기서 이용한 자료 bug 는 살충제의 독성실험에서 살충제에 노출된 벌레들의 생존개체수를 시간대별로 관측한 것이다.\n\ny :생존벌레의 수\ntime :시간(분)\n\n\nplot(y~time, regbook::bug)\n\n\n\n\n\n\n\n\n이제 로그변화을 고려해 보자.\n\nbug2 &lt;-regbook::bug\nbug2$logy &lt;- log(bug2$y)\nplot(logy~time, bug2)\n\n\n\n\n\n\n\n\n변환된 자료에 대한 회귀분석을 수행해 보자.\n\nfitlog &lt;- lm(logy~time, bug2)\nplot(logy~time, bug2)\nabline(fitlog)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>R-실습: 중회귀 모형 진단</span>"
    ]
  },
  {
    "objectID": "qmd/practice-02.html#box-cox-변환",
    "href": "qmd/practice-02.html#box-cox-변환",
    "title": "부록 H — R-실습: 중회귀 모형 진단",
    "section": "H.2 Box-Cox 변환",
    "text": "H.2 Box-Cox 변환\nBox-Cox 변환은 다음과 같이 수행한다. 패키지 MASS 의 함수 boxcox 를 이용한다.\n\nH.2.1 예제 4.10\n\nfoot:발길이(mm), 양말을 벗은 상태로 측정하였고 오른쪽 발만 측정하였다.\nforearm: 팔안쪽길이(mm), 손목부터 팔꿈치가 접히는 부분까지의 길이이다. 오른쪽 팔만 측정하였다.\n\n변환이 필요없는 경우에 대한 예제이다.\n\n# plot histogram of foot by ggplot2\naflength %&gt;% ggplot(aes(x=foot)) + geom_histogram(binwidth=15, fill=\"skyblue\", color=\"black\") + theme_minimal()\n\n\n\n\n\n\n\n\n\nplot(foot ~ forearm, data=aflength)\n\n\n\n\n\n\n\nex411 &lt;- boxcox(lm(foot ~ forearm, data=aflength))\n\n\n\n\n\n\n\n\n\n\nH.2.2 예제 4.11\n예제 4.11 자료 wool 는 Box & Cox의 1964년 논문에서 사용한 예제로, 양모의 강력을 알아보기 위해 \\(3^3\\) 요인실험을 수행한 결과이다.\n\ncycle :반응변수. 시편이 끊어질 때까지의 측정 횟수.\nlength :시편의 길이\nload : 시편에 가한 하중\namplitude :하중을 가한 폭\n\n반응변수 cycle 의 히스토그램능 보면 오른쪽으로 매우 치우친 분포로서 정규분포와 매우 다른 모양을 보인다.\n\n# plot histogram of foot by ggplot2\nwool %&gt;% ggplot(aes(x=cycle)) + geom_histogram(binwidth=200, fill=\"skyblue\", color=\"black\") + theme_minimal()\n\n\n\n\n\n\n\n\n잔차 분석의 결과를 보면 잔차에\n\nwoolfm1 &lt;- lm(cycle~length + amplitude + load, data=wool)\nsummary(woolfm1)\n\n\nCall:\nlm(formula = cycle ~ length + amplitude + load, data = wool)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-644.5 -279.1 -150.2  199.5 1268.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4521.370   1621.721   2.788 0.010454 *  \nlength        13.200      2.301   5.736 7.66e-06 ***\namplitude   -535.833    115.057  -4.657 0.000109 ***\nload         -62.167     23.011  -2.702 0.012734 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 488.1 on 23 degrees of freedom\nMultiple R-squared:  0.7291,    Adjusted R-squared:  0.6937 \nF-statistic: 20.63 on 3 and 23 DF,  p-value: 1.028e-06\n\n\n이제 Box-Cox 변환을 적용해 보자.\n\nboxcox(woolfm1)\n\n\n\n\n\n\n\n\n위의 결과에서 \\(\\lambda = 0\\)이 가장 좋은 변환으로 나타났다. 이는 로그 변환이 가장 적절하다는 의미이다. 이제 이 변환을 적용해 보자.\n\nwool$logcycle &lt;- log(wool$cycle)\nwoolfm2 &lt;- lm(logcycle~length + amplitude + load, data=wool)\nsummary(woolfm2)\n\n\nCall:\nlm(formula = logcycle ~ length + amplitude + load, data = wool)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43592 -0.11250  0.00802  0.11635  0.26790 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.551813   0.616683  17.111 1.41e-14 ***\nlength       0.016648   0.000875  19.025 1.43e-15 ***\namplitude   -0.630866   0.043752 -14.419 5.22e-13 ***\nload        -0.078524   0.008750  -8.974 5.66e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1856 on 23 degrees of freedom\nMultiple R-squared:  0.9658,    Adjusted R-squared:  0.9614 \nF-statistic: 216.8 on 3 and 23 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(woolfm2)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>R-실습: 중회귀 모형 진단</span>"
    ]
  },
  {
    "objectID": "qmd/practice-02.html#다중공선성",
    "href": "qmd/practice-02.html#다중공선성",
    "title": "부록 H — R-실습: 중회귀 모형 진단",
    "section": "H.3 다중공선성",
    "text": "H.3 다중공선성\n\nH.3.1 고유값과 고유벡터에 대한 이론\n선형모형 \\(E(\\pmb y | \\pmb X) = \\pmb X \\pmb \\beta\\) 에서 계획행렬 \\(\\pmb X\\)의 열들이 선형독립이 아닌 경우 다중공선성이 발생한다. 다중공선성은 계획행렬 \\(\\pmb X\\)의 열들이 선형종속인 경우에 발생한다.\n대칭행렬 \\(\\pmb X^t \\pmb  X\\)의 고유값 \\(\\lambda_i\\)와 그에 대응하는 고유벡터 \\(\\pmb  p_i\\)는 다음을 만족하는 실수와 벡터이다.\n\\[ (\\pmb X^t \\pmb  X ) \\pmb  p_i = \\lambda_i \\pmb  p_i \\]\n고유값 \\(\\lambda_i\\)을 구하는 방법은 다음의 방정식을 만족하는 해를 구하는 것이다.\n\\[ det \\left ( \\pmb X^t \\pmb  X - \\lambda_i \\pmb I \\right ) = 0\\]\n여기서 \\(det(\\pmb A)\\)는 행렬 \\(\\pmb A\\)의 행렬식을 의미한다.\n\\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{p}\\)를 \\(\\pmb X^t \\pmb X\\)의 고유값이라고 하자. \\(\\pmb X^t \\pmb X\\)의 각 고유값에 대한 정규직교 고유벡터(orthonormal eigenvector)를 \\(\\pmb  p_1, \\pmb  p_2,\\dots,\\pmb  p_{p}\\)라고 하자, 즉\n\\[ \\pmb  p_i^t \\pmb  p_i = 1 , \\quad \\pmb  p_i^t \\pmb  p_j = 0 \\quad (i \\ne j) \\]\n더 나아가 행렬 \\(\\pmb P\\)를 고유벡터를 모아놓은 행렬로 정의하자.\n\\[ \\pmb P=[\\pmb p_1 ~ \\pmb p_2 ~\\dots ~ \\pmb p_{p} ] \\]\n이때 \\(p \\times p\\) - 차원의 행렬 \\(\\pmb P\\)는 직교행렬이다.\n\\[ \\pmb P^t \\pmb P =\\pmb P  \\pmb P^t =\\pmb I \\]\n이제 다음과 같이 \\(\\pmb X^t \\pmb X\\)를 나타낼 수 있다.\n\\[ \\pmb P^t (\\pmb X^t \\pmb X) \\pmb P = \\text{diag}(\\lambda_1 , \\lambda_2 , \\dots , \\lambda_{p}) = \\pmb \\Lambda \\]\n또한\n\\[ \\pmb P^t (\\pmb X^t \\pmb X)^{-1} \\pmb P = \\text{diag} \\left (\\frac{1}{\\lambda_1} , \\frac{1}{\\lambda_2} , \\dots , \\frac{1}{\\lambda_{p}} \\right ) = \\pmb \\Lambda^{-1} \\]\n위의 식에서 알 수 있듯이 \\(1/\\lambda_i\\)는 \\((\\pmb X^t \\pmb X)^{-1}\\)의 고유값이다.\n행렬 \\(\\pmb P\\)가 직교행렬이기 때문에 다음과 같은 표현도 가능하다.\n\\[ (\\pmb X^t \\pmb X) =  \\pmb P \\pmb \\Lambda \\pmb P^t,\n\\quad (\\pmb X^t \\pmb X)^{-1} =  \\pmb P \\pmb \\Lambda^{-1} \\pmb P^t  \\]\n고유벡터와 고유값의 정의에 의하여 고유값 \\(\\lambda_k\\)이 매우 0에 가까우면 다음이 성립하고\n\\[ \\pmb p_k^t (\\pmb X^t \\pmb X) \\pmb p_k = (\\pmb X \\pmb p_k)^t ( \\pmb X \\pmb p_k) \\approx 0   \\] 위의 식은 다음과 같이 행렬 \\(\\pmb X\\)의 열들간에 선형관계 $ X p_k = $ 이 있다는 것을 의미한다.\n\\[  p_{1k} \\pmb x_1 +  p_{2k} \\pmb x_2 + \\dots  p_{p,k} \\pmb x_p \\approx 0 \\]\n위에서 \\(\\pmb p_k\\)와 \\(\\pmb X\\)는 다음과 같이 표시한다.\n\\[ \\pmb X=[\\pmb x_1~ \\pmb x_2~ \\dots~\\pmb x_{p}], \\quad\n\\pmb p_k =\n\\begin{bmatrix}\np_{1k} \\\\  \np_{2k} \\\\\n\\vdots \\\\\np_{p,k}\n\\end{bmatrix}\\]\n또한 회귀계수 벡터 \\(\\hat \\beta\\)의 공분산 행렬이 다음과 같이 주어지므로\n\\[\nCov(\\hat {\\pmb \\beta}) = \\sigma^2 (\\pmb X^t \\pmb X)^{-1} = \\sigma^2  \\pmb P \\pmb \\Lambda^{-1} \\pmb P^t\n\\tag{H.1}\\]\n다음과 같은 식이 성립한다.\n\\[\nvar(\\hat \\beta_k) / \\sigma^2 = \\frac{p^2_{k1}}{\\lambda_1} + \\frac{p^2_{k2}}{\\lambda_2} + \\dots \\frac{p^2_{k, p}}{\\lambda_{p}}\n\\tag{H.2}\\]\n\n\nH.3.2 고유값과 고유벡터에 대한 예제: 두 개의 독립변수\n이제 다음과 두 개의 독립변수가 있는 회귀 모형을 고려해 보자.\n\\[ y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + e_i, i=1,2,\\cdots,n \\]\n절편을 제외한 두 개의 표준화된 독립변수들로 이루어진 행렬을 \\(\\pmb X\\)로 표시하자.\n\\[  \\pmb X = [ \\pmb x_1 ~ \\pmb x_2 ]   \\]\n위에서 디자인 행렬 \\(\\pmb X\\)는 원래 독립변수의 디자인 행렬 \\(X\\)의 열들을 표준화한 변수로 구성된 것이다..\n\\[ \\sum_{i=1}^n x_{i1} =0, \\quad \\sum_{i=1}^n x_{i2} =0, \\quad \\sum_{i=1}^n x_{i1}^2 =1, \\quad \\sum_{i=1}^n x_{i2}^2 =1, \\quad \\sum_{i=1}^n x_{i1} x_{i2} =\\rho \\]\n이제 \\(\\pmb X^t \\pmb X\\)는 두 독립변수의 상관계수 행렬임을 알 수 있다.\n\\[  \\pmb X^t \\pmb X =\n\\begin{bmatrix}\n1  &  \\rho \\\\\n\\rho & 1\n\\end{bmatrix}\n=\\pmb R, \\quad\n0 &lt; \\rho &lt; 1\n\\]\n여기서 두 독립변수 \\(x_1\\)과 \\(x_2\\)의 상관계수 \\(\\rho\\)는 0보다 크다고 가정하자.\n이제 \\(\\pmb X^t \\pmb X\\)의 고유값(\\(\\lambda_i\\))과 고유벡터(\\(\\pmb p_i\\))는 다음과 같은 방정식을 만족하는 수 \\(\\lambda_i\\)와 벡터 \\(\\pmb p_i\\) 이다.\n\\[ (\\pmb X^t \\pmb X) \\pmb p_i = \\lambda_i \\pmb p_i, \\quad \\pmb p_i^t \\pmb p_i=1  \\]\n일단 먼저 고유값을 구하는 방법은 \\(det(\\pmb X^t \\pmb X - \\lambda_i \\pmb I ) =0\\)을 만족하는 값을 찾는 것이다. 여기서 \\(det(\\pmb A)\\)는 \\(\\pmb A\\)의 행렬식을 의미한다.\n\\[\ndet(\\pmb X^t \\pmb X - \\lambda_i \\pmb I ) = det \\left (\n\\begin{bmatrix}\n1-\\lambda_i  &  \\rho \\\\\n\\rho & 1-\\lambda_i\n\\end{bmatrix}\n\\right ) =0\n\\] 위의 방정식은 다음과 같이 요약할 수 있고\n\\[ \\lambda_i^2 -2 \\lambda_i + (1-\\rho^2) =0 \\]\n해는 다음과 같이 주어진다.\n\\[ \\lambda_1 = 1+ \\rho, \\quad \\lambda_2 = 1 -\\rho \\quad (\\lambda_1 \\ge \\lambda_2) \\]\n이제 각 고유값에 대한 고유벡터를 구해보자. 각 고유값 \\(\\lambda_i\\)에 대한 고유벡터를 \\(\\pmb p_i\\) 라고 하면\n\\[\n\\pmb p_1 =\n\\begin{bmatrix}\np_{11} \\\\\np_{21}\n\\end{bmatrix},\n~ p^2_{11}+p^2_{21}=1\n\\quad \\quad\n\\pmb p_2 =\n\\begin{bmatrix}\np_{12} \\\\\np_{22}\n\\end{bmatrix},~\np^2_{12}+p^2_{11}=1\n\\]\n다음과 같은 방정식을 만족해야 한다.\n\\[ (\\pmb X^t \\pmb X) \\pmb p_1 = \\lambda_1 \\pmb p_1 , \\quad  (\\pmb X^t \\pmb X) \\pmb p_2 = \\lambda_2 \\pmb p_2 \\]\n즉,\n\\[\n\\begin{bmatrix}\n1  &  \\rho \\\\\n\\rho & 1\n\\end{bmatrix}\n\\begin{bmatrix}\np_{11} \\\\\np_{21}\n\\end{bmatrix}\n=\n(1+ \\rho)\n\\begin{bmatrix}\np_{11} \\\\\np_{21}\n\\end{bmatrix}\n, \\quad\n\\begin{bmatrix}\n1  &  \\rho \\\\\n\\rho & 1\n\\end{bmatrix}\n\\begin{bmatrix}\np_{12} \\\\\np_{22}\n\\end{bmatrix}\n=\n(1- \\rho)\n\\begin{bmatrix}\np_{12} \\\\\np_{22}\n\\end{bmatrix}\n\\]\n위의 두 방정식은 정리하면 다음과 더 단순한 방정식을 얻는다.\n\\[  p_{11} -  p_{21} = 0, \\quad  p_{12}+ p_{22}=0 \\]\n이제 위의 식을 만족하고 길이가 1인 두 벡터를 찾으면 다음과 같은 두 개의 직교하고 길이가 1인 고유벡터 \\(\\pmb p_1\\)과 \\(\\pmb p_2\\)를 찾을 수 있다.\n\\[\n\\pmb p_1 =\n\\begin{bmatrix}\np_{11} \\\\\np_{21}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1/\\sqrt{2} \\\\\n1/\\sqrt{2}\n\\end{bmatrix},\n\\quad \\quad\n\\pmb p_2 =\n\\begin{bmatrix}\np_{12} \\\\\np_{22}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1/\\sqrt{2} \\\\\n-1/\\sqrt{2}\n\\end{bmatrix}\n\\]\n따라서 앞 절의 이론에서 나온 고유벡터로 구성된 행렬 \\(\\pmb P\\)와 고유값을 대각원소로 하는 행렬 \\(\\pmb \\Lambda\\)는 다음과 같다.\n\\[\n\\pmb P = [\\pmb p_1~ \\pmb p_2]\n= \\begin{bmatrix}\np_{11} &  p_{12}\\\\\np_{21} &  p_{22}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1/\\sqrt{2} &  1/\\sqrt{2}\\\\\n1/\\sqrt{2} &  -1/\\sqrt{2}\n\\end{bmatrix},\n\\quad \\quad\n\\pmb \\Lambda =\n\\begin{bmatrix}\n\\lambda_1 & 0 \\\\\n0 & \\lambda_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1+\\rho & 0 \\\\\n0 & 1-\\rho\n\\end{bmatrix}\n\\]\n이제 다음이 성립함을 확인할 수 있다.\n\\[ \\pmb P^t (\\pmb X^t \\pmb X) \\pmb P =  \\pmb \\Lambda, \\quad (\\pmb X^t \\pmb X)^{-1} =  \\pmb P \\pmb \\Lambda^{-1} \\pmb P^t   \\]\n즉,\n\\[\\begin{align*}\n\\pmb P^t (\\pmb X^t \\pmb X) \\pmb P\n& =  \n\\begin{bmatrix}\n1/\\sqrt{2} &  1/\\sqrt{2}\\\\\n1/\\sqrt{2} &  -1/\\sqrt{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1/\\sqrt{2} &  1/\\sqrt{2}\\\\\n1/\\sqrt{2} &  -1/\\sqrt{2}\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n1+\\rho & 0 \\\\\n0 & 1-\\rho\n\\end{bmatrix} \\\\\n&=\n\\pmb \\Lambda\n\\end{align*}\\]\n또한 다음도 성립함을 확인할 수 있다.\n\\[  (\\pmb X^t \\pmb X)^{-1} =  \\pmb P \\pmb \\Lambda^{-1} \\pmb P^t   \\]\n즉,\n\\[\n\\begin{aligned}\n(\\pmb X^t \\pmb X)^{-1} & =  \\pmb P \\pmb \\Lambda^{-1} \\pmb P^t \\\\   \n&  =\n\\begin{bmatrix}\np_{11} &  p_{12}\\\\\np_{21} &  p_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{1}{\\lambda_1} & 0 \\\\\n0 & \\frac{1}{\\lambda_2}\n\\end{bmatrix}\n\\begin{bmatrix}\np_{11} &  p_{21}\\\\\np_{12} &  p_{22}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n1/\\sqrt{2} &  1/\\sqrt{2}\\\\\n1/\\sqrt{2} &  -1/\\sqrt{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{1}{1+\\rho} & 0 \\\\\n0 & \\frac{1}{1-\\rho}\n\\end{bmatrix}\n\\begin{bmatrix}\n1/\\sqrt{2} &  1/\\sqrt{2}\\\\\n1/\\sqrt{2} &  -1/\\sqrt{2}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\np_{11}^2 \\frac{1}{\\lambda_1} +   p_{12}^2 \\frac{1}{\\lambda_2} &\np_{11}  p_{21} \\frac{1}{\\lambda_1} +  p_{12}  p_{22} \\frac{1}{\\lambda_2} \\\\\np_{11}  p_{21} \\frac{1}{\\lambda_1} +  p_{12}  p_{22} \\frac{1}{\\lambda_2}   &\np_{21}^2 \\frac{1}{\\lambda_1} +   p_{22}^2 \\frac{1}{\\lambda_2}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n(\\frac{1}{\\sqrt{2}})^2 \\frac{1}{1+\\rho} +  (\\frac{1}{\\sqrt{2}})^2 \\frac{1}{1-\\rho} &\n(\\frac{1}{\\sqrt{2}})^2 \\frac{1}{1+\\rho} + (\\frac{1}{\\sqrt{2}}) (-\\frac{1}{\\sqrt{2}}) \\frac{1}{1-\\rho} \\\\\n(\\frac{1}{\\sqrt{2}})^2 \\frac{1}{1+\\rho} + (\\frac{1}{\\sqrt{2}}) (-\\frac{1}{\\sqrt{2}}) \\frac{1}{1-\\rho}  &\n(\\frac{1}{\\sqrt{2}})^2 \\frac{1}{1+\\rho} +  (-\\frac{1}{\\sqrt{2}})^2 \\frac{1}{1-\\rho}\n\\end{bmatrix} \\\\\n& =\n\\frac{1}{1-\\rho^2}\n\\begin{bmatrix}\n1  & -\\rho \\\\\n-\\rho & 1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n앞 절에서 나온 회귀계수 추정량의 분산 공식 식 H.1 과 식 H.2 를 적용하면 다음과 같은 식을 얻을 수 있다.\n\\[\n\\begin{aligned}\nVar(\\hat \\beta_k)/\\sigma^2\n& = \\frac{p^2_{k1}}{\\lambda_1} + \\frac{p^2_{k2}}{\\lambda_2} \\\\\n& = \\frac{1}{2} \\left ( \\frac{1}{1+\\rho} + \\frac{1}{1-\\rho} \\right ) \\\\\n& = \\frac{1}{1-\\rho^2}\n\\end{aligned}\n\\]\n위의 분산 공식에서 제일 작은 두 번째 고유값 \\(\\lambda_2 = 1- \\rho\\)가 0에 가까우면 분산이 매우 커지는 것을 알 수 있다. 이 고유값은 상관계수 \\(\\rho\\)가 1에 가까울 수록 0에 가까워 진다.\n\n\nH.3.3 예제 4.13\n중고차 예제에서 가상의 변수를 만들어 적합할 때 완벽한 선형관계가 존재하면 적합 시 변수를 제거하는 것을 알 수 있다.\n\nusedcars2 &lt;- usedcars %&gt;%  mutate(ccmile = cc + mileage)\nfitcoll1 &lt;- lm(price ~ year + mileage + cc + automatic + ccmile, usedcars2)\nsummary(fitcoll1)\n\n\nCall:\nlm(formula = price ~ year + mileage + cc + automatic + ccmile, \n    data = usedcars2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-177.35  -63.91   -0.99   70.34  212.69 \n\nCoefficients: (1 not defined because of singularities)\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.253e+02  3.998e+02   1.314 0.200823    \nyear        -5.800e+00  9.283e-01  -6.247 1.55e-06 ***\nmileage     -2.263e-03  7.211e-04  -3.138 0.004324 ** \ncc           3.888e-01  2.022e-01   1.923 0.065958 .  \nautomatic    1.653e+02  3.986e+01   4.147 0.000339 ***\nccmile              NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 101.1 on 25 degrees of freedom\nMultiple R-squared:  0.9045,    Adjusted R-squared:  0.8892 \nF-statistic: 59.21 on 4 and 25 DF,  p-value: 2.184e-12\n\n\n\n\nH.3.4 예제 4.14\n모형을 적합해 보자.\n\nhald.lm &lt;- lm(y~ ., data=hald)\nsummary(hald.lm)\n\n\nCall:\nlm(formula = y ~ ., data = hald)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1750 -1.6709  0.2508  1.3783  3.9254 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  62.4054    70.0710   0.891   0.3991  \nx1            1.5511     0.7448   2.083   0.0708 .\nx2            0.5102     0.7238   0.705   0.5009  \nx3            0.1019     0.7547   0.135   0.8959  \nx4           -0.1441     0.7091  -0.203   0.8441  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.446 on 8 degrees of freedom\nMultiple R-squared:  0.9824,    Adjusted R-squared:  0.9736 \nF-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07\n\n\n상관계수 행렬의 고유값을 계산해 보자.\n\nR &lt;- cor(hald[2:5])\nR\n\n           x1         x2         x3         x4\nx1  1.0000000  0.2285795 -0.8241338 -0.2454451\nx2  0.2285795  1.0000000 -0.1392424 -0.9729550\nx3 -0.8241338 -0.1392424  1.0000000  0.0295370\nx4 -0.2454451 -0.9729550  0.0295370  1.0000000\n\nsolve(R)\n\n         x1        x2        x3       x4\nx1 38.49621  94.11969  41.88410  99.7858\nx2 94.11969 254.42317 105.09139 267.5394\nx3 41.88410 105.09139  46.86839 111.1451\nx4 99.78580 267.53942 111.14509 282.5129\n\ndiag(solve(R))\n\n       x1        x2        x3        x4 \n 38.49621 254.42317  46.86839 282.51286 \n\neigenval &lt;- eigen(R)$values\neigenval\n\n[1] 2.235704035 1.576066070 0.186606149 0.001623746\n\nsqrt(max(eigenval)/eigenval)\n\n[1]  1.000000  1.191022  3.461339 37.106342\n\n\nVIF를 구해보자.\n\ncar::vif(hald.lm)\n\n       x1        x2        x3        x4 \n 38.49621 254.42317  46.86839 282.51286 \n\nsummary(regbook::vif(hald.lm))\n\n\nVIF:\n    x1     x2     x3     x4 \n 38.50 254.42  46.87 282.51 \n\nVariance Proportion:\n  Eigenvalues Cond.Index          x1           x2          x3           x4\n1 2.235704035   1.000000 0.002632084 0.0005589686 0.001481988 0.0004753347\n2 1.576066070   1.191022 0.004269804 0.0004272931 0.004954638 0.0004572915\n3 0.186606149   3.461339 0.063519491 0.0020822791 0.046495910 0.0007243995\n4 0.001623746  37.106342 0.929578621 0.9969314592 0.947067464 0.9983429744\n\n\n\\(x_2\\)를 제외하고 분석해 보자.\n\nhald.lm2 &lt;- lm(y~ x1 + x3 + x4, data=hald)\nsummary(hald.lm2)\n\n\nCall:\nlm(formula = y ~ x1 + x3 + x4, data = hald)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9323 -1.8090  0.4806  1.1398  3.7771 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 111.68441    4.56248  24.479 1.52e-09 ***\nx1            1.05185    0.22368   4.702  0.00112 ** \nx3           -0.41004    0.19923  -2.058  0.06969 .  \nx4           -0.64280    0.04454 -14.431 1.58e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.377 on 9 degrees of freedom\nMultiple R-squared:  0.9813,    Adjusted R-squared:  0.975 \nF-statistic: 157.3 on 3 and 9 DF,  p-value: 4.312e-08\n\nsummary(regbook::vif(hald.lm2))\n\n\nVIF:\n   x1    x3    x4 \n3.678 3.460 1.181 \n\nVariance Proportion:\n  Eigenvalues Cond.Index           x1         x3         x4\n1   1.8683737   1.000000 0.0720157120 0.07053018 0.02229687\n2   0.9838532   1.378056 0.0002285765 0.02382939 0.79011946\n3   0.1477731   3.555775 0.9277557115 0.90564042 0.18758367",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>R-실습: 중회귀 모형 진단</span>"
    ]
  }
]