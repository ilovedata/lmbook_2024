# 모형의 선택 {#sec-selsection}

```{r warning=FALSE, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
source(here::here("common", "_common.R"))
source(here::here("common", "_preprocess.R"))
```

## 서론

모형의 선택은 자료의 분석에서 고려하는 다수의 모형들(a family of models) 중에서 가장 적합한 모형(best model)을 찾는 것이다. 여기서 가장 적합하다는 의미는 다양한 기준이 있지만 일반적으로 선택된 모형의 예측력 또는 설명력이 다른 모든 모형들보다 더 좋다는 의미이다.

분석에서 고려하는 모형들의 집합을 모형 공간(model space)이라고 하며 이 공간에서 가장 적합한 모형을 찾는 것이 모형 선택(model selection)이다.  이 장에서 **모형의 예측력 또는 설명력** 을 정의하고 비교하는 방법에 대하여 배울 것이다.

주어진 모형 공간에서 가장 좋은 모형을 선택했을 때 다음과 같은 질문이 가능하다.

  선택된 모형보다 더 좋은 모형이 있지 않을까? 더 좋은 모형이 주어진 모형공간에 포함되지 않을 수도 있다.
  
주어진 자료에서 반응변수와 설명변수의 관계를 더욱 잘 설명할 수 있는모형을 계속 찾는다면 결국에는 예측력을 높이기 위하여 더 많은 설명변수를 포함하는 모형을 찾게 될 것이다. 궁국적으로는 반응변수의 관측값 $y$ 와 예측값 $\hat y$의 차이가  가장 작은 모형, 즉 설명력이 가장 좋은 모형을 선택하려는 노력을 계속한다면 **과적합(overfitting)** 이 발생할 수 있다.

과적합은 모형의 복잡도가 증가함에 따라 주어진 자료에 대한 모형의 예측력은 증가하지만 모형의 일반적인 예측의 효율은 
오히려 감소하는 현상을 말한다.

이러한 과적합을 피하려면 모형의 복잡도와 예측력 사이의 적절한 균형을 찾아야 한다. 

현실 세계의 상황에서는 진정한 모형이 알려지지 않거나 자료의 정확한 분포와 관계를 기술할 수 있는 모형을 파악하는 것은 매우 어렵다. 하지만 실제로 데이터를 생성하는 과정이나 현상을 정확하게 기술하는 가상의 모형이 존재한다고 가정할 수는 있다. 이렇게 자료의 분포와 관계를 정확하게 기술하는 가상의 모형을  참모형(true model)이라고 한다.
다시 강조하지만 가상의 모형이라고 말한 의미는 자료의 생성 과정을 정확하게 기술할 수 있는 모형을 구체화하여 표현하는 것이 매우 힘들기 떄문이다.  

모형의 선택하는 또 다른 기준은 가상의 **참모형에 제일 가까운 모형** 을 선택하는 것이다. 우리가 생각할 수 있는 대부분의 모형 공간은 참모형을 포함하지 않는 다고 가정할 수 있다. 이러한 경우 고려하는 다수의 모형들 중에서 참모형에 가장 가까운 모형을 최적의 모형이라고 할 수 있다.


## 모형선택의 측도


회귀분석모형의 구축을 시작할 때는 될 수 있는 한 많은 독립변수들을 고려하고 그 중에 모형에 적합한 변수들과 그렇지 않은 변수들을 구별하여 최선의 모형을 찾으려고 많은 노력을 기울인다. 

이 절에서는 설명변수의 조합으로 만들 수 있는 다양한 모형들을 비교할 수 있는 기준과 통계적 방법에 대하여 알아보고자 한다.


일반적인 회귀분석모형에서 다음과 같은 선형 회귀모형을 가정한다.

$$ {\pmb y} = {\pmb X_p} {\pmb \beta}_p + \pmb e $$

오차항이 다음과 같이 서로 독립이고 등분산성을 만족한다면

$$ V(\pmb e) =\sigma^2 \pmb I_n $$

최소제곱법에 의한 회귀계수 추정량 $\hat {\pmb \beta}_p$ 다음과 같고

$$  \hat {\pmb \beta}_p = ({\pmb X}_p^t{\pmb X}_p)^{-1}{\pmb X}_p^t{\pmb y} $$

::: {.callout-important}

이 절의 선형 회귀모형에서는 독립변수의 개수가 $p$개인 것을 강조하기 위하여 ${\pmb X}_p$ 와 ${\pmb \beta}_p$ 를 사용하였다. 

::: 


모든 가능한 회귀모형의 개수는 $2^p-1$개이므로 $p$가 크지 않다면 가능한 모든 회귀모형을 비교하여 하나의 모형을 선택하는 것이 좋을 것이다. 여러가지 모형들을 비교할 수 있는 모형 선택의 측도들을 알아보자.


### 결정계수

총제곱합에서 회귀모형으로 설명할 수 있는 변동 모형 제곱합이 차지하는 부분의 비율, 즉 모형제곱합 $SSR$을 총제곱합 $SST$으로 나눈 비율을 결정계수(coefficient of determination)라 하며 $R^2$으로 표현한다.

$$
 R^2 = \frac{SSR}{SST} =  1 -\frac{SSE}{SST}  =1- \frac{\sum^n_{i=1}(y_i-\hat y_i)^2}{ \sum^n_{i=1}(y_i - \bar y)^2} 
$$   {#eq-selr2}


결정계수 $R^2$는  언제나 0 이상 1 이하의 값을 갖는다. 회귀모형이 데이터에 아주 잘 적합되면 결정계수의 값은 1 에 가깝게 된다.

주의할 점은 회귀식에 독립변수를 추가하면 결정계수는 언제나 증가한다. 즉 반응변수와 관련이 없는 변수도 회귀식에 추가하면 결정계수의 값이 증가하기 때문에 결정계수로 모형을 선택하면 언제나 모든 독립변수가 모형에 들어간 가장 큰 모델이 선택된다.


### 결정계수의 수정

수정 결정계수 $\tilde R^2$는 독립변수의 개수가 증가함에 따라 증가하는 결정계수 $R^2$를 보정한 모형 선택의 척도이다.

$$
\begin{aligned}
 \tilde R^2 & = 1-  \frac{SSE_p/(n-p)}{SST/(n-1)} \\ \notag 
  & = 1 - \frac{s_p^2}{SST/(n-1)}  
\end{aligned}
$$   {#eq-seladjr2}  

여기서 $p$는 회귀모형에 포함된 독립변수의 개수이다.


### Mallow's $C_p$ 

모형의 적합도를 측정하기 위한 여러 가지 통계량중 가장 중요하고 자주 쓰이는 통계량이 평균제곱오차(mean squared error; MSE)이다 이 책에서는 평균제곱오차를 $\Delta_p^2$ 으로 표시할 것이다.

반응변수 $y_i$의 평균을 $\mu_i=E(y_i)$로 하고 독립 변수의 개수가 $p$개인 선형회귀 모형에서 최소제곱법에 의한 예측값을
$\hat y_{ip} = {\pmb x}_{ip}^t \hat {\pmb \beta}_p$라고 하면 MSE는 다음과 같이 주어진다.

$$
\begin{aligned}
E [ (\hat y_{ip} -\mu_i)^2 ] &= E[( {\pmb x}_{ip}^t \hat {\pmb \beta}_p-\mu_i)^2 ] \\
  &=  E[( {\pmb x}_{ip}^t \hat {\pmb \beta}_p -E({\pmb x}_{ip}^t \hat {\pmb \beta}_p) + E({\pmb x}_{ip}^t \hat {\pmb \beta}_p)-\mu_i)^2 ] \\
  &= Var({\pmb x}_{ip}^t \hat {\pmb \beta}_p)+ [E({\pmb x}_{ip}^t \hat {\pmb \beta}_p)-\mu_i]^2 \\
  &= \sigma^2 {\pmb x}_{ip}^t({\pmb X}_p^t {\pmb X}_p^t)^{-1} {\pmb x}_{ip} + ( \eta_{ip}-\mu_i)^2
\end{aligned}
$$

여기서 $\eta_{ip} = E({\pmb x}_{ip}^t \hat {\pmb \beta}_p)$이다. 여기서 유의할 점은  반응변수 $y_i$의 평균을 $\mu_i$와 $\eta_{ip}$ 는 다를 수도 있으며 그 차이를 모형에 의한 편이(bias)라고 한다.

$$ E({\pmb x}_{ip}^t \hat {\pmb \beta}_p ) -\mu_i =  \eta_{ip} -\mu_i $$

이제 평균제곱오차를  구하기 위하여 각각의 관측값 $y_1, y_2,\dots,y_n$에 대한 제곱합을 구해보자 

$$
\begin{aligned}
\Delta_p^2 &= \sum_i E(\hat y_{ip} -\mu_i)^2   \\
  &= \sigma^2 \sum_i {\pmb x}_{ip}^t({\pmb X}_p^t {\pmb X}_p^t)^{-1} {\pmb x}_{ip} + \sum_i (\eta_{ip}-\mu_i)^2 \\ \notag
  & = \sigma^2 tr( {\pmb X}_p ({\pmb X}_p^t {\pmb X}_p^t)^{-1} {\pmb X}_p^t ) + \sum_i (\eta_{ip}-\mu_i)^2 \\ \notag
  &= p \sigma^2 + SSB_p
\end{aligned}
$$ {#eq-selmse1}    

여기서 $SSB_p=\sum_i (\eta_{pi}-\mu_i)^2$이며 예측값의 편이들의 제곱합이다.평균제곱오차 $\Delta_p^2$ 은 모형에서 추정된 값이  실제 평균과 가까운 정도를 나타내는 측도이지만 실제로 자료를 이용하여 구할 수는 없는 양이다.

여기서 중요한 점은 평균제곱오차 $\Delta_p^2$ 는 분산과 편차 제곱들의 합이다.  

 
실제 평균제곱오차 $\Delta_p^2$는 계산할 수 있는 값이 아니므로 이를 적절히 추정할 수 있는 통계량으로 잔차제곱합 (SSE)를 생각해 보자. 독립 변수의 개수가 $p$개인 선형회귀 모형에 의한 잔차제곱합을 고려하고 그 기대값을 구해보면

$$
\begin{aligned}
E(SSE_p) &= E[ {\pmb y}^t ({\pmb I} - {\pmb X}_p ({\pmb X}_p^t {\pmb X}_p)^{-1}{\pmb X}_p^t) {\pmb y} ] \\ \notag
       &= tr( \sigma^2 ({\pmb I} - {\pmb X}_p ({\pmb X}_p^t {\pmb X}_p)^{-1}{\pmb X}_p^t)) +
         E({\pmb y})^t  ({\pmb I} - {\pmb X}_p ({\pmb X}_p^t {\pmb X}_p)^{-1}{\pmb X}_p^t) E({\pmb y}) \\ \notag
      &= \sigma^2 tr(  ({\pmb I} - {\pmb X}_p ({\pmb X}_p^t {\pmb X}_p)^{-1}{\pmb X}_p^t)) +
             E({\pmb y})^t ({\pmb I} -\pmb H_p)({\pmb I} -\pmb H_p ) E({\pmb y}) \\ \notag 
         &= \sigma^2 (n-p) + [E({\pmb y}) - E(\hat {\pmb y}_p)]^t [E({\pmb y}) - E(\hat {\pmb y}_p)] \\
       &= \sigma^2(n-p) + SSB_p
\end{aligned}
$$  {#eq-selmse2}   

위의 결과는  ${\pmb I} - {\pmb X}_p ({\pmb X}_p^t {\pmb X}_p)^{-1}{\pmb X}_p^t = {\pmb I} -{\pmb H}_p$가 멱등행렬인 사실과 아래의 식을 이용하였다.


$$ \eta_p = E(\hat {\pmb y}_p) = E({\pmb X}_p^t \hat {\pmb \beta}_p) =  {\pmb X}_p ({\pmb X}_p^t {\pmb X}_p)^{-1}{\pmb X}_p^t E( {\pmb y})  = {\pmb H}_p E( {\pmb y})$$
만약 $\sigma^2$의 불편추정량을 $\hat \sigma^2$라 하면 식 @eq-selmse1 와 @eq-selmse2 를 
이용하여 다음과 같은 결과를 얻는다.

$$
\begin{aligned}
E[SSE_p -(n-2p)\hat \sigma^2] & = \sigma^2(n-p) + SSB_p -(n-2p) E(\hat \sigma^2) \\ \notag  
       & = p\sigma^2 +SSB_p  \\ \notag 
       & = \Delta_p^2 
\end{aligned}
$$  {#eq-selcp2}  

따라서 평균제곱오차 $\Delta_p^2$의 추정량으로  $SSE_p -(n-2p)\hat \sigma^2$ 을 사용할 수 있다.  Mallow(1973)가 제안한 Mallow's $C_p$는 평균제곱오차를 분산의 추정량으로 나눈값 $\Delta_p^2/\sigma^2$이며 이를 최소화는 모형을 선택할 것을 Mallow가 제안하였다.




$$
C_p = \frac{SSE_p}{\hat \sigma^2 } + (2p-n) 
$$  {#eq-selcp}  


식 @eq-selcp  에서 주어진 $C_p$ 에서 $\hat \sigma^2$은 고려하는 모든 변수를 포함하는 모형(full model)에서  구한 오차항 분산의 추정량이다. Mallow(1973)는  $\Delta_p^2$이
$SSB_p$가 0일 때, 즉 $E(\hat {\pmb y}_p)=E({\pmb y})$일 때 최소값 $p \sigma^2$를 같는다는 사실에 의거하여
$C_p$와 $p$에 대한 그림을 그리고 $C_p$의 값이 해당하는 $p$값에 가깝거나 작은 모형을 선택하는 탐색적 방법을 제안하였다.


여기서 주목할 점은 Mallow's $C_p$ 에서 설명변수의 개수 $p$의 개수를 크게 하면 $SSE_p$ 는 작아지지만 
항 $2p-n$은 증가하게 된다. 따라서 $SSE_p$에 더해주는 항  $2p-n$ 은 설명변수의 증가에 따른 
벌칙항(penalty term)으로 볼 수 있다.



### PRESS

 PRESS는 prediction error sum of square의 약자로 Cross-validation에 의거한 모형선택을 위한 척도이다. 전차분석에서 보았던 처럼 $i$번째 관측치 $(y_i,{\pmb x}_i)$를 제외한 반응변수 벡터, 계획행렬,  회귀계수를 각각 ${\pmb y}_{-i}$, ${\pmb X}_{-i}$, $\hat {\pmb \beta}_{-i}$와 같이  표시하고 그에 해당하는 예측값을 $\hat y_{ip,-i}$라 하면 RESS는 다음과 같이 정의된다.
 
 
$$ 
 PRESS_p  = \frac{1}{n} \sum_{i=1}^n (y_i - \hat y_{ip,-i})^2 
$$   {#eq-selpress}  
 

여기서 잔차분석에서 유도한 것처럼

$$ y_i - \hat y_{ip,-i} = \frac{r_i}{1-h_{ii}} $$

를 이용하면 PRESS를 다음과 같이 표현할 수 있다.

$$ 
PRESS_p = \frac{1}{n} \sum_{i=1}^n  \left [ \frac{y_i - \hat y_{ip}}{1-h_{ii}} \right ]^2
\approx \frac{ SSE_p }{n(1-p/n)^2} 
$$

위의 식 마지막 근사는 모든 $h_{ii}$가 그 평균값 $p/n$에 가깝다는 가정 하에 세워진 식이다.


## AIC 와 BIC

통계모형을 선택하는 척도로서 가능도함수이론에 근거한 AIC(Akaike information criteria)와 베이지안 검정이론에 기초한 BIC(bayesian or schwartz information criteria)가 있다. 


AIC와 BIC는 회귀분석뿐 아니라 일반적인 통계 모형에서 자주 사용하는 모형의 선택에 대한 척도이다. AIC와 BIC의 정의는 다음과 같다.

$$
AIC =  -2 \log \ell(\hat {\pmb \theta}) + 2k 
$$  {#eq-selbic}  

$$
BIC =  -2 \log \ell(\hat {\pmb \theta}) + (\log n) k 
$$   {#eq-selaic}  

여기서 $k$는 모형에 포함된 모수의 총 개수 이다.
$\ell(\hat {\pmb \theta})$은 최대가능도추정량  $\hat {\pmb \theta}$에서 계산된 로그 가능도함수이다. 

선형모형에 대한 가능도 추정에서 식 @eq-linregloglike 에서 보았듯이 정규분포 가정 하에서 회귀모형에 대한 로그 가능도함수는 다음과 같으므로


$$
l_n(\hat { \pmb \theta} ) = l_n(\hat { \pmb \beta} ,\hat \sigma^2 ) 
= -\frac{n}{2}\log(2\pi) - \frac{n}{2}  - \frac{n}{2} \log \frac{SSE_p}{n} 
$$

따라서 선형회귀 모형에서의  AIC와 BIC는 다음과 같이 주어진다.

$$
\begin{aligned}
AIC &= n\log(2\pi) + n + n \log  \frac{SSE_p}{n} + 2(p+1) \\
BIC &= n\log(2\pi) + n + n \log  \frac{SSE_p}{n} + (\log n) (p+1)
\end{aligned}
$$

여기서 $p$는 회귀모형에 포함된 독립변수의 개수이며 오차항의 분산까지 포함하여 모수의 총 개수는 $p+1$ 이다.  

이제 잔차제곱합 $SSE_p$ 가 작아지면 AIC 와 BIC의 $SSE_p$ 부분이 작아지지만 각 측도의  벌칙항은 증가하게 된다.
여러 개의 모형을 비교하 때 AIC, BIC 의 값이 작은 모형이 좋은 모형이라고 할 수 있다.
또한 주목할 점은 AIC, BIC 의 벌칙항이 다르며 특히 BIC 의 벌칙항에 표본의 개수 $n$ 이 로그스케일로 포함되어 있다.  

AIC 와  BIC 에 대한 이론적인 설명은 @sec-aic  에 제시되어 있으니 참고하자.


## 변수 선택법

주어진 설명변수들 중에 반응변수에 유의한 영향을 미치는 변수들을 단계적으로 선택하는 방법(variable selection procedure)은 다음과 같이 세 종류의 방법이 있다.


- Forward selection: Forward selection 방법은 회귀모형에 독립변수를 하나 씩 추가하는 방법이다. 첫 번째 추가하는 변수는 설명변수가 한 개인 모형 중에 결정계수 $R^2$(또는 다른 측도)이 가장 큰 변수를 선택하며 두번째 부터는 추가되었을 때  $R^2$의 증가가 가장 큰 값을 선택하게 된다. 변수의 추가가 멈추는 조건은 추가된 변수가 주어진 신뢰수준에서 유의하지 않을 때이다.

- Backward elimination: Backward elimination 방법은 모든 설명변수를 포함한 가장 큰 회귀모형(full model)에서 설명변수를 하나 씩 제거하는 방법이다. 제거하는 변수의 선택은 변수가 제거되었을 때  $R^2$의 감소가 가장 작은 값을 선택하게 된다.

- Stepwise: Stepwise는  Forward selection과 Backward elimination을 조합하여 변수의 추가와 제거가 모두 가능한 방법이다.


변수선택법은 이 방법이 제안되었을 당시 매우 유용한 방법으로 여겨졌다. 그러나 변수선택법의 무리한 남용 등 여러 가지 단점들로 인하여 조심해서 사용해야 한다는 것이 현재의 공통된 의견이다. 변수선택법의 이용과 그 유의사항은 다음과 같이 요약할 수 있다.



- 미숙한 이용자에 의해 남용될 수 있다.

- 다중공선성이 존재할 때 불안정하다.

- Stepwise는 주어진 추가와 제거 시 사용되는 유의수준에 따라  최적의 모형이 다를 수 있다.

- 모든 가능한 회귀 모형(All possible regressions)을 사용하는 것이 대안이 될 수 있다.

- 과적합(overfitting)의 위험성이 크다.

- 변수의 추가나 제거에 통계적 검정법을 쓰는데 여러 가지 위험성이 존재한다 (예로 다중비교 문제)






